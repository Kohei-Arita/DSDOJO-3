from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, Iterable, List, Optional, Tuple

import numpy as np
import pandas as pd


SHOT_TYPES = {"shot", "shot_freekick", "shot_penalty"}


def _sort_actions(actions: pd.DataFrame, match_col: str) -> pd.DataFrame:
    # å®‰å…¨ãªå®‰å®šã‚½ãƒ¼ãƒˆï¼ˆmergesortï¼‰ã§æ™‚ç³»åˆ—é †ã‚’æ‹…ä¿
    sort_cols = [match_col]
    if "period_id" in actions.columns:
        sort_cols.append("period_id")
    if "time_seconds" in actions.columns:
        sort_cols.append("time_seconds")
    actions = actions.sort_values(sort_cols, kind="mergesort").copy()
    return actions


def build_nstep_chain_features(
    actions: pd.DataFrame,
    match_col: str = "match_id",
    player_col: str = "player_id",
    team_col: str = "team_id",
    type_col: str = "type_name",
    n_steps: int = 3,
    gamma: float = 0.7,
) -> pd.DataFrame:
    """Næ‰‹å…ˆã®ã‚·ãƒ§ãƒ¼ãƒˆãƒ›ãƒ©ã‚¤ã‚¾ãƒ³é€£é–ï¼ˆåŒä¸€ãƒãƒ¼ãƒ å†…ï¼‰ã¨ã€xtãƒ‡ãƒ«ã‚¿ã®å‰²å¼•å’Œã‚’é›†è¨ˆã€‚

    æˆ»ã‚Šå€¤ã¯ matchÃ—player ã®é›†è¨ˆ DataFrameï¼ˆnstep_to_shot, nstep_xt_deltaï¼‰ã€‚
    xtãƒ‡ãƒ«ã‚¿åˆ—ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ nstep_xt_delta ã¯0ã§åŸ‹ã‚ã¾ã™ã€‚
    """
    sa = _sort_actions(actions, match_col)

    # æ¬¡ã®Næ‰‹ã® type / team / player ã‚’ç”¨æ„
    for k in range(1, n_steps + 1):
        sa[f"next{k}_type"] = sa.groupby(match_col)[type_col].shift(-k)
        sa[f"next{k}_team"] = sa.groupby(match_col)[team_col].shift(-k)
        sa[f"next{k}_player"] = sa.groupby(match_col)[player_col].shift(-k)

    # è‡ªèº«ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒã€åŒä¸€ãƒãƒ¼ãƒ å†…ã§Næ‰‹ä»¥å†…ã«ã‚·ãƒ¥ãƒ¼ãƒˆã«åˆ°é”ã—ãŸã‹ï¼ˆå‰²å¼•å’Œï¼‰
    weights = {k: (gamma ** (k - 1)) for k in range(1, n_steps + 1)}
    contrib = np.zeros(len(sa), dtype=float)
    for k in range(1, n_steps + 1):
        mask_k = (sa[f"next{k}_team"] == sa[team_col]) & (sa[f"next{k}_type"].isin(SHOT_TYPES))
        contrib += weights[k] * mask_k.astype(float)

    # xtãƒ‡ãƒ«ã‚¿ã®å‰²å¼•å’Œï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
    if "xt_learned_delta" in sa.columns:
        xt_contrib = np.zeros(len(sa), dtype=float)
        for k in range(1, n_steps + 1):
            future_same_team = (sa[f"next{k}_team"] == sa[team_col])
            # æ¬¡kæ‰‹ã®xt_deltaã‚’å–ã‚‹ãŸã‚ã€æœªæ¥è¡Œã®å€¤ã‚’ç¾åœ¨è¡Œã«åˆã‚ã›ã¦ã‚·ãƒ•ãƒˆ
            future_xt = sa.groupby(match_col)["xt_learned_delta"].shift(-k)
            xt_contrib += weights[k] * np.where(future_same_team, future_xt.fillna(0.0), 0.0)
    else:
        xt_contrib = np.zeros(len(sa), dtype=float)

    out = (
        sa.assign(nstep_to_shot=contrib, nstep_xt_delta=xt_contrib)
        .groupby([match_col, player_col], as_index=False)[["nstep_to_shot", "nstep_xt_delta"]]
        .sum()
    )
    return out


def build_second_assist_sca_gca(
    actions: pd.DataFrame,
    match_col: str = "match_id",
    player_col: str = "player_id",
    team_col: str = "team_id",
    type_col: str = "type_name",
    result_col: str = "result_name",
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """ã‚»ã‚«ãƒ³ãƒ‰ã‚¢ã‚·ã‚¹ãƒˆã€SCA(1/2æ‰‹å‰)ã€GCA(1/2æ‰‹å‰) ã®å„é›†è¨ˆã‚’è¿”ã™ã€‚

    Returns: (second_assist, sca1, sca2, gca1, gca2)
    """
    sa = _sort_actions(actions, match_col)
    sa["next_type"] = sa.groupby(match_col)[type_col].shift(-1)
    sa["next_team"] = sa.groupby(match_col)[team_col].shift(-1)
    sa["next2_type"] = sa.groupby(match_col)[type_col].shift(-2)
    sa["next2_team"] = sa.groupby(match_col)[team_col].shift(-2)

    second_assist_mask = (
        (sa[type_col] == "pass")
        & (sa["next_type"] == "pass")
        & (sa["next2_type"].isin(SHOT_TYPES))
        & (sa["next_team"] == sa[team_col])
        & (sa["next2_team"] == sa[team_col])
    )
    second_assist = (
        sa.loc[second_assist_mask]
        .groupby([match_col, player_col], as_index=False)
        .size()
        .rename(columns={"size": "second_assist_count"})
    )

    # SCA / GCA
    sa["prev_type"] = sa.groupby(match_col)[type_col].shift(1)
    sa["prev_team"] = sa.groupby(match_col)[team_col].shift(1)
    sa["prev_player"] = sa.groupby(match_col)[player_col].shift(1)
    sa["prev2_type"] = sa.groupby(match_col)[type_col].shift(2)
    sa["prev2_team"] = sa.groupby(match_col)[team_col].shift(2)
    sa["prev2_player"] = sa.groupby(match_col)[player_col].shift(2)

    is_shot = sa[type_col].isin(SHOT_TYPES)
    same1 = sa["prev_team"].eq(sa[team_col])
    same2 = sa["prev2_team"].eq(sa[team_col])

    sca1 = (
        sa[is_shot & same1]
        .groupby([match_col, "prev_player"], as_index=False)
        .size()
        .rename(columns={"prev_player": player_col, "size": "SCA_1"})
    )
    sca2 = (
        sa[is_shot & same2]
        .groupby([match_col, "prev2_player"], as_index=False)
        .size()
        .rename(columns={"prev2_player": player_col, "size": "SCA_2"})
    )

    is_goal = is_shot & sa[result_col].eq("success")
    gca1 = (
        sa[is_goal & same1]
        .groupby([match_col, "prev_player"], as_index=False)
        .size()
        .rename(columns={"prev_player": player_col, "size": "GCA_1"})
    )
    gca2 = (
        sa[is_goal & same2]
        .groupby([match_col, "prev2_player"], as_index=False)
        .size()
        .rename(columns={"prev2_player": player_col, "size": "GCA_2"})
    )

    return second_assist, sca1, sca2, gca1, gca2


def build_pass_geometry_and_timing(
    actions: pd.DataFrame,
    match_col: str = "match_id",
    player_col: str = "player_id",
    type_col: str = "type_name",
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """ãƒ‘ã‚¹å¹¾ä½•ï¼ˆè·é›¢/ã‚´ãƒ¼ãƒ«æ–¹å‘ï¼‰ã¨ passâ†’shot ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·é›†è¨ˆã€‚

    Returns: (pass_geometry_agg, latency_agg)
    pass_geometry_agg: pass_dist_mean/max, to_goal_angle_abs_mean, to_goal_dist_mean
    latency_agg: pass_to_shot_latency_mean/min
    """
    sa = _sort_actions(actions, match_col)
    # ãƒ‘ã‚¹å¹¾ä½•
    p = sa[sa[type_col] == "pass"].copy()
    if p.empty:
        pass_geom_agg = pd.DataFrame(columns=[match_col, player_col, "pass_dist_mean", "pass_dist_max", "to_goal_angle_abs_mean", "to_goal_dist_mean"])  # empty
    else:
        p["dx"] = (p["end_x"] - p["start_x"]).fillna(0.0)
        p["dy"] = (p["end_y"] - p["start_y"]).fillna(0.0)
        p["pass_dist"] = np.hypot(p["dx"], p["dy"]).astype(float)
        # ã‚´ãƒ¼ãƒ«åº§æ¨™ã‚’(105,34)ã¨ã¿ãªã™ï¼ˆåº§æ¨™ã¯ãƒ›ãƒ¼ãƒ åŸºæº–ã¸æ¨™æº–åŒ–æ¸ˆã¿å‰æï¼‰
        p["to_goal_dx"] = 105.0 - p["end_x"]
        p["to_goal_dy"] = 34.0 - p["end_y"]
        # è§’åº¦ãŒå°ã•ã„ã»ã©ã‚´ãƒ¼ãƒ«ã¸ç›´é€²
        p["to_goal_angle_abs"] = np.arctan2(np.abs(p["to_goal_dy"]), np.maximum(p["to_goal_dx"], 1e-6))
        p["to_goal_dist"] = np.hypot(p["to_goal_dx"], p["to_goal_dy"]).astype(float)
        pass_geom_agg = (
            p.groupby([match_col, player_col], as_index=False)
            .agg(
                pass_dist_mean=("pass_dist", "mean"),
                pass_dist_max=("pass_dist", "max"),
                to_goal_angle_abs_mean=("to_goal_angle_abs", "mean"),
                to_goal_dist_mean=("to_goal_dist", "mean"),
            )
        )

    # passâ†’shotã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·
    sa["next_type"] = sa.groupby(match_col)[type_col].shift(-1)
    sa["next_team"] = sa.groupby(match_col)["team_id"].shift(-1) if "team_id" in sa.columns else pd.NA
    sa["next_time"] = sa.groupby(match_col)["time_seconds"].shift(-1)
    lat_df = sa[(sa[type_col] == "pass") & (sa["next_type"].isin(SHOT_TYPES))].copy()
    if "team_id" in sa.columns:
        lat_df = lat_df[lat_df["next_team"] == lat_df["team_id"]]
    if lat_df.empty:
        latency_agg = pd.DataFrame(columns=[match_col, player_col, "pass_to_shot_latency_mean", "pass_to_shot_latency_min"])  # empty
    else:
        lat_df["pass_to_shot_latency"] = (lat_df["next_time"] - lat_df["time_seconds"]).clip(lower=0.0).astype(float)
        latency_agg = (
            lat_df.groupby([match_col, player_col], as_index=False)["pass_to_shot_latency"]
            .agg(pass_to_shot_latency_mean=("mean"), pass_to_shot_latency_min=("min"))
        )

    return pass_geom_agg, latency_agg


def build_xpass_risk_features(
    actions: pd.DataFrame,
    match_col: str = "match_id",
    player_col: str = "player_id",
    type_col: str = "type_name",
    result_col: str = "result_name",
) -> pd.DataFrame:
    """xPassãƒ™ãƒ¼ã‚¹ã®ãƒªã‚¹ã‚¯/å‰µé€ æ€§æŒ‡æ¨™ã‚’é›†è¨ˆã—ã¦è¿”ã™ã€‚

    - risk_creativity_sum = Î£( (1 - xpass_prob) * xt_learned_delta_on_success )
    - xpass_mean, xpass_min
    - pass_success_minus_xpass = å®ŸæˆåŠŸç‡ - xpass_mean
    - xpass_deep_mean (end_x >= 70), xpass_box_mean (end_x >= 88 & 14<=y<=54)
    """
    if ("xpass_prob" not in actions.columns) or (type_col not in actions.columns):
        return pd.DataFrame(columns=[match_col, player_col])

    p = actions[actions[type_col] == "pass"].copy()
    if p.empty:
        return pd.DataFrame(columns=[match_col, player_col, "risk_creativity_sum", "xpass_mean", "xpass_min", "pass_success_minus_xpass", "xpass_deep_mean", "xpass_box_mean"])  # empty

    p["xpass_prob"] = p["xpass_prob"].astype(float)
    if "xt_learned_delta_on_success" in p.columns:
        delta_on_success = p["xt_learned_delta_on_success"].fillna(0.0).astype(float)
    elif "xt_learned_delta" in p.columns:
        # æˆåŠŸæ™‚ã®ã¿ä¾¡å€¤å‰é€²ã¨ã¿ãªã™
        success_flag = p[result_col].eq("success").astype(int)
        delta_on_success = np.where(success_flag == 1, p["xt_learned_delta"].fillna(0.0), 0.0)
    else:
        delta_on_success = 0.0

    p_success = p[result_col].eq("success").astype(float)
    p["risk_creativity_term"] = (1.0 - p["xpass_prob"]) * delta_on_success

    # æ·±ã„ä½ç½®ã®åˆ¤å®š
    deep_mask = p["end_x"].ge(70.0)
    box_mask = p["end_x"].ge(88.0) & p["end_y"].between(14.0, 54.0)

    agg = (
        p.groupby([match_col, player_col])
        .agg(
            risk_creativity_sum=("risk_creativity_term", "sum"),
            xpass_mean=("xpass_prob", "mean"),
            xpass_min=("xpass_prob", "min"),
            empirical_success=(p_success.name, "mean"),
            xpass_deep_mean=(pd.Series(p.loc[deep_mask, "xpass_prob"]).groupby([p.loc[deep_mask, match_col], p.loc[deep_mask, player_col]]).transform("mean") if deep_mask.any() else ("xpass_prob", "mean")),
        )
        .reset_index()
    )

    # xpass_deep_mean ã®è¨ˆç®—ã‚’å®‰å®šåŒ–ï¼ˆä¸Šã®è¤‡é›‘ãªå¼ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    if "xpass_deep_mean" not in agg.columns or agg["xpass_deep_mean"].isna().all():
        deep = p.loc[deep_mask, [match_col, player_col, "xpass_prob"]].copy()
        if not deep.empty:
            deep_agg = deep.groupby([match_col, player_col], as_index=False)["xpass_prob"].mean().rename(columns={"xpass_prob": "xpass_deep_mean"})
            agg = agg.merge(deep_agg, on=[match_col, player_col], how="left")
        else:
            agg["xpass_deep_mean"] = np.nan

    box = p.loc[box_mask, [match_col, player_col, "xpass_prob"]].copy()
    if not box.empty:
        box_agg = box.groupby([match_col, player_col], as_index=False)["xpass_prob"].mean().rename(columns={"xpass_prob": "xpass_box_mean"})
        agg = agg.merge(box_agg, on=[match_col, player_col], how="left")
    else:
        agg["xpass_box_mean"] = np.nan

    agg["pass_success_minus_xpass"] = agg["empirical_success"].fillna(0.0) - agg["xpass_mean"].fillna(0.0)
    agg = agg.drop(columns=["empirical_success"], errors="ignore")

    for c in ["risk_creativity_sum", "xpass_mean", "xpass_min", "pass_success_minus_xpass", "xpass_deep_mean", "xpass_box_mean"]:
        if c in agg.columns:
            agg[c] = agg[c].fillna(0.0)
    return agg


def add_player_trend(
    train_df: pd.DataFrame,
    test_df: pd.DataFrame,
    date_col: str = "Date",
    group_key: str = "player_id",
    target_col: str = "xAG",
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """é¸æ‰‹ã®æ™‚ç³»åˆ—ãƒ•ã‚©ãƒ¼ãƒ ã‚’ãƒªãƒ¼ã‚¯å¯¾ç­–ä»˜ãã§ä»˜ä¸ï¼ˆexpanding/rolling3/diffï¼‰ã€‚

    FIX ISSUE 2: Time series leakage prevention
    - For train: compute statistics using ONLY train data (chronologically sorted)
    - For test: compute statistics using train history only (no test data)
    - Ensures test rows never contribute to train features, even when test matches
      occur chronologically before some training matches
    """
    def _safe_series(x: pd.Series) -> pd.Series:
        if x.dtype.kind in {"f", "i"}:
            return x
        return pd.to_numeric(x, errors="coerce")

    # ============================================================
    # Process training data: compute time series features from train data only
    # ============================================================
    train_copy = train_df.copy()
    train_copy[date_col] = pd.to_datetime(train_copy[date_col], errors="coerce")
    train_copy = train_copy.sort_values([date_col, "match_id"], kind="mergesort").copy()

    # Compute expanding/rolling statistics on TRAIN data only (with shift for leak prevention)
    train_grp = train_copy.groupby(group_key)
    train_copy[f"{target_col}_expanding_mean"] = (
        train_grp[target_col]
        .apply(lambda s: _safe_series(s).expanding().mean())
        .reset_index(level=0, drop=True)
        .shift(1)
    )
    train_copy[f"{target_col}_rolling3_mean"] = (
        train_grp[target_col]
        .apply(lambda s: _safe_series(s).rolling(3, min_periods=1).mean())
        .reset_index(level=0, drop=True)
        .shift(1)
    )
    train_copy[f"{target_col}_diff_prev"] = train_grp[target_col].diff().shift(0)

    # ============================================================
    # Process test data: compute features using ONLY train history
    # ============================================================
    test_copy = test_df.copy()
    test_copy[date_col] = pd.to_datetime(test_copy[date_col], errors="coerce")

    # Initialize test features
    test_copy[f"{target_col}_expanding_mean"] = np.nan
    test_copy[f"{target_col}_rolling3_mean"] = np.nan
    test_copy[f"{target_col}_diff_prev"] = np.nan

    # For each test player, use their train history to compute statistics
    for player in test_copy[group_key].unique():
        # Get all train data for this player (sorted chronologically)
        player_train_hist = train_copy[train_copy[group_key] == player].copy()

        if player_train_hist.empty:
            # No training history for this player - leave as NaN
            continue

        # Get test rows for this player
        test_player_mask = test_copy[group_key] == player
        test_player_rows = test_copy[test_player_mask].copy()

        if test_player_rows.empty:
            continue

        # For each test match, compute statistics using only train data up to that point
        # (but this is conservative - we use ALL train history regardless of date)
        # This is safe because test data never contributes to features

        # Use the final expanding mean from training data
        if len(player_train_hist) > 0:
            final_expanding_mean = _safe_series(player_train_hist[target_col]).expanding().mean().iloc[-1]
            test_copy.loc[test_player_mask, f"{target_col}_expanding_mean"] = final_expanding_mean

            # Rolling mean uses last 3 train values
            if len(player_train_hist) >= 3:
                final_rolling_mean = _safe_series(player_train_hist[target_col]).iloc[-3:].mean()
            else:
                final_rolling_mean = _safe_series(player_train_hist[target_col]).mean()
            test_copy.loc[test_player_mask, f"{target_col}_rolling3_mean"] = final_rolling_mean

            # Diff: difference from last train value
            last_train_value = _safe_series(player_train_hist[target_col]).iloc[-1]
            test_copy.loc[test_player_mask, f"{target_col}_diff_prev"] = (
                _safe_series(test_player_rows[target_col]) - last_train_value
            )

    # Fill remaining NaN with 0 for players without train history
    for col in [f"{target_col}_expanding_mean", f"{target_col}_rolling3_mean", f"{target_col}_diff_prev"]:
        train_copy[col] = train_copy[col].fillna(0.0)
        test_copy[col] = test_copy[col].fillna(0.0)

    # Validation assertions
    assert f"{target_col}_expanding_mean" in train_copy.columns, "Train expanding_mean not created"
    assert f"{target_col}_expanding_mean" in test_copy.columns, "Test expanding_mean not created"
    assert len(train_copy) == len(train_df), "Train row count mismatch"
    assert len(test_copy) == len(test_df), "Test row count mismatch"

    return train_copy, test_copy


def build_time_based_features(
    actions: pd.DataFrame,
    match_col: str = "match_id",
    player_col: str = "player_id",
    time_col: str = "time_seconds",
    period_col: str = "period_id",
) -> pd.DataFrame:
    """æ™‚é–“å¸¯åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç‰¹å¾´é‡ã‚’æ§‹ç¯‰ã€‚

    Returns: è©¦åˆÃ—é¸æ‰‹åˆ¥ã®æ™‚é–“å¸¯åˆ¥çµ±è¨ˆ
    - first_half_actions: å‰åŠã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°
    - second_half_actions: å¾ŒåŠã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°
    - final_15min_actions: ãƒ©ã‚¹ãƒˆ15åˆ†ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°
    - early_10min_actions: åºç›¤10åˆ†ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°
    - time_weighted_intensity: æ™‚é–“é‡ã¿ä»˜ãå¼·åº¦(å¾ŒåŠã»ã©é‡è¦)
    """
    sa = _sort_actions(actions, match_col)

    # å‰åŠ/å¾ŒåŠã®åˆ¤å®š
    first_half = (sa[period_col] == 1)
    second_half = (sa[period_col] == 2)

    # ãƒ©ã‚¹ãƒˆ15åˆ†: å¾ŒåŠ30åˆ†ä»¥é™ (time_seconds >= 2700)
    final_15min = second_half & (sa[time_col] >= 2700)

    # åºç›¤10åˆ†: å‰åŠ0-600ç§’
    early_10min = first_half & (sa[time_col] <= 600)

    # æ™‚é–“é‡ã¿ä»˜ã (0.0-1.0ã®æ­£è¦åŒ–æ™‚é–“ Ã— ã‚¢ã‚¯ã‚·ãƒ§ãƒ³é‡è¦åº¦)
    sa["time_weight"] = np.where(
        first_half,
        (sa[time_col] / 2700.0) * 0.5,  # å‰åŠã¯0.0-0.5ã®é‡ã¿
        0.5 + (sa[time_col] / 2700.0) * 0.5  # å¾ŒåŠã¯0.5-1.0ã®é‡ã¿
    )

    agg = (
        sa.assign(
            first_half=first_half.astype(int),
            second_half=second_half.astype(int),
            final_15min=final_15min.astype(int),
            early_10min=early_10min.astype(int),
        )
        .groupby([match_col, player_col], as_index=False)
        .agg(
            first_half_actions=("first_half", "sum"),
            second_half_actions=("second_half", "sum"),
            final_15min_actions=("final_15min", "sum"),
            early_10min_actions=("early_10min", "sum"),
            time_weighted_intensity=("time_weight", "sum"),
        )
    )

    return agg


def build_zone_based_features(
    actions: pd.DataFrame,
    match_col: str = "match_id",
    player_col: str = "player_id",
) -> pd.DataFrame:
    """ã‚¾ãƒ¼ãƒ³åˆ¥ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å¯†åº¦ç‰¹å¾´é‡ã‚’æ§‹ç¯‰ã€‚

    ãƒ”ãƒƒãƒã‚’9åˆ†å‰²(3x3)ã—ã¦ã‚¾ãƒ¼ãƒ³åˆ¥çµ±è¨ˆã‚’ç®—å‡º:
    - defensive_zone_actions: è‡ªé™£ã‚¾ãƒ¼ãƒ³(x < 35)
    - middle_zone_actions: ä¸­ç›¤ã‚¾ãƒ¼ãƒ³(35 <= x < 70)
    - attacking_zone_actions: æ•µé™£ã‚¾ãƒ¼ãƒ³(x >= 70)
    - halfspace_left_actions: å·¦ãƒãƒ¼ãƒ•ã‚¹ãƒšãƒ¼ã‚¹(y < 22.67)
    - halfspace_right_actions: å³ãƒãƒ¼ãƒ•ã‚¹ãƒšãƒ¼ã‚¹(y > 45.33)
    - central_corridor_actions: ä¸­å¤®ãƒ¬ãƒ¼ãƒ³(22.67 <= y <= 45.33)
    - final_third_penetrations: æ•µé™£æœ€çµ‚ãƒ©ã‚¤ãƒ³é€²å…¥(x >= 70)
    - box_entries: ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚¨ãƒªã‚¢é€²å…¥(x >= 88.5, 13.84 <= y <= 54.16)
    """
    sa = actions.copy()

    # Xè»¸ã‚¾ãƒ¼ãƒ³åˆ†å‰² (0-105ã‚’3åˆ†å‰²)
    defensive_zone = (sa["start_x"] < 35.0)
    middle_zone = (sa["start_x"] >= 35.0) & (sa["start_x"] < 70.0)
    attacking_zone = (sa["start_x"] >= 70.0)

    # Yè»¸ã‚¾ãƒ¼ãƒ³åˆ†å‰² (0-68ã‚’3åˆ†å‰²)
    halfspace_left = (sa["start_y"] < 22.67)
    central_corridor = (sa["start_y"] >= 22.67) & (sa["start_y"] <= 45.33)
    halfspace_right = (sa["start_y"] > 45.33)

    # ç‰¹æ®Šã‚¾ãƒ¼ãƒ³
    final_third = (sa["start_x"] >= 70.0)
    penalty_box = (sa["start_x"] >= 88.5) & (sa["start_y"] >= 13.84) & (sa["start_y"] <= 54.16)

    agg = (
        sa.assign(
            defensive_zone=defensive_zone.astype(int),
            middle_zone=middle_zone.astype(int),
            attacking_zone=attacking_zone.astype(int),
            halfspace_left=halfspace_left.astype(int),
            halfspace_right=halfspace_right.astype(int),
            central_corridor=central_corridor.astype(int),
            final_third=final_third.astype(int),
            box_entry=penalty_box.astype(int),
        )
        .groupby([match_col, player_col], as_index=False)
        .agg(
            defensive_zone_actions=("defensive_zone", "sum"),
            middle_zone_actions=("middle_zone", "sum"),
            attacking_zone_actions=("attacking_zone", "sum"),
            halfspace_left_actions=("halfspace_left", "sum"),
            halfspace_right_actions=("halfspace_right", "sum"),
            central_corridor_actions=("central_corridor", "sum"),
            final_third_penetrations=("final_third", "sum"),
            box_entries=("box_entry", "sum"),
        )
    )

    return agg


def build_pass_network_centrality(
    actions: pd.DataFrame,
    match_col: str = "match_id",
    player_col: str = "player_id",
    team_col: str = "team_id",
    type_col: str = "type_name",
    time_col: str = "time_seconds",
) -> pd.DataFrame:
    """ãƒ‘ã‚¹ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¸­å¿ƒæ€§ç‰¹å¾´é‡ã‚’æ§‹ç¯‰ (é«˜é€Ÿç‰ˆ)ã€‚

    ã‚°ãƒ©ãƒ•ç†è«–ãƒ™ãƒ¼ã‚¹ã®é¸æ‰‹å½¹å‰²è©•ä¾¡:
    - betweenness_centrality: åª’ä»‹ä¸­å¿ƒæ€§(æ”»æ’ƒã®ä¸­ç¶™ç‚¹åº¦)
    - closeness_centrality: è¿‘æ¥ä¸­å¿ƒæ€§(æ”»æ’ƒã¸ã®è¿‘ã•)
    - degree_centrality: æ¬¡æ•°ä¸­å¿ƒæ€§(ãƒ‘ã‚¹æ¥ç¶šæ•°)
    - pass_receiver_diversity: ãƒ‘ã‚¹å…ˆã®å¤šæ§˜æ€§(ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼)
    - unique_pass_partners: ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ‘ã‚¹ç›¸æ‰‹æ•°
    """
    try:
        import networkx as nx
    except ImportError:
        # NetworkXãŒãªã„å ´åˆã¯ç©ºã®DataFrameã‚’è¿”ã™
        return pd.DataFrame(columns=[match_col, player_col])

    sa = _sort_actions(actions, match_col)

    if sa.empty:
        return pd.DataFrame(columns=[match_col, player_col,
                                    "betweenness_centrality", "closeness_centrality",
                                    "degree_centrality", "pass_receiver_diversity",
                                    "unique_pass_partners"])

    results = []

    # ãƒ‘ã‚¹ã®ã¿æŠ½å‡º
    passes = sa[sa[type_col] == "pass"].copy()

    if passes.empty:
        return pd.DataFrame(columns=[match_col, player_col,
                                    "betweenness_centrality", "closeness_centrality",
                                    "degree_centrality", "pass_receiver_diversity",
                                    "unique_pass_partners"])

    # ğŸš€ é«˜é€ŸåŒ–: æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®é¸æ‰‹ã‚’äº‹å‰è¨ˆç®—ï¼ˆãƒ‘ã‚¹ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ï¼‰
    passes["next_player"] = passes.groupby([match_col, team_col])[player_col].shift(-1)

    for (match_id, team_id), group in passes.groupby([match_col, team_col]):
        G = nx.DiGraph()

        # ğŸš€ é«˜é€ŸåŒ–: ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã§ã‚°ãƒ©ãƒ•æ§‹ç¯‰
        # ãƒ‘ã‚¹ã®é€ã‚Šæ‰‹ã¨æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®é¸æ‰‹ï¼ˆå—ã‘æ‰‹ï¼‰ã§ã‚¨ãƒƒã‚¸ä½œæˆ
        pass_edges = group[[player_col, "next_player"]].dropna()
        pass_edges = pass_edges[pass_edges[player_col] != pass_edges["next_player"]]

        if pass_edges.empty:
            continue

        # ã‚¨ãƒƒã‚¸ã‚’ä¸€æ‹¬è¿½åŠ 
        edges = list(zip(pass_edges[player_col], pass_edges["next_player"]))
        G.add_edges_from(edges)

        if G.number_of_nodes() == 0:
            continue

        # ä¸­å¿ƒæ€§è¨ˆç®—
        try:
            betweenness = nx.betweenness_centrality(G)
            closeness = nx.closeness_centrality(G)
            degree = nx.degree_centrality(G)
        except:
            betweenness = {n: 0.0 for n in G.nodes()}
            closeness = {n: 0.0 for n in G.nodes()}
            degree = {n: 0.0 for n in G.nodes()}

        # ğŸš€ é«˜é€ŸåŒ–: ãƒ‘ã‚¹å…ˆã®å¤šæ§˜æ€§ã‚’ä¸€æ‹¬è¨ˆç®—
        pass_receiver_stats = (
            pass_edges.groupby(player_col)["next_player"]
            .agg(lambda x: len(set(x)))  # unique partners
            .to_dict()
        )

        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—
        diversity_dict = {}
        for passer in pass_receiver_stats.keys():
            receivers = pass_edges[pass_edges[player_col] == passer]["next_player"]
            if len(receivers) > 0:
                receiver_counts = receivers.value_counts(normalize=True)
                diversity_dict[passer] = -np.sum(receiver_counts * np.log2(receiver_counts + 1e-9))
            else:
                diversity_dict[passer] = 0.0

        # å„é¸æ‰‹ã®çµ±è¨ˆã‚’æ§‹ç¯‰
        for player_id in G.nodes():
            results.append({
                match_col: match_id,
                player_col: player_id,
                "betweenness_centrality": betweenness.get(player_id, 0.0),
                "closeness_centrality": closeness.get(player_id, 0.0),
                "degree_centrality": degree.get(player_id, 0.0),
                "pass_receiver_diversity": diversity_dict.get(player_id, 0.0),
                "unique_pass_partners": pass_receiver_stats.get(player_id, 0),
            })

    return pd.DataFrame(results)


def build_extended_chain_features(
    actions: pd.DataFrame,
    match_col: str = "match_id",
    player_col: str = "player_id",
    team_col: str = "team_id",
    type_col: str = "type_name",
    n_steps: int = 7,
    gamma: float = 0.6,
) -> pd.DataFrame:
    """æ‹¡å¼µã‚·ãƒ¼ã‚±ãƒ³ã‚¹é€£é–ç‰¹å¾´é‡ (5-10æ‰‹å…ˆã¾ã§)ã€‚

    nstep_chain_featuresã®æ‹¡å¼µç‰ˆã§ã€ã‚ˆã‚Šé•·ã„é€£é–ã‚’è©•ä¾¡:
    - longchain_to_shot: 7æ‰‹å…ˆã¾ã§ã®ã‚·ãƒ¥ãƒ¼ãƒˆåˆ°é”å‰²å¼•å’Œ
    - longchain_xt_delta: 7æ‰‹å…ˆã¾ã§ã®xTå¢—åŠ å‰²å¼•å’Œ
    """
    sa = _sort_actions(actions, match_col)

    # æ¬¡ã®Næ‰‹ã® type / team ã‚’ç”¨æ„
    for k in range(1, n_steps + 1):
        sa[f"next{k}_type"] = sa.groupby(match_col)[type_col].shift(-k)
        sa[f"next{k}_team"] = sa.groupby(match_col)[team_col].shift(-k)

    # é•·æœŸé€£é–ã®å‰²å¼•å’Œ
    weights = {k: (gamma ** (k - 1)) for k in range(1, n_steps + 1)}
    contrib = np.zeros(len(sa), dtype=float)
    for k in range(1, n_steps + 1):
        mask_k = (sa[f"next{k}_team"] == sa[team_col]) & (sa[f"next{k}_type"].isin(SHOT_TYPES))
        contrib += weights[k] * mask_k.astype(float)

    # xtãƒ‡ãƒ«ã‚¿ã®å‰²å¼•å’Œï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
    if "xt_learned_delta" in sa.columns:
        xt_contrib = np.zeros(len(sa), dtype=float)
        for k in range(1, n_steps + 1):
            future_same_team = (sa[f"next{k}_team"] == sa[team_col])
            future_xt = sa.groupby(match_col)["xt_learned_delta"].shift(-k)
            xt_contrib += weights[k] * np.where(future_same_team, future_xt.fillna(0.0), 0.0)
    else:
        xt_contrib = np.zeros(len(sa), dtype=float)

    out = (
        sa.assign(longchain_to_shot=contrib, longchain_xt_delta=xt_contrib)
        .groupby([match_col, player_col], as_index=False)[["longchain_to_shot", "longchain_xt_delta"]]
        .sum()
    )
    return out


def build_dynamic_positioning_features(
    actions: pd.DataFrame,
    match_col: str = "match_id",
    player_col: str = "player_id",
) -> pd.DataFrame:
    """å‹•çš„ãƒã‚¸ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ç‰¹å¾´é‡ã‚’æ§‹ç¯‰ã€‚

    ä½ç½®ã®åˆ†æ•£ãƒ»ç§»å‹•ç¯„å›²ã‚’è©•ä¾¡:
    - position_variance_x: å‰å¾Œæ–¹å‘ã®æ´»å‹•ç¯„å›²(åˆ†æ•£)
    - position_variance_y: å·¦å³æ–¹å‘ã®æ´»å‹•ç¯„å›²(åˆ†æ•£)
    - position_range_x: å‰å¾Œæ–¹å‘ã®æœ€å¤§ç§»å‹•è·é›¢
    - position_range_y: å·¦å³æ–¹å‘ã®æœ€å¤§ç§»å‹•è·é›¢
    - avg_action_distance: ã‚¢ã‚¯ã‚·ãƒ§ãƒ³é–“å¹³å‡ç§»å‹•è·é›¢
    """
    sa = _sort_actions(actions, match_col)

    # ç§»å‹•è·é›¢è¨ˆç®—
    sa["prev_x"] = sa.groupby([match_col, player_col])["start_x"].shift(1)
    sa["prev_y"] = sa.groupby([match_col, player_col])["start_y"].shift(1)
    sa["move_dist"] = np.hypot(
        sa["start_x"] - sa["prev_x"].fillna(sa["start_x"]),
        sa["start_y"] - sa["prev_y"].fillna(sa["start_y"])
    )

    agg = (
        sa.groupby([match_col, player_col], as_index=False)
        .agg(
            position_variance_x=("start_x", "var"),
            position_variance_y=("start_y", "var"),
            position_range_x=("start_x", lambda x: x.max() - x.min() if len(x) > 1 else 0),
            position_range_y=("start_y", lambda x: x.max() - x.min() if len(x) > 1 else 0),
            avg_action_distance=("move_dist", "mean"),
        )
    )

    # NaNåŸ‹ã‚
    for col in agg.columns:
        if col not in [match_col, player_col]:
            agg[col] = agg[col].fillna(0.0)

    return agg


def merge_blocks(
    base: pd.DataFrame,
    blocks: Iterable[pd.DataFrame],
    match_col: str = "match_id",
    player_col: str = "player_id",
    how: str = "left",
) -> pd.DataFrame:
    df = base.copy()
    for b in blocks:
        if b is None or b.empty:
            continue
        df = df.merge(b, on=[match_col, player_col], how=how)
    return df


