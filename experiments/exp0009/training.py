# ---
# jupyter:
#   colab:
#     provenance: []
#   jupytext:
#     cell_metadata_filter: all
#     notebook_metadata_filter: all,-jupytext.text_representation.jupytext_version
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#   kernelspec:
#     display_name: Python 3 (ipykernel)
#     language: python
#     name: python3
#   language_info:
#     codemirror_mode:
#       name: ipython
#       version: 3
#     file_extension: .py
#     mimetype: text/x-python
#     name: python
#     nbconvert_exporter: python
#     pygments_lexer: ipython3
#     version: 3.11.13
#   widgets:
#     application/vnd.jupyter.widget-state+json:
#       09a43976bef243599b2e9b64cffb91b4:
#         model_module: '@jupyter-widgets/controls'
#         model_module_version: 1.5.0
#         model_name: HBoxModel
#         state:
#           _dom_classes: []
#           _model_module: '@jupyter-widgets/controls'
#           _model_module_version: 1.5.0
#           _model_name: HBoxModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/controls'
#           _view_module_version: 1.5.0
#           _view_name: HBoxView
#           box_style: ''
#           children:
#           - IPY_MODEL_d3e34a9bb0d24577acc83d7ae4b2486d
#           - IPY_MODEL_dcffd72448ea44bf9ec8aec5b31762f6
#           - IPY_MODEL_3a83f4956d7d41938ef54f146a5cc541
#           layout: IPY_MODEL_69216dc0f0f4422e8166c4a8f8abe666
#       09a7b0f32ef942afb2963f355e80a8bc:
#         model_module: '@jupyter-widgets/controls'
#         model_module_version: 1.5.0
#         model_name: DescriptionStyleModel
#         state:
#           _model_module: '@jupyter-widgets/controls'
#           _model_module_version: 1.5.0
#           _model_name: DescriptionStyleModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/base'
#           _view_module_version: 1.2.0
#           _view_name: StyleView
#           description_width: ''
#       3a83f4956d7d41938ef54f146a5cc541:
#         model_module: '@jupyter-widgets/controls'
#         model_module_version: 1.5.0
#         model_name: HTMLModel
#         state:
#           _dom_classes: []
#           _model_module: '@jupyter-widgets/controls'
#           _model_module_version: 1.5.0
#           _model_name: HTMLModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/controls'
#           _view_module_version: 1.5.0
#           _view_name: HTMLView
#           description: ''
#           description_tooltip: null
#           layout: IPY_MODEL_5b6ef11c003d4a79b3924bc396c0ff56
#           placeholder: "\u200B"
#           style: IPY_MODEL_78773c5f4ebc47fea4077d687ab0ea0c
#           value: "\u200740041/40041\u2007[03:10&lt;00:00,\u2007234.12it/s]"
#       5b6ef11c003d4a79b3924bc396c0ff56:
#         model_module: '@jupyter-widgets/base'
#         model_module_version: 1.2.0
#         model_name: LayoutModel
#         state:
#           _model_module: '@jupyter-widgets/base'
#           _model_module_version: 1.2.0
#           _model_name: LayoutModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/base'
#           _view_module_version: 1.2.0
#           _view_name: LayoutView
#           align_content: null
#           align_items: null
#           align_self: null
#           border: null
#           bottom: null
#           display: null
#           flex: null
#           flex_flow: null
#           grid_area: null
#           grid_auto_columns: null
#           grid_auto_flow: null
#           grid_auto_rows: null
#           grid_column: null
#           grid_gap: null
#           grid_row: null
#           grid_template_areas: null
#           grid_template_columns: null
#           grid_template_rows: null
#           height: null
#           justify_content: null
#           justify_items: null
#           left: null
#           margin: null
#           max_height: null
#           max_width: null
#           min_height: null
#           min_width: null
#           object_fit: null
#           object_position: null
#           order: null
#           overflow: null
#           overflow_x: null
#           overflow_y: null
#           padding: null
#           right: null
#           top: null
#           visibility: null
#           width: null
#       5e50717143ae4771b600c2a5f73488e2:
#         model_module: '@jupyter-widgets/controls'
#         model_module_version: 1.5.0
#         model_name: ProgressStyleModel
#         state:
#           _model_module: '@jupyter-widgets/controls'
#           _model_module_version: 1.5.0
#           _model_name: ProgressStyleModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/base'
#           _view_module_version: 1.2.0
#           _view_name: StyleView
#           bar_color: null
#           description_width: ''
#       69216dc0f0f4422e8166c4a8f8abe666:
#         model_module: '@jupyter-widgets/base'
#         model_module_version: 1.2.0
#         model_name: LayoutModel
#         state:
#           _model_module: '@jupyter-widgets/base'
#           _model_module_version: 1.2.0
#           _model_name: LayoutModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/base'
#           _view_module_version: 1.2.0
#           _view_name: LayoutView
#           align_content: null
#           align_items: null
#           align_self: null
#           border: null
#           bottom: null
#           display: null
#           flex: null
#           flex_flow: null
#           grid_area: null
#           grid_auto_columns: null
#           grid_auto_flow: null
#           grid_auto_rows: null
#           grid_column: null
#           grid_gap: null
#           grid_row: null
#           grid_template_areas: null
#           grid_template_columns: null
#           grid_template_rows: null
#           height: null
#           justify_content: null
#           justify_items: null
#           left: null
#           margin: null
#           max_height: null
#           max_width: null
#           min_height: null
#           min_width: null
#           object_fit: null
#           object_position: null
#           order: null
#           overflow: null
#           overflow_x: null
#           overflow_y: null
#           padding: null
#           right: null
#           top: null
#           visibility: null
#           width: null
#       78773c5f4ebc47fea4077d687ab0ea0c:
#         model_module: '@jupyter-widgets/controls'
#         model_module_version: 1.5.0
#         model_name: DescriptionStyleModel
#         state:
#           _model_module: '@jupyter-widgets/controls'
#           _model_module_version: 1.5.0
#           _model_name: DescriptionStyleModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/base'
#           _view_module_version: 1.2.0
#           _view_name: StyleView
#           description_width: ''
#       79fcd266aa524e9b9d656b2eab1a8cd3:
#         model_module: '@jupyter-widgets/base'
#         model_module_version: 1.2.0
#         model_name: LayoutModel
#         state:
#           _model_module: '@jupyter-widgets/base'
#           _model_module_version: 1.2.0
#           _model_name: LayoutModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/base'
#           _view_module_version: 1.2.0
#           _view_name: LayoutView
#           align_content: null
#           align_items: null
#           align_self: null
#           border: null
#           bottom: null
#           display: null
#           flex: null
#           flex_flow: null
#           grid_area: null
#           grid_auto_columns: null
#           grid_auto_flow: null
#           grid_auto_rows: null
#           grid_column: null
#           grid_gap: null
#           grid_row: null
#           grid_template_areas: null
#           grid_template_columns: null
#           grid_template_rows: null
#           height: null
#           justify_content: null
#           justify_items: null
#           left: null
#           margin: null
#           max_height: null
#           max_width: null
#           min_height: null
#           min_width: null
#           object_fit: null
#           object_position: null
#           order: null
#           overflow: null
#           overflow_x: null
#           overflow_y: null
#           padding: null
#           right: null
#           top: null
#           visibility: null
#           width: null
#       d3e34a9bb0d24577acc83d7ae4b2486d:
#         model_module: '@jupyter-widgets/controls'
#         model_module_version: 1.5.0
#         model_name: HTMLModel
#         state:
#           _dom_classes: []
#           _model_module: '@jupyter-widgets/controls'
#           _model_module_version: 1.5.0
#           _model_name: HTMLModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/controls'
#           _view_module_version: 1.5.0
#           _view_name: HTMLView
#           description: ''
#           description_tooltip: null
#           layout: IPY_MODEL_79fcd266aa524e9b9d656b2eab1a8cd3
#           placeholder: "\u200B"
#           style: IPY_MODEL_09a7b0f32ef942afb2963f355e80a8bc
#           value: "Calculating\u2007success\u2007rates:\u2007100%"
#       dc043a4a5ed74b1c9b6cc8e027e5b2b6:
#         model_module: '@jupyter-widgets/base'
#         model_module_version: 1.2.0
#         model_name: LayoutModel
#         state:
#           _model_module: '@jupyter-widgets/base'
#           _model_module_version: 1.2.0
#           _model_name: LayoutModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/base'
#           _view_module_version: 1.2.0
#           _view_name: LayoutView
#           align_content: null
#           align_items: null
#           align_self: null
#           border: null
#           bottom: null
#           display: null
#           flex: null
#           flex_flow: null
#           grid_area: null
#           grid_auto_columns: null
#           grid_auto_flow: null
#           grid_auto_rows: null
#           grid_column: null
#           grid_gap: null
#           grid_row: null
#           grid_template_areas: null
#           grid_template_columns: null
#           grid_template_rows: null
#           height: null
#           justify_content: null
#           justify_items: null
#           left: null
#           margin: null
#           max_height: null
#           max_width: null
#           min_height: null
#           min_width: null
#           object_fit: null
#           object_position: null
#           order: null
#           overflow: null
#           overflow_x: null
#           overflow_y: null
#           padding: null
#           right: null
#           top: null
#           visibility: null
#           width: null
#       dcffd72448ea44bf9ec8aec5b31762f6:
#         model_module: '@jupyter-widgets/controls'
#         model_module_version: 1.5.0
#         model_name: FloatProgressModel
#         state:
#           _dom_classes: []
#           _model_module: '@jupyter-widgets/controls'
#           _model_module_version: 1.5.0
#           _model_name: FloatProgressModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/controls'
#           _view_module_version: 1.5.0
#           _view_name: ProgressView
#           bar_style: success
#           description: ''
#           description_tooltip: null
#           layout: IPY_MODEL_dc043a4a5ed74b1c9b6cc8e027e5b2b6
#           max: 40041
#           min: 0
#           orientation: horizontal
#           style: IPY_MODEL_5e50717143ae4771b600c2a5f73488e2
#           value: 40041
# ---

# %% [markdown] id="HU48Dzb4iuhP"
# # xAG予測コンペ　ベースラインコード（その2）
#
# ベースラインコード（host_baseline_001.ipynb）について、特徴量の追加作成やパラメータ最適化を行った改善版コードです。

# %% id="4N4IPuA_J7sw" trusted=true
#第一回はこちら
#https://www.kaggle.com/competitions/dsdojo_1/overview

# %% [markdown] id="cdc2NNOJiuhU"
# ---
# ## セットアップ
#
#

# %% colab={"base_uri": "https://localhost:8080/"} id="GZJyn-MciuhU" outputId="eb7079dd-b265-4b57-fda9-ff61cec466b9" trusted=true
# 必要モジュールでColab環境にないものはinstall
# !pip install japanize_matplotlib

# %% id="km_jW_2YiuhU" trusted=true
# 必要モジュールをimport
import json
from datetime import datetime
from pathlib import Path

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib_venn import venn2
import seaborn as sns
import japanize_matplotlib
import lightgbm as lgb
from sklearn.model_selection import train_test_split, KFold, GroupKFold
from sklearn.isotonic import IsotonicRegression
from tqdm.auto import tqdm
import warnings
warnings.filterwarnings('ignore')

# ランダム性を伴う処理を行うため、結果の再現性を保つにはシード値を固定しておく必要があります
SEED = 42
np.random.seed(SEED)

# %% id="v0L9gXW5iuhU" trusted=true
# 表示できるdfの行、列数を増やす
pd.set_option("display.max_rows", 100)    # 最大100行まで表示
pd.set_option("display.max_columns", 100) # 最大100列まで表示

# %% [markdown] id="29WdPR8riuhV"
# ## データ読み込み

# %% colab={"base_uri": "https://localhost:8080/", "height": 804} id="e6eZzzhRiuhV" outputId="b494a902-ca3a-4cde-d9e9-52c00f7f2c38" trusted=true
# ローカル実行用のパス設定
base_path = '../../data'
print(f"データ読み込み元パス: {base_path}")

# データ読み込み
# player_idやmatch_idの数値的大小に意味はないのでstring形式で読み込み
train_df = pd.read_csv(f"{base_path}/match_train_data.csv", dtype={"player_id": "string", "match_id": "string"})
test_df = pd.read_csv(f"{base_path}/match_test_data.csv", dtype={"player_id": "string", "match_id": "string"})
actions_df = pd.read_csv(f"{base_path}/action_data.csv", dtype={"player_id": "string", "match_id": "string"})
submission_df = pd.read_csv(f"{base_path}/sample_submission.csv")

print(f"trainデータ形状: {train_df.shape}")
display(train_df.head(3))

print(f"\ntestデータ形状: {test_df.shape}")
display(test_df.head(3))

print(f"\nアクションデータ形状: {actions_df.shape}")
display(actions_df.head(3))


# %% [markdown] id="a4Pt7U5biuhV"
# ## 特徴量エンジニアリング - 基本特徴量
#
# まず、001と同じ基本的な特徴量を作成します。

# %% colab={"base_uri": "https://localhost:8080/"} id="F3d_UMBliuhV" outputId="3eec7699-ba5f-4497-855d-8c6c40c55b20" trusted=true
# 所与のデータから簡単に計算できる年齢特徴量を追加する

# 2017/18シーズン終了時点での年齢を計算
train_df['Date'] = pd.to_datetime(train_df['Date'])
train_df['birth_date'] = pd.to_datetime(train_df['birth_date'])
train_df['age'] = (train_df['Date'] - train_df['birth_date']).dt.days / 365.25

test_df['Date'] = pd.to_datetime(test_df['Date'])
test_df['birth_date'] = pd.to_datetime(test_df['birth_date'])
test_df['age'] = (test_df['Date'] - test_df['birth_date']).dt.days / 365.25

print(f"\nマージ後のtrainデータ形状: {train_df.shape}")
print(f"\nマージ後のtestデータ形状: {test_df.shape}")

# %% colab={"base_uri": "https://localhost:8080/"} id="r_urDy8riuhV" outputId="ef6080ba-2f4d-42ed-aea8-35cffd58e420" trusted=true
# アクションデータから試合×選手レベルの特徴量を作成

# train/testに含まれる試合×選手の組み合わせを作成する
target_match_players_train = train_df[['match_id', 'player_id']].drop_duplicates()
target_match_players_test = test_df[['match_id', 'player_id']].drop_duplicates()
target_match_players = pd.concat([target_match_players_train, target_match_players_test]).drop_duplicates()

print(f"分析対象となる試合×選手: {len(target_match_players)}組")

# アクションデータのうち、train/testデータに含まれる試合×選手のアクションのみを抽出
relevant_actions = actions_df.merge(
    target_match_players,
    on=['match_id', 'player_id'],
    how='inner'
)
print(f"抽出されたアクション数: {len(relevant_actions)}件")

# %% colab={"base_uri": "https://localhost:8080/", "height": 395} id="hXudsW7vjuBf" outputId="5dcd2476-bef4-4b52-a970-22614a3b7fd9" trusted=true
# 位置データについては、homeとawayで基準が異なる
# homeの場合は、x=0が自陣ゴールライン、x=105が敵陣ゴールライン、y=0が右サイドライン、y=68が左サイドラインに対応する
# awayでは逆になるため、homeの選手とawayの選手で平均的なx,yの値を比較することができない
display(relevant_actions[(relevant_actions["type_name"] == "shot") & (relevant_actions["result_name"] == "success")][["is_home", "start_x", "start_y", "end_x", "end_y"]].head())

# そこで、位置を標準化するため、awayチームの場合は、x' = 105-x, y' = 68-yに修正する
relevant_actions.loc[relevant_actions['is_home'] == False, 'start_x'] = 105 - relevant_actions.loc[relevant_actions['is_home'] == False, 'start_x']
relevant_actions.loc[relevant_actions['is_home'] == False, 'end_x'] = 105 - relevant_actions.loc[relevant_actions['is_home'] == False, 'end_x']
relevant_actions.loc[relevant_actions['is_home'] == False, 'start_y'] = 68 - relevant_actions.loc[relevant_actions['is_home'] == False, 'start_y']
relevant_actions.loc[relevant_actions['is_home'] == False, 'end_y'] = 68 - relevant_actions.loc[relevant_actions['is_home'] == False, 'end_y']

relevant_actions[(relevant_actions["type_name"] == "shot") & (relevant_actions["result_name"] == "success")][["is_home", "start_x", "start_y", "end_x", "end_y"]].head()

# %% [markdown] id="sQOHu5iUnWLA"
# is_homeの値に関係なく、ゴールした場合end_x=105となっており、位置が標準化されている

# %% colab={"base_uri": "https://localhost:8080/", "height": 162} id="5LCBi5A3joBl" outputId="935dbfa8-dd0f-44c5-e3f9-c2e8d4b26b25" trusted=true
# 基本的な統計特徴量の作成
# groupby()とagg()を組み合わせることで、列ごとに任意の集計方法を指定できる。
match_player_stats = (
    relevant_actions
    .groupby(['match_id', 'player_id'])
    .agg(
        action_count   = ('type_name', 'size'), # アクション数合計
        avg_x          = ('start_x', 'mean'), # 平均ポジション（前後方向）
        avg_y          = ('start_y', 'mean'), # 平均ポジション（左右方向）
        minutes_played = ('minutes_played', 'first')  # 出場時間
    )
    .round(2)
    .reset_index()
)

print(f"作成したデータ形状: {match_player_stats.shape}")
display(match_player_stats.head(3))

# %% colab={"base_uri": "https://localhost:8080/", "height": 162} id="RAZHAVEanr4A" outputId="a989b03e-8c48-472d-9aaf-03b24fac09bf" trusted=true
# ゴール数の集計
# type_nameにshotが含まれて、成功したアクションはゴールになる
is_shot  = relevant_actions['type_name'].isin(['shot', 'shot_freekick', 'shot_penalty'])
is_success = relevant_actions['result_name'].eq('success')
is_goal = (is_shot & is_success).astype(int)

match_player_goals = (
    relevant_actions
    .assign(is_goal=is_goal) # is_goal列を追加
    .groupby(['match_id', 'player_id'], as_index=False)['is_goal']
    .sum() # ゴールであるアクションを合計
    .rename(columns={'is_goal': 'goal_count'})
)

print(f"作成したデータ形状: {match_player_goals.shape}")
display(match_player_goals.head(3))

# %% colab={"base_uri": "https://localhost:8080/", "height": 182} id="ZHhHF5_tiuhV" outputId="dfa2bb40-8395-43e4-c7c8-afe51f28a8dd" trusted=true
# アクションタイプ数の集計
# type_name列の値ごとに数を集計する
action_type_stats = (
    relevant_actions
    .groupby(['match_id', 'player_id', 'type_name'])
    .size()
    .unstack(fill_value=0)  # type_name を列に展開、欠損は0で埋める
    .rename_axis(None, axis=1)
    .add_prefix('type_').add_suffix('_count') # 列名に接頭辞と接尾辞を追加する（type_nameがshotなら「type_shot_count」になる）
    .reset_index()
)

print(f"作成したデータ形状: {action_type_stats.shape}")
display(action_type_stats.head(3))

# %% colab={"base_uri": "https://localhost:8080/"} id="oMpQ9RL1n1h8" outputId="e3a47fbf-85f2-44ee-d77c-2acb13ee8c48" trusted=true
# ベース特徴量をtrain/testへマージ
train_df = (
    train_df
    .merge(match_player_stats, on=['match_id', 'player_id'], how='left')
    .merge(match_player_goals, on=['match_id', 'player_id'], how='left')
    .merge(action_type_stats, on=['match_id', 'player_id'], how='left')
)


test_df = (
    test_df
    .merge(match_player_stats, on=['match_id', 'player_id'], how='left')
    .merge(match_player_goals, on=['match_id', 'player_id'], how='left')
    .merge(action_type_stats, on=['match_id', 'player_id'], how='left')
)


action_type_cols = [col for col in train_df.columns if col.startswith('type_')]
stats_count_cols = ['action_count', 'minutes_played', 'goal_count']

for col in action_type_cols + stats_count_cols:
    if col in train_df.columns:
        train_df[col] = train_df[col].fillna(0)
    if col in test_df.columns:
        test_df[col] = test_df[col].fillna(0)

print(f"ベース特徴量マージ後のtrainデータshape: {train_df.shape}")
print(f"ベース特徴量マージ後のtestデータshape: {test_df.shape}")


# %% [markdown] id="vGTCBG_BiuhV"
# ## 特徴量エンジニアリング - 応用特徴量
#
# ここから、より高度な特徴量を作成していきます。各特徴量の意図と計算方法を詳しく説明します。

# %% colab={"base_uri": "https://localhost:8080/", "height": 232, "referenced_widgets": ["09a43976bef243599b2e9b64cffb91b4", "d3e34a9bb0d24577acc83d7ae4b2486d", "dcffd72448ea44bf9ec8aec5b31762f6", "3a83f4956d7d41938ef54f146a5cc541", "69216dc0f0f4422e8166c4a8f8abe666", "79fcd266aa524e9b9d656b2eab1a8cd3", "09a7b0f32ef942afb2963f355e80a8bc", "dc043a4a5ed74b1c9b6cc8e027e5b2b6", "5e50717143ae4771b600c2a5f73488e2", "5b6ef11c003d4a79b3924bc396c0ff56", "78773c5f4ebc47fea4077d687ab0ea0c"]} id="maM2mFbTiuhV" outputId="6c028ca4-a3f4-41f8-b6f5-79d096d17f19" trusted=true
# アクション成功率特徴量
# アシストに繋がる可能性を評価するため、各種アクションの成功率を計算する

# 成功率を計算するアクションタイプ
action_types_with_result = ['pass', 'shot', 'take_on', 'cross', 'corner_crossed', 'freekick_crossed']  # take_onはドリブルでの仕掛け

success_rates_list = []
print("アクション成功率特徴量を計算中...")

for (match_id, player_id), group in tqdm(relevant_actions.groupby(['match_id', 'player_id']), desc="Calculating success rates"):
    row_data = {'match_id': match_id, 'player_id': player_id}

    for action_type in action_types_with_result:
        type_actions = group[group['type_name'] == action_type] # 対象アクションを抽出

        if len(type_actions) > 0:
            success_count = len(type_actions[type_actions['result_name'] == 'success'])
            total_count = len(type_actions)

            # 成功率を計算
            success_rate = success_count / total_count
            row_data[f'{action_type}_success_rate'] = success_rate
        else:
            # 該当アクションがない場合は0
            row_data[f'{action_type}_success_rate'] = 0

    success_rates_list.append(row_data)

success_rates = pd.DataFrame(success_rates_list)

print(f"作成したデータ形状: {success_rates.shape}")
display(success_rates.head(3))

# %% colab={"base_uri": "https://localhost:8080/", "height": 310} id="yjC2SnrIiuhW" outputId="ac96bf5f-28cf-4e2f-a395-3c52f5b32736" trusted=true
# 位置ベース特徴量
# フィールド上での活動エリアを分析し、攻撃的な選手を識別

print("位置ベース特徴量を計算中...")

# フィールドを3つのエリアに分割（x座標ベース）
def categorize_position(x):
    """x座標からフィールドエリアを判定"""
    if x < 35:
        return 'defensive'  # 守備的エリア
    elif x < 70:
        return 'midfield'   # 中盤エリア
    else:
        return 'attacking'  # 攻撃的エリア

# 各アクションのエリアを判定
relevant_actions['start_zone'] = relevant_actions['start_x'].apply(categorize_position)

# ゾーン別アクション数を集計
zone_actions = (
    relevant_actions
    .pivot_table(
        index=['match_id', 'player_id'],
        columns='start_zone',
        values='period_id',
        aggfunc='count',
        fill_value=0
    )
    .add_prefix('zone_')
    .add_suffix('_actions')
    .reset_index()
)

# 各ゾーンでのアクション比率を計算
zone_actions['total_actions'] = (
    zone_actions.get('zone_defensive_actions', 0) +
    zone_actions.get('zone_midfield_actions', 0) +
    zone_actions.get('zone_attacking_actions', 0)
)

zone_actions['zone_attacking_actions_ratio'] = np.where(
    zone_actions['total_actions'] > 0,
    zone_actions.get('zone_attacking_actions', 0) / zone_actions['total_actions'],
    0
)

zone_actions['zone_midfield_actions_ratio'] = np.where(
    zone_actions['total_actions'] > 0,
    zone_actions.get('zone_midfield_actions', 0) / zone_actions['total_actions'],
    0
)

zone_actions['zone_defensive_actions_ratio'] = np.where(
    zone_actions['total_actions'] > 0,
    zone_actions.get('zone_defensive_actions', 0) / zone_actions['total_actions'],
    0
)
zone_actions = zone_actions.drop(columns=['total_actions'])

print(f"\nゾーン別アクション統計:")
for zone in ['defensive', 'midfield', 'attacking']:
    col_name = f'zone_{zone}_actions'
    if col_name in zone_actions.columns:
        mean_val = zone_actions[col_name].mean()
        print(f"  {zone:10s}エリア: 平均 {mean_val:.1f} アクション")

print(f"\n作成したデータ形状: {zone_actions.shape}")
display(zone_actions.head(3))

# %% colab={"base_uri": "https://localhost:8080/", "height": 219} id="W2m7WIjGiuhW" outputId="87b1e9c5-034e-4850-89e4-65e627031806" trusted=true
# 時間正規化特徴量
# 出場時間による影響を排除し、公平な比較を可能にする

print("時間正規化特徴量を計算中...")

per_minute_features = match_player_stats.copy()

# 全体アクション数の正規化
per_minute_features['action_count_per_minute'] = np.where(
    per_minute_features['minutes_played'] > 0,
    per_minute_features['action_count'] / per_minute_features['minutes_played'],
    0
)

# ゴール数をマージ・ゼロ埋め
per_minute_features = per_minute_features.merge(
    match_player_goals,
    on=['match_id', 'player_id'],
    how='left'
)
per_minute_features['goal_count'] = per_minute_features['goal_count'].fillna(0)

# ゴール数の正規化
per_minute_features['goal_count_per_minute'] = np.where(
    per_minute_features['minutes_played'] > 0,
    per_minute_features['goal_count'] / per_minute_features['minutes_played'],
    0
)

# アクションタイプ別アクション数をマージ・ゼロ埋め
per_minute_features = per_minute_features.merge(
    action_type_stats,
    on=['match_id', 'player_id'],
    how='left'
)
action_type_cols = [col for col in per_minute_features.columns if col.startswith('type_') and col.endswith('_count')] # アクションタイプ別アクション数の列
for col in action_type_cols:
    per_minute_features[col] = per_minute_features[col].fillna(0)

# アクションタイプ別アクション数の正規化
for col in action_type_cols:
    new_col_name = col.replace('_count', '_count_per_minute')
    per_minute_features[new_col_name] = np.where(
        per_minute_features['minutes_played'] > 0,
        per_minute_features[col] / per_minute_features['minutes_played'],
        0
    )

# 新規作成した列のみに絞り込み
per_minute_cols = [col for col in per_minute_features.columns if col.endswith('_per_minute')]
per_minute_features = per_minute_features[['match_id', 'player_id'] + per_minute_cols]

print(f"\n作成したデータ形状: {per_minute_features.shape}")
display(per_minute_features.head(3))

# %% colab={"base_uri": "https://localhost:8080/", "height": 199} id="5m47SqPMiuhW" outputId="5ceb8fa8-cc5a-478a-bb4b-7be3075b35f1" trusted=true
# 攻撃/守備バランス特徴量
# 選手のプレースタイルを定量化し、攻撃的な選手を識別

print("攻撃/守備バランス特徴量を計算中...")

# 攻撃/守備アクションの定義
offensive_actions = ['shot', 'pass', 'cross', 'take_on', 'dribble']
defensive_actions = ['tackle', 'interception', 'clearance']

# 各アクションの分類を付与
def categorize_ad(action):
    if action in offensive_actions:
        return 'offensive'
    elif action in defensive_actions:
        return 'defensive'
    else:
        return None

relevant_actions['action_type'] = relevant_actions['type_name'].apply(categorize_ad)

# 攻守別アクション数を集計
offense_defense_balance = (
    relevant_actions
    .pivot_table(
        index=['match_id', 'player_id'],
        columns='action_type',
        values='period_id',
        aggfunc='count',
        fill_value=0
    )
    .add_prefix('type_')
    .add_suffix('_actions')
    .reset_index()
)

# 攻守バランス指標を計算
offense_defense_balance['total_actions'] = (
    offense_defense_balance.get('type_offensive_actions', 0) +
    offense_defense_balance.get('type_defensive_actions', 0)
)

offense_defense_balance['type_offensive_action_ratio'] = np.where(
    offense_defense_balance['total_actions'] > 0,
    offense_defense_balance['type_offensive_actions'] / offense_defense_balance['total_actions'],
    0
)

offense_defense_balance = offense_defense_balance.drop(columns=['total_actions'])

print(f"\n作成したデータ形状: {offense_defense_balance.shape}")
display(offense_defense_balance.head(3))

# %% colab={"base_uri": "https://localhost:8080/", "height": 199} id="sLKXukn5pIUx" outputId="3c3f0531-8bd0-4e66-9c7e-dacfa3a0b86a" trusted=true
# 時系列要素を加味した特徴量
# xAGの定義を考えると、パスした味方のシュートが多いほどxAGは高くなる
# そこで、次アクションがシュートであるパスの数を選手-試合ごとに集計する

print("次アクションがシュートのパス数を計算中...")

# 直後のアクションタイプをシフトで付与
relevant_actions = relevant_actions.sort_values(['match_id', 'period_id', 'time_seconds'])  # 念の為アクションを時間でソート
relevant_actions["next_type"] = relevant_actions.groupby("match_id")["type_name"].shift(-1)

# pass → shot となっている行を抽出
pass_to_shot = relevant_actions[
    (relevant_actions["type_name"] == "pass") &
    (relevant_actions["next_type"] == "shot")
]

# match_id, player_idごとにカウント
pass_leads_to_shot = (
    pass_to_shot.groupby(["match_id", "player_id"])
    .size()
    .reset_index(name="pass_leads_to_shot")
)

print(f"\n作成したデータ形状: {pass_leads_to_shot.shape}")
display(pass_leads_to_shot.head(3))

# %% trusted=true
# プログレッシブ/ディープ系の特徴量
print("プログレッシブ/ディープ系特徴量を計算中...")

PASS_PROGRESSIVE_TYPES = {"pass", "cross", "freekick_crossed", "corner_crossed"}
CARRY_PROGRESSIVE_TYPES = {"carry", "dribble", "take_on"}

progressive_pass_actions = relevant_actions[
    relevant_actions["type_name"].isin(PASS_PROGRESSIVE_TYPES)
].copy()

if not progressive_pass_actions.empty:
    dx = (progressive_pass_actions["end_x"] - progressive_pass_actions["start_x"]).fillna(0.0)
    dy = (progressive_pass_actions["end_y"] - progressive_pass_actions["start_y"]).fillna(0.0)
else:
    dx = pd.Series(dtype=float)
    dy = pd.Series(dtype=float)

progressive_pass_actions["delta_x"] = dx
progressive_pass_actions["delta_total"] = np.hypot(dx, dy)
progressive_pass_actions["is_completed"] = progressive_pass_actions["result_name"] == "success"

FINAL_THIRD_X = 70.0
DEEP_COMPLETION_X = 85.0
PENALTY_AREA_X = 88.0
PROGRESS_ADVANCE_MIN = 10.0

progressive_pass_actions["is_progressive"] = (
    (progressive_pass_actions["delta_x"] >= PROGRESS_ADVANCE_MIN)
    | (
        (progressive_pass_actions["start_x"] < FINAL_THIRD_X)
        & (progressive_pass_actions["end_x"] >= FINAL_THIRD_X)
    )
    | (progressive_pass_actions["end_x"] >= DEEP_COMPLETION_X)
)

progressive_pass_actions["progressive_attempt"] = progressive_pass_actions["is_progressive"].astype(int)
progressive_pass_actions["progressive_success"] = (
    progressive_pass_actions["is_progressive"] & progressive_pass_actions["is_completed"]
).astype(int)
progressive_pass_actions["progressive_distance"] = np.where(
    progressive_pass_actions["is_progressive"],
    progressive_pass_actions["delta_total"],
    0.0,
)

progressive_pass_actions["is_final_third_entry"] = (
    progressive_pass_actions["is_completed"]
    & (progressive_pass_actions["start_x"] < FINAL_THIRD_X)
    & (progressive_pass_actions["end_x"] >= FINAL_THIRD_X)
)
progressive_pass_actions["is_deep_completion"] = (
    progressive_pass_actions["is_completed"]
    & (progressive_pass_actions["end_x"] >= DEEP_COMPLETION_X)
)
progressive_pass_actions["is_penalty_area_entry"] = (
    progressive_pass_actions["is_completed"]
    & (progressive_pass_actions["end_x"] >= PENALTY_AREA_X)
)

pass_progressive_features = (
    progressive_pass_actions.groupby(["match_id", "player_id"]).agg(
        progressive_pass_count=("progressive_attempt", "sum"),
        progressive_pass_success=("progressive_success", "sum"),
        progressive_pass_distance_total=("progressive_distance", "sum"),
        final_third_entry_count=("is_final_third_entry", "sum"),
        deep_completion_count=("is_deep_completion", "sum"),
        penalty_area_entry_count=("is_penalty_area_entry", "sum"),
    )
    .reset_index()
)

if "progressive_pass_count" in pass_progressive_features:
    pass_progressive_features["progressive_pass_success_rate"] = np.where(
        pass_progressive_features["progressive_pass_count"] > 0,
        pass_progressive_features["progressive_pass_success"]
        / pass_progressive_features["progressive_pass_count"],
        0.0,
    )
    pass_progressive_features["progressive_pass_distance_mean"] = np.where(
        pass_progressive_features["progressive_pass_count"] > 0,
        pass_progressive_features["progressive_pass_distance_total"]
        / pass_progressive_features["progressive_pass_count"],
        0.0,
    )
else:
    pass_progressive_features["progressive_pass_success_rate"] = []
    pass_progressive_features["progressive_pass_distance_mean"] = []

carry_actions = relevant_actions[
    relevant_actions["type_name"].isin(CARRY_PROGRESSIVE_TYPES)
].copy()

if not carry_actions.empty:
    carry_actions["end_x"] = carry_actions["end_x"].fillna(carry_actions["start_x"])
    carry_actions["end_y"] = carry_actions["end_y"].fillna(carry_actions["start_y"])
    carry_dx = (carry_actions["end_x"] - carry_actions["start_x"]).fillna(0.0)
    carry_dy = (carry_actions["end_y"] - carry_actions["start_y"]).fillna(0.0)
    carry_actions["delta_total"] = np.hypot(carry_dx, carry_dy)
    carry_actions["delta_x"] = carry_dx
    carry_actions["is_success"] = carry_actions["result_name"] == "success"
    carry_actions["is_progressive"] = (
        (carry_actions["delta_x"] >= 5.0)
        | (
            (carry_actions["start_x"] < FINAL_THIRD_X)
            & (carry_actions["end_x"] >= FINAL_THIRD_X)
        )
    )
    carry_actions["progressive_carry_attempt"] = carry_actions["is_progressive"].astype(int)
    carry_actions["progressive_carry_success"] = (
        carry_actions["is_progressive"] & carry_actions["is_success"]
    ).astype(int)
    carry_actions["progressive_carry_distance"] = np.where(
        carry_actions["is_progressive"], carry_actions["delta_total"], 0.0
    )

    carry_progressive_features = (
        carry_actions.groupby(["match_id", "player_id"]).agg(
            progressive_carry_count=("progressive_carry_attempt", "sum"),
            progressive_carry_success=("progressive_carry_success", "sum"),
            progressive_carry_distance_total=("progressive_carry_distance", "sum"),
        )
        .reset_index()
    )

    carry_progressive_features["progressive_carry_success_rate"] = np.where(
        carry_progressive_features["progressive_carry_count"] > 0,
        carry_progressive_features["progressive_carry_success"]
        / carry_progressive_features["progressive_carry_count"],
        0.0,
    )
    carry_progressive_features["progressive_carry_distance_mean"] = np.where(
        carry_progressive_features["progressive_carry_count"] > 0,
        carry_progressive_features["progressive_carry_distance_total"]
        / carry_progressive_features["progressive_carry_count"],
        0.0,
    )
else:
    carry_progressive_features = pd.DataFrame(
        columns=[
            "match_id",
            "player_id",
            "progressive_carry_count",
            "progressive_carry_success",
            "progressive_carry_distance_total",
            "progressive_carry_success_rate",
            "progressive_carry_distance_mean",
        ]
    )

progressive_features = pass_progressive_features.merge(
    carry_progressive_features,
    on=["match_id", "player_id"],
    how="outer",
).fillna(0.0)

print(f"作成したプログレッシブ系特徴量: {progressive_features.shape}")
display(progressive_features.head(3))


# %% trusted=true
# xT (Expected Threat) 特徴量
print("xT (Expected Threat) 特徴量を計算中...")

# xTグリッドの定義（簡易版：16x12グリッド）
# Karun Singh の手法に基づく簡易実装
# ピッチサイズ: x=105m, y=68m
# グリッドサイズ: x方向16分割、y方向12分割

# 簡易的なxT値マトリクス（攻撃方向：x軸正方向）
# ゴールに近いほど、中央に近いほど高い値
# 実際のxTは機械学習で計算するが、ここでは経験則ベースの簡易版
xt_grid = np.array([
    # ゾーン1（自陣深い位置）: x=0-13m
    [0.00078, 0.00089, 0.00095, 0.00095, 0.00095, 0.00095, 0.00095, 0.00095, 0.00095, 0.00095, 0.00089, 0.00078],
    # ゾーン2: x=13-26m
    [0.00116, 0.00144, 0.00161, 0.00161, 0.00161, 0.00161, 0.00161, 0.00161, 0.00161, 0.00161, 0.00144, 0.00116],
    # ゾーン3: x=26-39m
    [0.00191, 0.00264, 0.00321, 0.00321, 0.00321, 0.00321, 0.00321, 0.00321, 0.00321, 0.00321, 0.00264, 0.00191],
    # ゾーン4: x=39-52m（中盤）
    [0.00462, 0.00657, 0.00814, 0.00814, 0.00814, 0.00814, 0.00814, 0.00814, 0.00814, 0.00814, 0.00657, 0.00462],
    # ゾーン5: x=52-65m
    [0.01051, 0.01486, 0.01794, 0.01794, 0.01794, 0.01794, 0.01794, 0.01794, 0.01794, 0.01794, 0.01486, 0.01051],
    # ゾーン6: x=65-78m（ファイナルサード手前）
    [0.02132, 0.02842, 0.03251, 0.03251, 0.03251, 0.03251, 0.03251, 0.03251, 0.03251, 0.03251, 0.02842, 0.02132],
    # ゾーン7: x=78-91m（ファイナルサード）
    [0.03627, 0.04759, 0.05372, 0.05372, 0.05372, 0.05372, 0.05372, 0.05372, 0.05372, 0.05372, 0.04759, 0.03627],
    # ゾーン8: x=91-104m（ペナルティエリア周辺）
    [0.06031, 0.07749, 0.08666, 0.08666, 0.08666, 0.08666, 0.08666, 0.08666, 0.08666, 0.08666, 0.07749, 0.06031],
    # ゾーン9: x=104-117m（ペナルティエリア内、簡易版は16分割なので最後まで）
    [0.09821, 0.12363, 0.13704, 0.13704, 0.13704, 0.13704, 0.13704, 0.13704, 0.13704, 0.13704, 0.12363, 0.09821],
])

# 16分割に拡張（線形補間）
# 実際は9x12だが、16x12にするため最後のゾーンを分割
xt_grid_16x12 = np.zeros((16, 12))
for y_idx in range(12):
    # 既存の9ゾーンを16ゾーンに線形補間
    xt_grid_16x12[:9, y_idx] = xt_grid[:, y_idx]
    # 残り7ゾーンは最後のゾーンから線形に増加
    for x_idx in range(9, 16):
        # ゴールに近づくほど価値が高くなる（最大0.25程度）
        factor = 1.2 + (x_idx - 9) * 0.15
        xt_grid_16x12[x_idx, y_idx] = xt_grid[8, y_idx] * factor

# xT値を取得する関数
def get_xt_value(x, y, grid_x_bins=16, grid_y_bins=12):
    """
    座標(x, y)に対応するxT値を返す
    x: 0-105m, y: 0-68m
    """
    if pd.isna(x) or pd.isna(y):
        return 0.0
    
    # グリッドインデックスに変換
    x_idx = int(np.clip(x / 105.0 * grid_x_bins, 0, grid_x_bins - 1))
    y_idx = int(np.clip(y / 68.0 * grid_y_bins, 0, grid_y_bins - 1))
    
    return xt_grid_16x12[x_idx, y_idx]

# パスとキャリーのアクションにxT値を計算
xt_action_types = {"pass", "cross", "carry", "dribble", "free_kick", "corner"}
xt_actions = relevant_actions[relevant_actions["type_name"].isin(xt_action_types)].copy()

if not xt_actions.empty:
    print(f"  - xT計算対象アクション数: {len(xt_actions):,}")
    
    # 開始位置と終了位置のxT値を計算
    xt_actions["xt_start"] = xt_actions.apply(
        lambda row: get_xt_value(row["start_x"], row["start_y"]), axis=1
    )
    xt_actions["xt_end"] = xt_actions.apply(
        lambda row: get_xt_value(row["end_x"], row["end_y"]), axis=1
    )
    
    # ΔxT（脅威の増分）を計算
    # 成功したアクションのみ正の価値、失敗は負の価値
    xt_actions["xt_delta"] = xt_actions["xt_end"] - xt_actions["xt_start"]
    xt_actions["xt_delta_positive"] = xt_actions["xt_delta"].clip(lower=0)  # 正の増分のみ
    
    # 成功/失敗を考慮したxT
    xt_actions["is_successful"] = xt_actions["result_name"] == "success"
    xt_actions["xt_value"] = np.where(
        xt_actions["is_successful"],
        xt_actions["xt_delta"],  # 成功: 実際の増分
        -xt_actions["xt_start"] * 0.3  # 失敗: 開始地点の価値を30%失う
    )
    
    # 選手×試合レベルで集約
    xt_features = xt_actions.groupby(["match_id", "player_id"]).agg({
        "xt_delta": ["sum", "mean", "max"],  # 総増分、平均増分、最大増分
        "xt_delta_positive": ["sum", "mean"],  # 正の増分のみ
        "xt_value": ["sum", "mean"],  # 成功/失敗を考慮した価値
        "xt_start": ["mean", "max"],  # 平均開始位置の価値
    }).reset_index()
    
    # カラム名をフラット化
    xt_features.columns = [
        "match_id", "player_id",
        "xt_delta_sum", "xt_delta_mean", "xt_delta_max",
        "xt_positive_sum", "xt_positive_mean",
        "xt_value_sum", "xt_value_mean",
        "xt_start_mean", "xt_start_max"
    ]
    
    # train/testにマージ
    train_df = train_df.merge(xt_features, on=["match_id", "player_id"], how="left")
    test_df = test_df.merge(xt_features, on=["match_id", "player_id"], how="left")
    
    # 欠損値を0で埋める（xT対象アクションがない選手）
    xt_cols = [col for col in xt_features.columns if col.startswith("xt_")]
    for col in xt_cols:
        train_df[col] = train_df[col].fillna(0.0)
        test_df[col] = test_df[col].fillna(0.0)
    
    print(f"  - 作成したxT特徴量: {len(xt_cols)}個")
    print(f"  - xT特徴量一覧: {xt_cols}")
else:
    print("  - xT計算対象アクションなし")
    xt_cols = []


# %% [markdown] id="hHxhgz7ypDWn"
# さらに深掘りする価値がありそうな取り組み案
# - パスのstart→endの長さやシュートの距離についての特徴量追加
# - 試合x選手単位でなく、選手単位での集計（1試合だけだと調子の波もあるので、通年で見るほうが選手特性は把握しやすいかも）
# - などなど

# %% [markdown] id="A7xHPcUziuhW"
# ## 特徴量の統合
#
# 作成した全ての特徴量を統合し、train/testデータにマージします。

# %% colab={"base_uri": "https://localhost:8080/"} id="oiPaa-Dz6Gvj" outputId="cc523b94-b8ff-47fd-b87c-8ced83eb52a0" trusted=true
# 応用特徴量をtrain/testへマージ
train_df = (
    train_df
    .merge(success_rates, on=['match_id', 'player_id'], how='left')
    .merge(zone_actions, on=['match_id', 'player_id'], how='left')
    .merge(per_minute_features, on=['match_id', 'player_id'], how='left')
    .merge(offense_defense_balance, on=['match_id', 'player_id'], how='left')
    .merge(pass_leads_to_shot, on=['match_id', 'player_id'], how='left')
    .merge(progressive_features, on=['match_id', 'player_id'], how='left')
)

train_df['pass_leads_to_shot'] = train_df['pass_leads_to_shot'].fillna(0)

progressive_cols = [col for col in progressive_features.columns if col not in ['match_id', 'player_id']]
for col in progressive_cols:
    train_df[col] = train_df[col].fillna(0.0)


test_df = (
    test_df
    .merge(success_rates, on=['match_id', 'player_id'], how='left')
    .merge(zone_actions, on=['match_id', 'player_id'], how='left')
    .merge(per_minute_features, on=['match_id', 'player_id'], how='left')
    .merge(offense_defense_balance, on=['match_id', 'player_id'], how='left')
    .merge(pass_leads_to_shot, on=['match_id', 'player_id'], how='left')
    .merge(progressive_features, on=['match_id', 'player_id'], how='left')
)

test_df['pass_leads_to_shot'] = test_df['pass_leads_to_shot'].fillna(0)
for col in progressive_cols:
    test_df[col] = test_df[col].fillna(0.0)

print(f"マージ後のtrainデータshape: {train_df.shape}")
print(f"マージ後のtestデータshape: {test_df.shape}")


# %% [markdown] id="FHzc1BjgZYRR"
# ## クロスバリデーション分割
#
# 初回のbaselineでは、全データをランダムに分割するKFoldを用いましたが、今回はデータ特性に合わせた別の分割方法を行います。
#
# EDAで確認したように、今回はtrainデータとtestデータについては、match_idの重なりはありません。
# すなわち、testデータを予測するときには、これまで見たことのない試合のデータに対して予測をする必要があります。
# この状況をtrainデータ内部でのCross Validationでもなるべく再現することによって、実際のタスクに近い状況で正しい評価が可能になります。
#
# ここでは、GroupKFoldを用いて、trainデータをmatch_idが被らないように5分割します。こうすることで、各foldでのtrainデータとvalidデータのmatch_idが重ならなくなります。
#

# %% colab={"base_uri": "https://localhost:8080/", "height": 977} id="nzaeWhMpbDlr" outputId="f99d35a4-fc96-4bcc-f176-50c7235075ef" trusted=true
# 5分割のGroupKFoldを設定（match_idでグループ化）
gkf = GroupKFold(n_splits=5)
train_df["fold"] = 0  # 0で初期化

# xAG軸のスケールは共通化して見やすくする
x_min, x_max = train_df["xAG"].min(), train_df["xAG"].max()
xAG_vals = np.arange(x_min, x_max + 0.1, 0.1).round(1)

# 図: 各foldごとに 3カラム（Train分布, Val分布, match_idベン図）
fig, axes = plt.subplots(nrows=5, ncols=3, figsize=(18, 18), sharey=False, sharex=False)

for i, (trn_idx, val_idx) in enumerate(gkf.split(train_df, groups=train_df["match_id"])):
    # fold列をセット
    train_df.loc[val_idx, "fold"] = i + 1

    # train/val の xAG 分布を取得（共通スケールにリインデックス）
    trn_counts = (
        train_df.iloc[trn_idx]["xAG"].value_counts().sort_index()
        .reindex(xAG_vals, fill_value=0)
    )
    val_counts = (
        train_df.iloc[val_idx]["xAG"].value_counts().sort_index()
        .reindex(xAG_vals, fill_value=0)
    )

    # 左列: 各foldのtrainデータ分布
    ax_train = axes[i, 0]
    ax_train.bar(trn_counts.index, trn_counts.values, width=0.08, color="steelblue")
    ax_train.set_title(f"Fold {i+1} - Train xAG 分布")
    ax_train.set_xlabel("xAG")
    ax_train.set_ylabel("頻度")

    # 中列: 各foldのvalidationデータ分布
    ax_val = axes[i, 1]
    ax_val.bar(val_counts.index, val_counts.values, width=0.08, color="orange")
    ax_val.set_title(f"Fold {i+1} - Val xAG 分布")
    ax_val.set_xlabel("xAG")
    ax_val.set_ylabel("頻度")

    # 右列: match_idのベン図（Train vs Val）
    ax_venn = axes[i, 2]
    trn_match_ids = set(train_df.iloc[trn_idx]["match_id"])
    val_match_ids = set(train_df.iloc[val_idx]["match_id"])
    v = venn2(
        [trn_match_ids, val_match_ids],
        set_labels=(f"Train match_id (n={len(trn_match_ids)})",
                    f"Val match_id (n={len(val_match_ids)})"),
        ax=ax_venn
    )
    ax_venn.set_title(f"Fold {i+1} - match_id の重なり")

# x軸を共通スケールに揃える（分布図の2カラムに適用）
for i in range(5):
    for j in [0, 1]:
        ax = axes[i, j]
        ax.set_xlim(x_min - 0.05, x_max + 0.05)  # 端を少し余裕持たせる
        ax.set_xticks(xAG_vals[::2])  # ラベルの数を間引き

plt.tight_layout()
plt.show()

# %% [markdown] id="EqDZiepKaIf3"
# ## モデル学習用データ準備

# %% trusted=true
# ターゲットエンコーディング特徴量の作成
print("ターゲットエンコーディング特徴量を作成中...")

# Squad×Opponentの交互作用特徴を作成
train_df["Squad_x_Opponent"] = train_df["Squad"].astype(str) + "_vs_" + train_df["Opponent"].astype(str)
test_df["Squad_x_Opponent"] = test_df["Squad"].astype(str) + "_vs_" + test_df["Opponent"].astype(str)

target_encoding_cols = ["player_id", "Squad", "Opponent", "Squad_x_Opponent"]
global_mean = train_df["xAG"].mean()
smoothing = 10.0
fold_labels = sorted(train_df["fold"].unique())

for col in target_encoding_cols:
    enc_col = f"{col}_target_enc"
    train_df[enc_col] = np.nan

    for fold in fold_labels:
        trn = train_df[train_df["fold"] != fold]
        val_mask = train_df["fold"] == fold

        stats = trn.groupby(col)["xAG"].agg(["sum", "count"])
        stats["encoding"] = (stats["sum"] + global_mean * smoothing) / (stats["count"] + smoothing)

        train_df.loc[val_mask, enc_col] = train_df.loc[val_mask, col].map(stats["encoding"]).fillna(global_mean)

    overall_stats = train_df.groupby(col)["xAG"].agg(["sum", "count"])
    overall_stats["encoding"] = (overall_stats["sum"] + global_mean * smoothing) / (overall_stats["count"] + smoothing)

    test_df[enc_col] = test_df[col].map(overall_stats["encoding"]).fillna(global_mean)

    missing_train = train_df[enc_col].isna().sum()
    missing_test = test_df[enc_col].isna().sum()

    if missing_train > 0:
        train_df.loc[train_df[enc_col].isna(), enc_col] = global_mean
    if missing_test > 0:
        test_df.loc[test_df[enc_col].isna(), enc_col] = global_mean

    print(f"  {col}: train missing {int(missing_train)}, test missing {int(missing_test)}")


# %% colab={"base_uri": "https://localhost:8080/"} id="507qeMbXaIf3" outputId="bc0b39f4-fe13-4491-8f88-71c960669942" trusted=true
# 各特徴量グループの定義
base_features = ["age", "action_count", "avg_x", "avg_y", "minutes_played", "goal_count"]
categorical_features = ["Comp", "Squad", "Venue"]
action_type_features = [col for col in train_df.columns if (col.startswith('type_')) and (col.endswith('_count'))]
success_rate_features = [
    col for col in train_df.columns
    if col.endswith('_success_rate') and not col.startswith('progressive_')
]
zone_features = [col for col in train_df.columns if col.startswith('zone_')]
per_minute_features = [col for col in train_df.columns if col.endswith('_per_minute')]
ad_balance_features = ['type_offensive_actions', 'type_defensive_actions', 'type_offensive_action_ratio']
sequencial_features = ['pass_leads_to_shot']
progressive_feature_cols = [
    col for col in train_df.columns
    if col.startswith('progressive_')
    or col in ['deep_completion_count', 'final_third_entry_count', 'penalty_area_entry_count']
]
xt_cols = [col for col in train_df.columns if col.startswith('xt_')]
target_encoding_features = [f"{col}_target_enc" for col in ["player_id", "Squad", "Opponent", "Squad_x_Opponent"]]

all_features = (
    base_features
    + categorical_features
    + action_type_features
    + success_rate_features
    + zone_features
    + per_minute_features
    + ad_balance_features
    + sequencial_features
    + progressive_feature_cols
    + xt_cols
    + target_encoding_features
)

all_features = list(dict.fromkeys(all_features))

# カテゴリカル変数については、列の型を「category」に変更しておく
for col in categorical_features:
    train_df[col] = train_df[col].astype("category")
    test_df[col] = test_df[col].astype("category")


print(f"  - 使用する特徴量数: {len(all_features)}個")
print(f"  - 基本特徴量: {len(base_features)}個")
print(f"  - カテゴリカル特徴量: {len(categorical_features)}個")
print(f"  - アクション特徴量(type_*_count): {len(action_type_features)}個")
print(f"  - 成功率系: {len(success_rate_features)}個")
print(f"  - ゾーン系: {len(zone_features)}個")
print(f"  - per_minute系: {len(per_minute_features)}個")
print(f"  - 攻守バランス系: {len(ad_balance_features)}個")
print(f"  - 時系列系: {len(sequencial_features)}個")
print(f"  - プログレッシブ系: {len(progressive_feature_cols)}個")
print(f"  - xT系: {len(xt_cols)}個")
print(f"  - ターゲットエンコーディング系: {len(target_encoding_features)}個")



# %% colab={"base_uri": "https://localhost:8080/"} id="sUaVQAMtR2wk" outputId="a5af6982-d1c3-4d7d-f5aa-a51963962df1" trusted=true
# モデル学習用データの作成
X_train = train_df[all_features + ["fold"]]
y_train = train_df["xAG"]
X_test = test_df[all_features]

print(f"\nモデル学習用データ形状: X_train {X_train.shape}, y_train {y_train.shape}, X_test {X_test.shape}")

# %% trusted=true
# 重み付きRMSEで使用するサンプル重みを事前計算
WEIGHTED_RMSE_POS_WEIGHT = 5.0
WEIGHTED_RMSE_THRESHOLD = 0.1
train_sample_weights = pd.Series(
    np.where(y_train >= WEIGHTED_RMSE_THRESHOLD, WEIGHTED_RMSE_POS_WEIGHT, 1.0),
    index=train_df.index,
    name="sample_weight",
)
train_sample_weights.describe()


# %% [markdown] id="cCk52KvKaIf3"
# ## モデル学習（LightGBM）

# %% id="ah17yN7bu-Mu" trusted=true
# コンペの評価指標に合わせた目的関数/評価関数の定義
def weighted_rmse(y_true, y_pred):
    """
    重み付きRMSE評価関数
    コンペの評価指標に合わせて実装
    """
    w_pos = 5.0
    thresh = 0.1

    # Calculate weighted squared errors
    weights = np.where(y_true >= thresh, w_pos, 1.0)
    squared_errors = (y_true - y_pred) ** 2
    weighted_squared_errors = weights * squared_errors

    # Calculate weighted RMSE with small epsilon for numerical stability
    pw_rmse = np.sqrt(np.mean(weighted_squared_errors) + 1e-9)

    return float(pw_rmse)

def weighted_rmse_feval(y_pred, dtrain):
    """
    LightGBM用の重み付きRMSE評価関数
    """
    y_true = dtrain.get_label()
    weighted_rmse_value = weighted_rmse(y_true, y_pred)
    return "weighted_rmse", weighted_rmse_value, False

def weighted_rmse_obj(y_pred, dtrain):
    """
    LightGBM用の重み付きRMSE目的関数
    """
    w_pos = 5.0
    thresh = 0.1

    y_true = dtrain.get_label()
    weights = np.where(y_true >= thresh, w_pos, 1.0)

    # 勾配とヘッセ行列の計算
    grad = -2 * weights * (y_true - y_pred)
    hess = 2 * weights

    return grad, hess


# %% [markdown] id="fBFkOqtLhraM"
# 特徴量の数も増えており、ハイパーパラメータは探索してみないと分かりません。
#
# ここでは、より効率的に良さそうなパラメータを見つけられるOptunaを用いて最適化を行います。
# Optunaは探索空間から試行回数ごとに候補を提案し、良かった試行の情報を活かしながら次の探索に反映させるベイズ的最適化アルゴリズム（TPEサンプラー）を利用できるため、総当たりのGridSearchよりも少ない試行で良い結果に辿り着きやすいのがメリットです。
# （参考: https://zenn.dev/robes/articles/d53ff6d665650f ）


# %% colab={"base_uri": "https://localhost:8080/"} id="ngpAJwwnaIf3" outputId="321e83a6-aee6-4b0a-88c6-199be978c2c2" trusted=true
import optuna
from optuna.samplers import TPESampler

# Optunaで最適化しないベースパラメータ
base_params = {
    "objective": weighted_rmse_obj,
    "boosting_type": "gbdt",
    "random_state": SEED,
    "verbosity": -1,
    "force_col_wise": True,
    "bagging_fraction": 0.8,
    "bagging_freq": 1,
    "feature_fraction": 0.8,
}

# Optunaで探索するハイパーパラメータの概要
optuna_search_space = {
    "num_leaves": (10, 64),
    "learning_rate": (0.01, 0.1),
    "min_child_samples": (10, 50),
}

print("Optuna用ハイパーパラメータ設定完了")
print(f"探索対象パラメータ: {list(optuna_search_space.keys())}")

# %% colab={"base_uri": "https://localhost:8080/"} id="-hkjQJvTvhMO" outputId="61949fbb-9b97-43cd-9bfc-e704053383ba" trusted=true
try:
    base_dir = Path(__file__).resolve().parent
except NameError:  # __file__ はノートブック実行時には定義されない
    base_dir = Path.cwd()

log_dir = base_dir / "logs"
log_dir.mkdir(parents=True, exist_ok=True)


def save_training_run(
    cv_scores,
    oof_score,
    optuna_summary,
    best_params,
    log_directory: Path,
    log_prefix: str = "host_baseline_002",
):
    """Persist CV metrics to reusable JSON/text logs."""

    timestamp = datetime.now().astimezone().isoformat(timespec="seconds")

    metrics_payload = {
        "run_timestamp": timestamp,
        "cv": {
            "scores": [float(score) for score in cv_scores],
            "mean": float(np.mean(cv_scores)),
            "std": float(np.std(cv_scores)),
        },
        "per_fold": {f"fold_{idx + 1}": float(score) for idx, score in enumerate(cv_scores)},
        "oof_rmse": float(oof_score),
        "optuna": optuna_summary,
        "best_params": {key: (float(val) if isinstance(val, (np.floating, np.integer)) else val)
                         for key, val in best_params.items()},
    }

    metrics_path = log_directory / f"{log_prefix}_metrics.json"
    metrics_path.write_text(json.dumps(metrics_payload, ensure_ascii=False, indent=2), encoding="utf-8")

    log_lines = [
        f"[{timestamp}] {log_prefix}",
        f"  CV mean: {metrics_payload['cv']['mean']:.4f}",
        f"  CV std: {metrics_payload['cv']['std']:.4f}",
        f"  OOF RMSE: {metrics_payload['oof_rmse']:.4f}",
    ]
    for idx, score in enumerate(cv_scores, start=1):
        log_lines.append(f"  Fold {idx}: {score:.4f}")

    log_lines.append(
        "  Optuna best trial: "
        f"{optuna_summary['best_trial_number']} (CV mean {optuna_summary['best_cv_value']:.6f}, "
        f"fold1 RMSE {optuna_summary['fold1_val_rmse']:.6f})"
    )

    log_path = log_directory / f"{log_prefix}_training.log"
    with log_path.open("a", encoding="utf-8") as fp:
        fp.write("\n".join(log_lines) + "\n")


# Optunaによるハイパーパラメータチューニング
print("Optunaによるチューニングを開始します...")

# Fold 1を検証用に確保（後でスコア確認に利用）
trn_mask = train_df["fold"] != 1
val_mask = train_df["fold"] == 1

X_tr = train_df.loc[trn_mask, all_features].copy()
X_val = train_df.loc[val_mask, all_features].copy()
y_tr = y_train.loc[trn_mask].copy()
y_val = y_train.loc[val_mask].copy()

def objective(trial):
    params = base_params.copy()
    params.update({
        "num_leaves": trial.suggest_int("num_leaves", *optuna_search_space["num_leaves"]),
        "learning_rate": trial.suggest_float("learning_rate", *optuna_search_space["learning_rate"], log=True),
        "min_child_samples": trial.suggest_int("min_child_samples", *optuna_search_space["min_child_samples"]),
    })

    cv_scores = []
    for fold in range(1, 4):  # 計算量を抑えるためFold1~3でCV
        trn_mask_cv = train_df["fold"] != fold
        val_mask_cv = train_df["fold"] == fold

        X_tr_cv = train_df.loc[trn_mask_cv, all_features].copy()
        X_val_cv = train_df.loc[val_mask_cv, all_features].copy()
        y_tr_cv = y_train.loc[trn_mask_cv].copy()
        y_val_cv = y_train.loc[val_mask_cv].copy()

        train_data = lgb.Dataset(X_tr_cv, label=y_tr_cv)
        val_data = lgb.Dataset(X_val_cv, label=y_val_cv)

        model = lgb.train(
            params,
            train_data,
            valid_sets=[val_data],
            feval=weighted_rmse_feval,
            num_boost_round=1000,
            callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)],
        )

        preds = model.predict(X_val_cv, num_iteration=model.best_iteration)
        cv_scores.append(weighted_rmse(y_val_cv, preds))

    return float(np.mean(cv_scores))

optuna.logging.set_verbosity(optuna.logging.WARNING)
study = optuna.create_study(direction="minimize", sampler=TPESampler(seed=SEED))
study.optimize(objective, n_trials=30, show_progress_bar=True)

best_params = study.best_trial.params.copy()
best_lgbm_params = base_params.copy()
best_lgbm_params.update(best_params)

# Fold1でのスコアを再確認
train_data = lgb.Dataset(X_tr, label=y_tr)
val_data = lgb.Dataset(X_val, label=y_val)
best_model = lgb.train(
    best_lgbm_params,
    train_data,
    valid_sets=[train_data, val_data],
    valid_names=["train", "val"],
    feval=weighted_rmse_feval,
    num_boost_round=1000,
    callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)],
)

val_pred = best_model.predict(X_val, num_iteration=best_model.best_iteration)
val_score = weighted_rmse(y_val, val_pred)

print("=== Optunaチューニング結果 ===")
print(f"最良Trial番号: {study.best_trial.number}")
print(f"平均CV RMSE: {study.best_value:.6f}")
print(f"Fold1 Validation RMSE: {val_score:.6f}")
print("最適化されたパラメータ:")
for param, value in best_params.items():
    print(f"  {param}: {value}")

# %% colab={"base_uri": "https://localhost:8080/"} id="f1o2kC3wsi0n" outputId="2e156a42-4637-433a-f6db-76205a1e3b21" trusted=true
# 最適化されたパラメータでの5-Fold Cross Validation
print("最適化されたパラメータでの5-Fold Cross Validationを開始...")

# 単調性制約の設定
# xAGと単調増加関係にある特徴量を選定
monotone_increase_features = [
    # プログレッシブ系（攻撃的な前進プレー → xAG増加）
    'progressive_pass_count',
    'progressive_pass_success',
    'progressive_pass_distance_total',
    'progressive_pass_distance_mean',
    'progressive_carry_count',
    'progressive_carry_success',
    'progressive_carry_distance_total',
    'progressive_carry_distance_mean',
    'deep_completion_count',  # ディープゾーン到達数
    'final_third_entry_count',  # ファイナルサード進入数
    'penalty_area_entry_count',  # ペナルティエリア進入数

    # シュート・ゴール系（創造性の指標）
    'goal_count',  # ゴール数
    'pass_leads_to_shot',  # パス→ショットの連鎖

    # 攻撃的ゾーン活動（前線でのプレー → xAG増加）
    'zone_attacking_actions',  # 攻撃ゾーンでのアクション数
    'zone_attacking_actions_ratio',  # 攻撃ゾーン比率

    # 攻撃的バランス
    'type_offensive_actions',  # 攻撃アクション数
    'type_offensive_action_ratio',  # 攻撃アクション比率
]

missing_monotone_features = [feat for feat in monotone_increase_features if feat not in all_features]
if missing_monotone_features:
    print("単調性制約対象として指定したものの、特徴量一覧に存在しない列があります:")
    for feat in missing_monotone_features:
        print(f"  - {feat}")

applied_monotone_features = [feat for feat in monotone_increase_features if feat in all_features]

# all_features内での各特徴量のインデックスを取得し、単調性ベクトルを構築
monotone_constraints = [0] * len(all_features)  # デフォルトは制約なし(0)
for feat in applied_monotone_features:
    idx = all_features.index(feat)
    monotone_constraints[idx] = 1  # 単調増加制約

print(f"\n単調性制約を適用: {len(applied_monotone_features)}個の特徴量")
print("単調増加制約を適用した特徴量:")
for feat in applied_monotone_features:
    print(f"  - {feat}")

# LightGBM学習のパラメータを設定（Optunaで最適化されたパラメータを使用）
lgbm_params = {
    "objective": weighted_rmse_obj,
    "boosting_type": "gbdt",
    "random_state": SEED,
    "verbosity": -1,
    "force_col_wise": True,
    "bagging_fraction": 0.8,
    "bagging_freq": 1,
    "feature_fraction": 0.8,
    "monotone_constraints": monotone_constraints,  # 単調性制約を追加
    "monotone_constraints_method": "advanced",  # advanced methodを使用
}

# Optunaの最適パラメータをマージ
lgbm_params.update(best_params)

print(f"\n使用するパラメータ: {lgbm_params}")

# 5-Foldでのモデル学習（最適化されたパラメータ使用）
oof_preds = np.zeros(len(X_train))
cv_scores = []
models = []
feature_importance = pd.DataFrame()

# Training the models on the entire training data
for fold in range(5):
    print(f"=== Fold {fold + 1} ===")

    # データ分割
    trn_mask = train_df["fold"] != fold+1
    val_mask = train_df["fold"] == fold+1

    X_tr = train_df.loc[trn_mask, all_features].copy()
    X_val = train_df.loc[val_mask, all_features].copy()
    y_tr = y_train.loc[trn_mask].copy()
    y_val = y_train.loc[val_mask].copy()

    # LightGBMデータセット作成
    train_data = lgb.Dataset(X_tr, label=y_tr)
    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)

    # モデル学習（最適パラメータ使用）
    model = lgb.train(
        lgbm_params,
        train_data,
        valid_sets=[train_data, val_data],
        valid_names=["train", "val"],
        feval=weighted_rmse_feval,
        num_boost_round=1000,
        callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)]
    )

    # validationデータに対する予測、スコア算出
    y_pred_val = model.predict(X_val, num_iteration=model.best_iteration)
    oof_preds[val_mask] = y_pred_val
    score = weighted_rmse(y_val, y_pred_val)
    cv_scores.append(score)
    models.append(model)  # このfoldのモデルをmodelsに格納

    print(f"Fold {fold + 1} RMSE: {score:.4f}")

    # 特徴量重要度算出
    fold_importance = pd.DataFrame({
        "feature": all_features,
        "importance": model.feature_importance(importance_type="gain"),
        "fold": fold + 1
    })
    feature_importance = pd.concat([feature_importance, fold_importance], axis=0)

cv_mean = float(np.mean(cv_scores))
cv_std = float(np.std(cv_scores))

print("=== Cross Validation Results (Optimized Parameters) ===")
print(f"CV RMSE: {cv_mean:.4f} (+/- {cv_std * 2:.4f})")
for i, score in enumerate(cv_scores):
    print(f"Fold {i + 1}: {score:.4f}")

# OOF予測のスコア算出
oof_score = weighted_rmse(y_train, oof_preds)
print(f"OOF RMSE: {oof_score:.4f}")

optuna_summary = {
    "best_trial_number": int(study.best_trial.number),
    "best_cv_value": float(study.best_value),
    "fold1_val_rmse": float(val_score),
}

save_training_run(
    cv_scores=cv_scores,
    oof_score=oof_score,
    optuna_summary=optuna_summary,
    best_params=best_params,
    log_directory=log_dir,
)

print(f"メトリクスを保存しました: {log_dir / 'host_baseline_002_metrics.json'}")
print(f"ログを追記しました: {log_dir / 'host_baseline_002_training.log'}")


# %% [markdown]
# ## Tweedie目的の検証
#
# 自作の`weighted_rmse_obj`とは別に、ゼロ膨張かつ右裾が長い分布に適合するため
# Tweedie目的（`tweedie_variance_power`を変化）で再学習した結果を比較します。
#

# %% trusted=true
tweedie_variance_candidates = [1.2, 1.4, 1.6]
tweedie_results = []
tweedie_models = {}

for variance_power in tweedie_variance_candidates:
    tweedie_params = {
        "objective": "tweedie",
        "metric": "rmse",
        "tweedie_variance_power": variance_power,
        "random_state": SEED,
        "verbosity": -1,
        "force_col_wise": True,
        "bagging_fraction": 0.8,
        "bagging_freq": 1,
        "feature_fraction": 0.8,
    }
    for key in ("num_leaves", "learning_rate", "min_child_samples", "lambda_l1", "lambda_l2"):
        if key in best_params:
            tweedie_params[key] = best_params[key]

    oof_preds_tweedie = np.zeros(len(X_train))
    cv_scores_tweedie = []
    fold_models = []

    for fold in range(5):
        trn_mask = train_df["fold"] != fold + 1
        val_mask = train_df["fold"] == fold + 1

        X_tr = train_df.loc[trn_mask, all_features]
        X_val = train_df.loc[val_mask, all_features]
        y_tr = y_train.loc[trn_mask]
        y_val = y_train.loc[val_mask]
        w_tr = train_sample_weights.loc[trn_mask]
        w_val = train_sample_weights.loc[val_mask]

        train_data = lgb.Dataset(X_tr, label=y_tr, weight=w_tr)
        val_data = lgb.Dataset(X_val, label=y_val, weight=w_val, reference=train_data)

        model = lgb.train(
            tweedie_params,
            train_data,
            valid_sets=[train_data, val_data],
            valid_names=["train", "val"],
            num_boost_round=1000,
            feval=weighted_rmse_feval,
            callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)],
        )
        pred_val = model.predict(X_val, num_iteration=model.best_iteration)
        oof_preds_tweedie[val_mask] = pred_val
        cv_scores_tweedie.append(weighted_rmse(y_val, pred_val))
        fold_models.append(model)

    oof_score = weighted_rmse(y_train, oof_preds_tweedie)
    tweedie_results.append({
        "tweedie_variance_power": variance_power,
        "cv_mean": float(np.mean(cv_scores_tweedie)),
        "cv_std": float(np.std(cv_scores_tweedie)),
        "oof_rmse": float(oof_score),
    })
    test_pred = np.mean([m.predict(X_test, num_iteration=m.best_iteration) for m in fold_models], axis=0)
    tweedie_models[variance_power] = {
        "models": fold_models,
        "oof": oof_preds_tweedie,
        "test": test_pred,
        "cv_scores": cv_scores_tweedie,
    }

tweedie_summary = (
    pd.DataFrame(tweedie_results)
    .sort_values("oof_rmse")
    .reset_index(drop=True)
)
print("=== Tweedie Objective Summary ===")
display(tweedie_summary)
best_tweedie_power = tweedie_summary.loc[0, "tweedie_variance_power"]
print(f"Best tweedie_variance_power: {best_tweedie_power}")
best_tweedie_oof = tweedie_models[best_tweedie_power]["oof"]
print(f"Best Tweedie OOF weighted RMSE: {weighted_rmse(y_train, best_tweedie_oof):.5f}")


# %% [markdown]
# ## 分位回帰ブレンドと単調キャリブレーション
#
# 右側の誤差を抑えるため、複数の分位回帰モデルのブレンドと
# `IsotonicRegression`による0.1近傍の単調キャリブレーションを適用します。
#

# %% trusted=true
quantile_alphas = [0.6, 0.75, 0.9]
quantile_oof_preds = {}
quantile_test_preds = {}
quantile_cv_stats = {}

for alpha in quantile_alphas:
    # quantile objectiveでは単調性制約が使えないため除外
    quantile_params = {
        "objective": "quantile",
        "alpha": alpha,
        "metric": "quantile",
        "random_state": SEED,
        "verbosity": -1,
        "force_col_wise": True,
        "bagging_fraction": 0.8,
        "bagging_freq": 1,
        "feature_fraction": 0.8,
    }
    for key in ("num_leaves", "learning_rate", "min_child_samples", "lambda_l1", "lambda_l2"):
        if key in best_params:
            quantile_params[key] = best_params[key]

    oof_preds_quantile = np.zeros(len(X_train))
    cv_scores_quantile = []
    fold_models = []

    for fold in range(5):
        trn_mask = train_df["fold"] != fold + 1
        val_mask = train_df["fold"] == fold + 1

        X_tr = train_df.loc[trn_mask, all_features]
        X_val = train_df.loc[val_mask, all_features]
        y_tr = y_train.loc[trn_mask]
        y_val = y_train.loc[val_mask]
        w_tr = train_sample_weights.loc[trn_mask]
        w_val = train_sample_weights.loc[val_mask]

        train_data = lgb.Dataset(X_tr, label=y_tr, weight=w_tr)
        val_data = lgb.Dataset(X_val, label=y_val, weight=w_val, reference=train_data)

        model = lgb.train(
            quantile_params,
            train_data,
            valid_sets=[train_data, val_data],
            valid_names=["train", "val"],
            num_boost_round=1000,
            feval=weighted_rmse_feval,
            callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)],
        )
        pred_val = model.predict(X_val, num_iteration=model.best_iteration)
        oof_preds_quantile[val_mask] = pred_val
        cv_scores_quantile.append(weighted_rmse(y_val, pred_val))
        fold_models.append(model)

    quantile_oof_preds[alpha] = oof_preds_quantile
    quantile_test_preds[alpha] = np.mean([m.predict(X_test, num_iteration=m.best_iteration) for m in fold_models], axis=0)
    quantile_cv_stats[alpha] = cv_scores_quantile

quantile_summary = pd.DataFrame({
    "alpha": list(quantile_oof_preds.keys()),
    "cv_mean": [float(np.mean(quantile_cv_stats[a])) for a in quantile_oof_preds.keys()],
    "cv_std": [float(np.std(quantile_cv_stats[a])) for a in quantile_oof_preds.keys()],
    "oof_rmse": [float(weighted_rmse(y_train, quantile_oof_preds[a])) for a in quantile_oof_preds.keys()],
}).sort_values("alpha").reset_index(drop=True)
print("=== Quantile Objective Summary ===")
display(quantile_summary)

weight_grid = np.linspace(0.0, 1.0, 21)
best_blend_score = np.inf
best_blend_weights = None
for w_a in weight_grid:
    for w_b in weight_grid:
        if w_a + w_b > 1.0:
            continue
        w_c = 1.0 - w_a - w_b
        blend_oof = (
            w_a * quantile_oof_preds[quantile_alphas[0]]
            + w_b * quantile_oof_preds[quantile_alphas[1]]
            + w_c * quantile_oof_preds[quantile_alphas[2]]
        )
        score = weighted_rmse(y_train, blend_oof)
        if score < best_blend_score - 1e-6:
            best_blend_score = score
            best_blend_weights = (w_a, w_b, w_c)

assert best_blend_weights is not None
blend_oof = (
    best_blend_weights[0] * quantile_oof_preds[quantile_alphas[0]]
    + best_blend_weights[1] * quantile_oof_preds[quantile_alphas[1]]
    + best_blend_weights[2] * quantile_oof_preds[quantile_alphas[2]]
)
blend_test = (
    best_blend_weights[0] * quantile_test_preds[quantile_alphas[0]]
    + best_blend_weights[1] * quantile_test_preds[quantile_alphas[1]]
    + best_blend_weights[2] * quantile_test_preds[quantile_alphas[2]]
)

iso = IsotonicRegression(y_min=0.0, out_of_bounds="clip")
iso.fit(blend_oof, y_train)
blend_oof_calibrated = iso.predict(blend_oof)
blend_test_calibrated = iso.predict(blend_test)
calibrated_score = weighted_rmse(y_train, blend_oof_calibrated)

quantile_blend_result = {
    "weights": best_blend_weights,
    "oof_raw": blend_oof,
    "oof_calibrated": blend_oof_calibrated,
    "test_raw": blend_test,
    "test_calibrated": blend_test_calibrated,
    "raw_score": float(best_blend_score),
    "calibrated_score": float(calibrated_score),
}
print("=== Quantile Blend Results ===")
print(f"Best weights (alpha=0.6,0.75,0.9): {best_blend_weights}")
print(f"OOF weighted RMSE (raw): {best_blend_score:.5f}")
print(f"OOF weighted RMSE (calibrated): {calibrated_score:.5f}")


# %% [markdown] id="Sap_9i9DaIf3"
# ## テストデータに対する推論

# %% colab={"base_uri": "https://localhost:8080/"} id="3Xs1lGqfaIf4" outputId="47eceb84-34b8-480a-dfd5-85eebb203bc7" trusted=true
# アンサンブル予測（5モデルの平均） on Test Data
test_preds = np.zeros(len(X_test))

for model in models:
    pred = model.predict(X_test, num_iteration=model.best_iteration)
    test_preds += pred

test_preds /= len(models)


print(f"\n=== Test Set Predictions ===")
print(f"予測xAG範囲: {test_preds.min():.3f} 〜 {test_preds.max():.3f}")

# test_dfに予測結果を追加
test_df['predicted_xAG'] = test_preds

# %% [markdown] id="yL_4w1qWaIf4"
# ## 予測結果の分析

# %% colab={"base_uri": "https://localhost:8080/", "height": 998} id="KLKnE3KajdZp" outputId="d9b9cad3-dedc-458d-aad3-e23986d7e5a3" trusted=true
# 特徴量重要度の平均計算
feature_importance_mean = feature_importance.groupby('feature')['importance'].agg(['mean', 'std']).reset_index()
feature_importance_mean = feature_importance_mean.sort_values('mean', ascending=False)

# 可視化
plt.figure(figsize=(10, 8))
sns.barplot(data=feature_importance_mean.head(15), x='mean', y='feature')
plt.title('Top 15 Feature Importance (Weighted RMSE Baseline xAG Model)')
plt.xlabel('Importance')
plt.tight_layout()
plt.show()

print("特徴量重要度 Top 10:")
print(feature_importance_mean.head(10))

# %% colab={"base_uri": "https://localhost:8080/", "height": 607} id="kGIeBEIPaIf4" outputId="92e7fb77-a19e-45f3-c778-c010f56a2adf" trusted=true
# train, test予測値の分布を可視化
plt.figure(figsize=(8, 6))
sns.histplot(oof_preds, stat='density', kde=True, alpha=0.2, label='OOF予測', linewidth=0)
sns.histplot(test_preds, stat='density', kde=True, alpha=0.2, label='Test予測', linewidth=0)

# train正解値の分布を可視化
vc = y_train.value_counts().sort_index()
heights = vc / vc.sum() / 0.1 # 棒グラフの高さをdensity に合わせる
plt.bar(vc.index, heights, width=0.03, alpha=0.6, label='OOF正解', align='center')

plt.xlabel('xAG')
plt.ylabel('密度')
plt.xlim(0, 1)
plt.title('xAG予測値の分布（OOF予測 vs Test予測 vs OOF正解）')
plt.legend()
plt.tight_layout()
plt.show()

# %% [markdown] id="61-ufcOruCGb"
# 正解が0.0のデータの重み付けが小さいため、全体的に正の値を予想する傾向が見られる。

# %% [markdown] id="u4I2O1ntaIf4"
# ## 提出ファイル作成

# %% id="cbhq5ARAaIf4" trusted=true
# 提出用ファイルにtestデータに対する予測値を格納
submission_df["xAG"] = test_preds
submission_df.to_csv(f"{base_path}/host_baseline_009_submissions.csv", index=False)

# %% id="NuMygIW4xHs1" trusted=true
