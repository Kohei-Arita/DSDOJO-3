# ---
# jupyter:
#   colab:
#     provenance: []
#   jupytext:
#     cell_metadata_filter: all
#     formats: ipynb,py:percent
#     notebook_metadata_filter: all,-jupytext.text_representation.jupytext_version
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#   kernelspec:
#     display_name: Python 3 (ipykernel)
#     language: python
#     name: python3
#   language_info:
#     codemirror_mode:
#       name: ipython
#       version: 3
#     file_extension: .py
#     mimetype: text/x-python
#     name: python
#     nbconvert_exporter: python
#     pygments_lexer: ipython3
#     version: 3.11.13
#   widgets:
#     application/vnd.jupyter.widget-state+json:
#       09a43976bef243599b2e9b64cffb91b4:
#         model_module: '@jupyter-widgets/controls'
#         model_module_version: 1.5.0
#         model_name: HBoxModel
#         state:
#           _dom_classes: []
#           _model_module: '@jupyter-widgets/controls'
#           _model_module_version: 1.5.0
#           _model_name: HBoxModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/controls'
#           _view_module_version: 1.5.0
#           _view_name: HBoxView
#           box_style: ''
#           children:
#           - IPY_MODEL_d3e34a9bb0d24577acc83d7ae4b2486d
#           - IPY_MODEL_dcffd72448ea44bf9ec8aec5b31762f6
#           - IPY_MODEL_3a83f4956d7d41938ef54f146a5cc541
#           layout: IPY_MODEL_69216dc0f0f4422e8166c4a8f8abe666
#       09a7b0f32ef942afb2963f355e80a8bc:
#         model_module: '@jupyter-widgets/controls'
#         model_module_version: 1.5.0
#         model_name: DescriptionStyleModel
#         state:
#           _model_module: '@jupyter-widgets/controls'
#           _model_module_version: 1.5.0
#           _model_name: DescriptionStyleModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/base'
#           _view_module_version: 1.2.0
#           _view_name: StyleView
#           description_width: ''
#       3a83f4956d7d41938ef54f146a5cc541:
#         model_module: '@jupyter-widgets/controls'
#         model_module_version: 1.5.0
#         model_name: HTMLModel
#         state:
#           _dom_classes: []
#           _model_module: '@jupyter-widgets/controls'
#           _model_module_version: 1.5.0
#           _model_name: HTMLModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/controls'
#           _view_module_version: 1.5.0
#           _view_name: HTMLView
#           description: ''
#           description_tooltip: null
#           layout: IPY_MODEL_5b6ef11c003d4a79b3924bc396c0ff56
#           placeholder: "\u200B"
#           style: IPY_MODEL_78773c5f4ebc47fea4077d687ab0ea0c
#           value: "\u200740041/40041\u2007[03:10&lt;00:00,\u2007234.12it/s]"
#       5b6ef11c003d4a79b3924bc396c0ff56:
#         model_module: '@jupyter-widgets/base'
#         model_module_version: 1.2.0
#         model_name: LayoutModel
#         state:
#           _model_module: '@jupyter-widgets/base'
#           _model_module_version: 1.2.0
#           _model_name: LayoutModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/base'
#           _view_module_version: 1.2.0
#           _view_name: LayoutView
#           align_content: null
#           align_items: null
#           align_self: null
#           border: null
#           bottom: null
#           display: null
#           flex: null
#           flex_flow: null
#           grid_area: null
#           grid_auto_columns: null
#           grid_auto_flow: null
#           grid_auto_rows: null
#           grid_column: null
#           grid_gap: null
#           grid_row: null
#           grid_template_areas: null
#           grid_template_columns: null
#           grid_template_rows: null
#           height: null
#           justify_content: null
#           justify_items: null
#           left: null
#           margin: null
#           max_height: null
#           max_width: null
#           min_height: null
#           min_width: null
#           object_fit: null
#           object_position: null
#           order: null
#           overflow: null
#           overflow_x: null
#           overflow_y: null
#           padding: null
#           right: null
#           top: null
#           visibility: null
#           width: null
#       5e50717143ae4771b600c2a5f73488e2:
#         model_module: '@jupyter-widgets/controls'
#         model_module_version: 1.5.0
#         model_name: ProgressStyleModel
#         state:
#           _model_module: '@jupyter-widgets/controls'
#           _model_module_version: 1.5.0
#           _model_name: ProgressStyleModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/base'
#           _view_module_version: 1.2.0
#           _view_name: StyleView
#           bar_color: null
#           description_width: ''
#       69216dc0f0f4422e8166c4a8f8abe666:
#         model_module: '@jupyter-widgets/base'
#         model_module_version: 1.2.0
#         model_name: LayoutModel
#         state:
#           _model_module: '@jupyter-widgets/base'
#           _model_module_version: 1.2.0
#           _model_name: LayoutModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/base'
#           _view_module_version: 1.2.0
#           _view_name: LayoutView
#           align_content: null
#           align_items: null
#           align_self: null
#           border: null
#           bottom: null
#           display: null
#           flex: null
#           flex_flow: null
#           grid_area: null
#           grid_auto_columns: null
#           grid_auto_flow: null
#           grid_auto_rows: null
#           grid_column: null
#           grid_gap: null
#           grid_row: null
#           grid_template_areas: null
#           grid_template_columns: null
#           grid_template_rows: null
#           height: null
#           justify_content: null
#           justify_items: null
#           left: null
#           margin: null
#           max_height: null
#           max_width: null
#           min_height: null
#           min_width: null
#           object_fit: null
#           object_position: null
#           order: null
#           overflow: null
#           overflow_x: null
#           overflow_y: null
#           padding: null
#           right: null
#           top: null
#           visibility: null
#           width: null
#       78773c5f4ebc47fea4077d687ab0ea0c:
#         model_module: '@jupyter-widgets/controls'
#         model_module_version: 1.5.0
#         model_name: DescriptionStyleModel
#         state:
#           _model_module: '@jupyter-widgets/controls'
#           _model_module_version: 1.5.0
#           _model_name: DescriptionStyleModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/base'
#           _view_module_version: 1.2.0
#           _view_name: StyleView
#           description_width: ''
#       79fcd266aa524e9b9d656b2eab1a8cd3:
#         model_module: '@jupyter-widgets/base'
#         model_module_version: 1.2.0
#         model_name: LayoutModel
#         state:
#           _model_module: '@jupyter-widgets/base'
#           _model_module_version: 1.2.0
#           _model_name: LayoutModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/base'
#           _view_module_version: 1.2.0
#           _view_name: LayoutView
#           align_content: null
#           align_items: null
#           align_self: null
#           border: null
#           bottom: null
#           display: null
#           flex: null
#           flex_flow: null
#           grid_area: null
#           grid_auto_columns: null
#           grid_auto_flow: null
#           grid_auto_rows: null
#           grid_column: null
#           grid_gap: null
#           grid_row: null
#           grid_template_areas: null
#           grid_template_columns: null
#           grid_template_rows: null
#           height: null
#           justify_content: null
#           justify_items: null
#           left: null
#           margin: null
#           max_height: null
#           max_width: null
#           min_height: null
#           min_width: null
#           object_fit: null
#           object_position: null
#           order: null
#           overflow: null
#           overflow_x: null
#           overflow_y: null
#           padding: null
#           right: null
#           top: null
#           visibility: null
#           width: null
#       d3e34a9bb0d24577acc83d7ae4b2486d:
#         model_module: '@jupyter-widgets/controls'
#         model_module_version: 1.5.0
#         model_name: HTMLModel
#         state:
#           _dom_classes: []
#           _model_module: '@jupyter-widgets/controls'
#           _model_module_version: 1.5.0
#           _model_name: HTMLModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/controls'
#           _view_module_version: 1.5.0
#           _view_name: HTMLView
#           description: ''
#           description_tooltip: null
#           layout: IPY_MODEL_79fcd266aa524e9b9d656b2eab1a8cd3
#           placeholder: "\u200B"
#           style: IPY_MODEL_09a7b0f32ef942afb2963f355e80a8bc
#           value: "Calculating\u2007success\u2007rates:\u2007100%"
#       dc043a4a5ed74b1c9b6cc8e027e5b2b6:
#         model_module: '@jupyter-widgets/base'
#         model_module_version: 1.2.0
#         model_name: LayoutModel
#         state:
#           _model_module: '@jupyter-widgets/base'
#           _model_module_version: 1.2.0
#           _model_name: LayoutModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/base'
#           _view_module_version: 1.2.0
#           _view_name: LayoutView
#           align_content: null
#           align_items: null
#           align_self: null
#           border: null
#           bottom: null
#           display: null
#           flex: null
#           flex_flow: null
#           grid_area: null
#           grid_auto_columns: null
#           grid_auto_flow: null
#           grid_auto_rows: null
#           grid_column: null
#           grid_gap: null
#           grid_row: null
#           grid_template_areas: null
#           grid_template_columns: null
#           grid_template_rows: null
#           height: null
#           justify_content: null
#           justify_items: null
#           left: null
#           margin: null
#           max_height: null
#           max_width: null
#           min_height: null
#           min_width: null
#           object_fit: null
#           object_position: null
#           order: null
#           overflow: null
#           overflow_x: null
#           overflow_y: null
#           padding: null
#           right: null
#           top: null
#           visibility: null
#           width: null
#       dcffd72448ea44bf9ec8aec5b31762f6:
#         model_module: '@jupyter-widgets/controls'
#         model_module_version: 1.5.0
#         model_name: FloatProgressModel
#         state:
#           _dom_classes: []
#           _model_module: '@jupyter-widgets/controls'
#           _model_module_version: 1.5.0
#           _model_name: FloatProgressModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/controls'
#           _view_module_version: 1.5.0
#           _view_name: ProgressView
#           bar_style: success
#           description: ''
#           description_tooltip: null
#           layout: IPY_MODEL_dc043a4a5ed74b1c9b6cc8e027e5b2b6
#           max: 40041
#           min: 0
#           orientation: horizontal
#           style: IPY_MODEL_5e50717143ae4771b600c2a5f73488e2
#           value: 40041
# ---

# %% [markdown] id="HU48Dzb4iuhP"
# # xAGäºˆæ¸¬ã‚³ãƒ³ãƒšã€€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆãã®2ï¼‰
#
# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆhost_baseline_001.ipynbï¼‰ã«ã¤ã„ã¦ã€ç‰¹å¾´é‡ã®è¿½åŠ ä½œæˆã‚„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’è¡Œã£ãŸæ”¹å–„ç‰ˆã‚³ãƒ¼ãƒ‰ã§ã™ã€‚

# %% id="4N4IPuA_J7sw" trusted=true
#ç¬¬ä¸€å›ã¯ã“ã¡ã‚‰
#https://www.kaggle.com/competitions/dsdojo_1/overview

# %% [markdown] id="cdc2NNOJiuhU"
# ---
# ## ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
#
#

# %% colab={"base_uri": "https://localhost:8080/"} id="GZJyn-MciuhU" outputId="eb7079dd-b265-4b57-fda9-ff61cec466b9" trusted=true
# å¿…è¦ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§Colabç’°å¢ƒã«ãªã„ã‚‚ã®ã¯install
# !pip install japanize_matplotlib
# !pip install catboost

# %% id="km_jW_2YiuhU" trusted=true
# å¿…è¦ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’import
import json
from datetime import datetime
from pathlib import Path

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib_venn import venn2
import seaborn as sns
import japanize_matplotlib
import networkx as nx
import lightgbm as lgb
import catboost as cb
from sklearn.model_selection import train_test_split, KFold, GroupKFold
from tqdm.auto import tqdm
import warnings
warnings.filterwarnings('ignore')

# ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’ä¼´ã†å‡¦ç†ã‚’è¡Œã†ãŸã‚ã€çµæœã®å†ç¾æ€§ã‚’ä¿ã¤ã«ã¯ã‚·ãƒ¼ãƒ‰å€¤ã‚’å›ºå®šã—ã¦ãŠãå¿…è¦ãŒã‚ã‚Šã¾ã™
SEED = 42
np.random.seed(SEED)


# %% id="ah17yN7bu-Mu" trusted=true
# ã‚³ãƒ³ãƒšã®è©•ä¾¡æŒ‡æ¨™ã«åˆã‚ã›ãŸç›®çš„é–¢æ•°/è©•ä¾¡é–¢æ•°ã®å®šç¾©
WEIGHTED_TARGET_THRESHOLD = 0.1
WEIGHTED_POSITIVE_WEIGHT = 5.0

def make_sample_weight(y_true):
    """
    ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«å¿œã˜ãŸé‡ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ
    """
    y_array = np.asarray(y_true, dtype=float)
    return np.where(y_array >= WEIGHTED_TARGET_THRESHOLD, WEIGHTED_POSITIVE_WEIGHT, 1.0)

def weighted_rmse(y_true, y_pred):
    """
    é‡ã¿ä»˜ãRMSEè©•ä¾¡é–¢æ•°
    ã‚³ãƒ³ãƒšã®è©•ä¾¡æŒ‡æ¨™ã«åˆã‚ã›ã¦å®Ÿè£…
    """
    weights = make_sample_weight(y_true)
    squared_errors = (y_true - y_pred) ** 2
    weighted_squared_errors = weights * squared_errors
    pw_rmse = np.sqrt(np.mean(weighted_squared_errors) + 1e-9)
    return float(pw_rmse)

def weighted_rmse_feval(y_pred, dtrain):
    """
    LightGBMç”¨ã®é‡ã¿ä»˜ãRMSEè©•ä¾¡é–¢æ•°
    """
    y_true = dtrain.get_label()
    weighted_rmse_value = weighted_rmse(y_true, y_pred)
    return "weighted_rmse", weighted_rmse_value, False

# %% id="v0L9gXW5iuhU" trusted=true
# è¡¨ç¤ºã§ãã‚‹dfã®è¡Œã€åˆ—æ•°ã‚’å¢—ã‚„ã™
pd.set_option("display.max_rows", 100)    # æœ€å¤§100è¡Œã¾ã§è¡¨ç¤º
pd.set_option("display.max_columns", 100) # æœ€å¤§100åˆ—ã¾ã§è¡¨ç¤º

# %% [markdown] id="29WdPR8riuhV"
# ## ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿

# %% colab={"base_uri": "https://localhost:8080/", "height": 804} id="e6eZzzhRiuhV" outputId="b494a902-ca3a-4cde-d9e9-52c00f7f2c38" trusted=true
# ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œç”¨ã®ãƒ‘ã‚¹è¨­å®š
base_path = '../../data'
print(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å…ƒãƒ‘ã‚¹: {base_path}")

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# player_idã‚„match_idã®æ•°å€¤çš„å¤§å°ã«æ„å‘³ã¯ãªã„ã®ã§stringå½¢å¼ã§èª­ã¿è¾¼ã¿
train_df = pd.read_csv(f"{base_path}/match_train_data.csv", dtype={"player_id": "string", "match_id": "string"})
test_df = pd.read_csv(f"{base_path}/match_test_data.csv", dtype={"player_id": "string", "match_id": "string"})
actions_df = pd.read_csv(f"{base_path}/action_data.csv", dtype={"player_id": "string", "match_id": "string"})
submission_df = pd.read_csv(f"{base_path}/sample_submission.csv")

print(f"trainãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {train_df.shape}")
display(train_df.head(3))

print(f"\ntestãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {test_df.shape}")
display(test_df.head(3))

print(f"\nã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {actions_df.shape}")
display(actions_df.head(3))


# %% [markdown] id="a4Pt7U5biuhV"
# ## ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° - åŸºæœ¬ç‰¹å¾´é‡
#
# ã¾ãšã€001ã¨åŒã˜åŸºæœ¬çš„ãªç‰¹å¾´é‡ã‚’ä½œæˆã—ã¾ã™ã€‚

# %% colab={"base_uri": "https://localhost:8080/"} id="F3d_UMBliuhV" outputId="3eec7699-ba5f-4497-855d-8c6c40c55b20" trusted=true
# æ‰€ä¸ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç°¡å˜ã«è¨ˆç®—ã§ãã‚‹å¹´é½¢ç‰¹å¾´é‡ã‚’è¿½åŠ ã™ã‚‹

# 2017/18ã‚·ãƒ¼ã‚ºãƒ³çµ‚äº†æ™‚ç‚¹ã§ã®å¹´é½¢ã‚’è¨ˆç®—
train_df['Date'] = pd.to_datetime(train_df['Date'])
train_df['birth_date'] = pd.to_datetime(train_df['birth_date'])
train_df['age'] = (train_df['Date'] - train_df['birth_date']).dt.days / 365.25

test_df['Date'] = pd.to_datetime(test_df['Date'])
test_df['birth_date'] = pd.to_datetime(test_df['birth_date'])
test_df['age'] = (test_df['Date'] - test_df['birth_date']).dt.days / 365.25

print(f"\nãƒãƒ¼ã‚¸å¾Œã®trainãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {train_df.shape}")
print(f"\nãƒãƒ¼ã‚¸å¾Œã®testãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {test_df.shape}")

# %% colab={"base_uri": "https://localhost:8080/"} id="r_urDy8riuhV" outputId="ef6080ba-2f4d-42ed-aea8-35cffd58e420" trusted=true
# ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è©¦åˆÃ—é¸æ‰‹ãƒ¬ãƒ™ãƒ«ã®ç‰¹å¾´é‡ã‚’ä½œæˆ

# train/testã«å«ã¾ã‚Œã‚‹è©¦åˆÃ—é¸æ‰‹ã®çµ„ã¿åˆã‚ã›ã‚’ä½œæˆã™ã‚‹
target_match_players_train = train_df[['match_id', 'player_id']].drop_duplicates()
target_match_players_test = test_df[['match_id', 'player_id']].drop_duplicates()
target_match_players = pd.concat([target_match_players_train, target_match_players_test]).drop_duplicates()

print(f"åˆ†æå¯¾è±¡ã¨ãªã‚‹è©¦åˆÃ—é¸æ‰‹: {len(target_match_players)}çµ„")

# ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ã†ã¡ã€train/testãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã‚‹è©¦åˆÃ—é¸æ‰‹ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ã¿ã‚’æŠ½å‡º
relevant_actions = actions_df.merge(
    target_match_players,
    on=['match_id', 'player_id'],
    how='inner'
)
print(f"æŠ½å‡ºã•ã‚ŒãŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°: {len(relevant_actions)}ä»¶")

# %% trusted=true
# è¿½åŠ ï¼ˆæ—©ã„æ®µéšã«æŒ¿å…¥ï¼‰: é«˜åº¦ç‰¹å¾´é‡ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã®èª­ã¿è¾¼ã¿
from pathlib import Path
import sys
import importlib

try:
    ROOT = Path.cwd().resolve().parents[1]
    if str(ROOT) not in sys.path:
        sys.path.append(str(ROOT))
except Exception:
    pass

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ãƒªãƒ­ãƒ¼ãƒ‰ï¼ˆä¿®æ­£ã‚’åæ˜ ã•ã›ã‚‹ï¼‰
from scripts import advanced_features
importlib.reload(advanced_features)

from scripts.advanced_features import (
    build_nstep_chain_features,
    build_second_assist_sca_gca,
    build_pass_geometry_and_timing,
    build_xpass_risk_features,
    add_player_trend,
    # ğŸ†• æ–°ç‰¹å¾´é‡é–¢æ•°
    build_time_based_features,
    build_zone_based_features,
    build_pass_network_centrality,
    build_extended_chain_features,
    build_dynamic_positioning_features,
)
print("advanced_features imported and reloaded (early cell).")


# %% colab={"base_uri": "https://localhost:8080/", "height": 395} id="hXudsW7vjuBf" outputId="5dcd2476-bef4-4b52-a970-22614a3b7fd9" trusted=true
# ä½ç½®ãƒ‡ãƒ¼ã‚¿ã«ã¤ã„ã¦ã¯ã€homeã¨awayã§åŸºæº–ãŒç•°ãªã‚‹
# homeã®å ´åˆã¯ã€x=0ãŒè‡ªé™£ã‚´ãƒ¼ãƒ«ãƒ©ã‚¤ãƒ³ã€x=105ãŒæ•µé™£ã‚´ãƒ¼ãƒ«ãƒ©ã‚¤ãƒ³ã€y=0ãŒå³ã‚µã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã€y=68ãŒå·¦ã‚µã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã«å¯¾å¿œã™ã‚‹
# awayã§ã¯é€†ã«ãªã‚‹ãŸã‚ã€homeã®é¸æ‰‹ã¨awayã®é¸æ‰‹ã§å¹³å‡çš„ãªx,yã®å€¤ã‚’æ¯”è¼ƒã™ã‚‹ã“ã¨ãŒã§ããªã„
display(relevant_actions[(relevant_actions["type_name"] == "shot") & (relevant_actions["result_name"] == "success")][["is_home", "start_x", "start_y", "end_x", "end_y"]].head())

# ãã“ã§ã€ä½ç½®ã‚’æ¨™æº–åŒ–ã™ã‚‹ãŸã‚ã€awayãƒãƒ¼ãƒ ã®å ´åˆã¯ã€x' = 105-x, y' = 68-yã«ä¿®æ­£ã™ã‚‹
relevant_actions.loc[relevant_actions['is_home'] == False, 'start_x'] = 105 - relevant_actions.loc[relevant_actions['is_home'] == False, 'start_x']
relevant_actions.loc[relevant_actions['is_home'] == False, 'end_x'] = 105 - relevant_actions.loc[relevant_actions['is_home'] == False, 'end_x']
relevant_actions.loc[relevant_actions['is_home'] == False, 'start_y'] = 68 - relevant_actions.loc[relevant_actions['is_home'] == False, 'start_y']
relevant_actions.loc[relevant_actions['is_home'] == False, 'end_y'] = 68 - relevant_actions.loc[relevant_actions['is_home'] == False, 'end_y']

relevant_actions[(relevant_actions["type_name"] == "shot") & (relevant_actions["result_name"] == "success")][["is_home", "start_x", "start_y", "end_x", "end_y"]].head()

# %% [markdown] id="sQOHu5iUnWLA"
# is_homeã®å€¤ã«é–¢ä¿‚ãªãã€ã‚´ãƒ¼ãƒ«ã—ãŸå ´åˆend_x=105ã¨ãªã£ã¦ãŠã‚Šã€ä½ç½®ãŒæ¨™æº–åŒ–ã•ã‚Œã¦ã„ã‚‹

# %% colab={"base_uri": "https://localhost:8080/", "height": 162} id="5LCBi5A3joBl" outputId="935dbfa8-dd0f-44c5-e3f9-c2e8d4b26b25" trusted=true
# åŸºæœ¬çš„ãªçµ±è¨ˆç‰¹å¾´é‡ã®ä½œæˆ
# groupby()ã¨agg()ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€åˆ—ã”ã¨ã«ä»»æ„ã®é›†è¨ˆæ–¹æ³•ã‚’æŒ‡å®šã§ãã‚‹ã€‚
match_player_stats = (
    relevant_actions
    .groupby(['match_id', 'player_id'])
    .agg(
        action_count   = ('type_name', 'size'), # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°åˆè¨ˆ
        avg_x          = ('start_x', 'mean'), # å¹³å‡ãƒã‚¸ã‚·ãƒ§ãƒ³ï¼ˆå‰å¾Œæ–¹å‘ï¼‰
        avg_y          = ('start_y', 'mean'), # å¹³å‡ãƒã‚¸ã‚·ãƒ§ãƒ³ï¼ˆå·¦å³æ–¹å‘ï¼‰
        minutes_played = ('minutes_played', 'first')  # å‡ºå ´æ™‚é–“
    )
    .round(2)
    .reset_index()
)

print(f"ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {match_player_stats.shape}")
display(match_player_stats.head(3))

# %% trusted=true
# è¿½åŠ ï¼ˆrelevant_actionsç›´å¾Œï¼‰: é«˜åº¦ç‰¹å¾´é‡è¨ˆç®—
print("Computing advanced features from relevant_actions (early) ...")

nstep_block = build_nstep_chain_features(
    relevant_actions,
    match_col="match_id",
    player_col="player_id",
    team_col="team_id",
    type_col="type_name",
    n_steps=3,
    gamma=0.7,
)

second_assist, sca1, sca2, gca1, gca2 = build_second_assist_sca_gca(
    relevant_actions,
    match_col="match_id",
    player_col="player_id",
    team_col="team_id",
    type_col="type_name",
    result_col="result_name",
)

pass_geom, pass_latency = build_pass_geometry_and_timing(
    relevant_actions,
    match_col="match_id",
    player_col="player_id",
    type_col="type_name",
)

xpass_risk = build_xpass_risk_features(
    relevant_actions,
    match_col="match_id",
    player_col="player_id",
    type_col="type_name",
    result_col="result_name",
)

print("Advanced feature blocks created (early).")


# %% trusted=true
# ğŸ†• æ–°ç‰¹å¾´é‡ã®è¨ˆç®— (EXP0025è¿½åŠ )
print("è¨ˆç®—ä¸­: æ–°ç‰¹å¾´é‡ (æ™‚é–“å¸¯åˆ¥/ã‚¾ãƒ¼ãƒ³åˆ¥/ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯/æ‹¡å¼µé€£é–/å‹•çš„ãƒã‚¸ã‚·ãƒ§ãƒ‹ãƒ³ã‚°)...")

# 1. æ™‚é–“å¸¯åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
time_based_features = build_time_based_features(
    relevant_actions,
    match_col="match_id",
    player_col="player_id",
    time_col="time_seconds",
    period_col="period_id"
)

# 2. ã‚¾ãƒ¼ãƒ³åˆ¥ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å¯†åº¦
zone_based_features = build_zone_based_features(
    relevant_actions,
    match_col="match_id",
    player_col="player_id"
)

# 3. ãƒ‘ã‚¹ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¸­å¿ƒæ€§
network_centrality_features = build_pass_network_centrality(
    relevant_actions,
    match_col="match_id",
    player_col="player_id",
    team_col="team_id",
    type_col="type_name",
    time_col="time_seconds"
)

# 4. æ‹¡å¼µã‚·ãƒ¼ã‚±ãƒ³ã‚¹é€£é– (7æ‰‹å…ˆ)
extended_chain_features = build_extended_chain_features(
    relevant_actions,
    match_col="match_id",
    player_col="player_id",
    team_col="team_id",
    type_col="type_name",
    n_steps=7,
    gamma=0.6
)

# 5. å‹•çš„ãƒã‚¸ã‚·ãƒ§ãƒ‹ãƒ³ã‚°
dynamic_positioning_features = build_dynamic_positioning_features(
    relevant_actions,
    match_col="match_id",
    player_col="player_id"
)

print("æ–°ç‰¹å¾´é‡ãƒ–ãƒ­ãƒƒã‚¯ä½œæˆå®Œäº†")
print(f"  - æ™‚é–“å¸¯åˆ¥: {len(time_based_features)}è¡Œ")
print(f"  - ã‚¾ãƒ¼ãƒ³åˆ¥: {len(zone_based_features)}è¡Œ")
print(f"  - ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¸­å¿ƒæ€§: {len(network_centrality_features)}è¡Œ")
print(f"  - æ‹¡å¼µé€£é–: {len(extended_chain_features)}è¡Œ")
print(f"  - å‹•çš„ãƒã‚¸ã‚·ãƒ§ãƒ‹ãƒ³ã‚°: {len(dynamic_positioning_features)}è¡Œ")

# %% colab={"base_uri": "https://localhost:8080/", "height": 162} id="RAZHAVEanr4A" outputId="a989b03e-8c48-472d-9aaf-03b24fac09bf" trusted=true
# ã‚´ãƒ¼ãƒ«æ•°ã®é›†è¨ˆ
# type_nameã«shotãŒå«ã¾ã‚Œã¦ã€æˆåŠŸã—ãŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¯ã‚´ãƒ¼ãƒ«ã«ãªã‚‹
is_shot  = relevant_actions['type_name'].isin(['shot', 'shot_freekick', 'shot_penalty'])
is_success = relevant_actions['result_name'].eq('success')
is_goal = (is_shot & is_success).astype(int)

match_player_goals = (
    relevant_actions
    .assign(is_goal=is_goal) # is_goalåˆ—ã‚’è¿½åŠ 
    .groupby(['match_id', 'player_id'], as_index=False)['is_goal']
    .sum() # ã‚´ãƒ¼ãƒ«ã§ã‚ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’åˆè¨ˆ
    .rename(columns={'is_goal': 'goal_count'})
)

print(f"ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {match_player_goals.shape}")
display(match_player_goals.head(3))

# %% colab={"base_uri": "https://localhost:8080/", "height": 182} id="ZHhHF5_tiuhV" outputId="dfa2bb40-8395-43e4-c7c8-afe51f28a8dd" trusted=true
# ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—æ•°ã®é›†è¨ˆ
# type_nameåˆ—ã®å€¤ã”ã¨ã«æ•°ã‚’é›†è¨ˆã™ã‚‹
action_type_stats = (
    relevant_actions
    .groupby(['match_id', 'player_id', 'type_name'])
    .size()
    .unstack(fill_value=0)  # type_name ã‚’åˆ—ã«å±•é–‹ã€æ¬ æã¯0ã§åŸ‹ã‚ã‚‹
    .rename_axis(None, axis=1)
    .add_prefix('type_').add_suffix('_count') # åˆ—åã«æ¥é ­è¾ã¨æ¥å°¾è¾ã‚’è¿½åŠ ã™ã‚‹ï¼ˆtype_nameãŒshotãªã‚‰ã€Œtype_shot_countã€ã«ãªã‚‹ï¼‰
    .reset_index()
)

print(f"ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {action_type_stats.shape}")
display(action_type_stats.head(3))

# %% colab={"base_uri": "https://localhost:8080/"} id="oMpQ9RL1n1h8" outputId="e3a47fbf-85f2-44ee-d77c-2acb13ee8c48" trusted=true
# ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ã‚’train/testã¸ãƒãƒ¼ã‚¸
train_df = (
    train_df
    .merge(match_player_stats, on=['match_id', 'player_id'], how='left')
    .merge(match_player_goals, on=['match_id', 'player_id'], how='left')
    .merge(action_type_stats, on=['match_id', 'player_id'], how='left')
)


test_df = (
    test_df
    .merge(match_player_stats, on=['match_id', 'player_id'], how='left')
    .merge(match_player_goals, on=['match_id', 'player_id'], how='left')
    .merge(action_type_stats, on=['match_id', 'player_id'], how='left')
)


action_type_cols = [col for col in train_df.columns if col.startswith('type_')]
stats_count_cols = ['action_count', 'minutes_played', 'goal_count']

for col in action_type_cols + stats_count_cols:
    if col in train_df.columns:
        train_df[col] = train_df[col].fillna(0)
    if col in test_df.columns:
        test_df[col] = test_df[col].fillna(0)

print(f"ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ãƒãƒ¼ã‚¸å¾Œã®trainãƒ‡ãƒ¼ã‚¿shape: {train_df.shape}")
print(f"ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ãƒãƒ¼ã‚¸å¾Œã®testãƒ‡ãƒ¼ã‚¿shape: {test_df.shape}")


# %% [markdown] id="vGTCBG_BiuhV"
# ## ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° - å¿œç”¨ç‰¹å¾´é‡
#
# ã“ã“ã‹ã‚‰ã€ã‚ˆã‚Šé«˜åº¦ãªç‰¹å¾´é‡ã‚’ä½œæˆã—ã¦ã„ãã¾ã™ã€‚å„ç‰¹å¾´é‡ã®æ„å›³ã¨è¨ˆç®—æ–¹æ³•ã‚’è©³ã—ãèª¬æ˜ã—ã¾ã™ã€‚

# %% colab={"base_uri": "https://localhost:8080/", "height": 232, "referenced_widgets": ["09a43976bef243599b2e9b64cffb91b4", "d3e34a9bb0d24577acc83d7ae4b2486d", "dcffd72448ea44bf9ec8aec5b31762f6", "3a83f4956d7d41938ef54f146a5cc541", "69216dc0f0f4422e8166c4a8f8abe666", "79fcd266aa524e9b9d656b2eab1a8cd3", "09a7b0f32ef942afb2963f355e80a8bc", "dc043a4a5ed74b1c9b6cc8e027e5b2b6", "5e50717143ae4771b600c2a5f73488e2", "5b6ef11c003d4a79b3924bc396c0ff56", "78773c5f4ebc47fea4077d687ab0ea0c"]} id="maM2mFbTiuhV" outputId="6c028ca4-a3f4-41f8-b6f5-79d096d17f19" trusted=true
# ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æˆåŠŸç‡ç‰¹å¾´é‡
# ã‚¢ã‚·ã‚¹ãƒˆã«ç¹‹ãŒã‚‹å¯èƒ½æ€§ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã€å„ç¨®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®æˆåŠŸç‡ã‚’è¨ˆç®—ã™ã‚‹

# æˆåŠŸç‡ã‚’è¨ˆç®—ã™ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—
action_types_with_result = ['pass', 'shot', 'take_on', 'cross', 'corner_crossed', 'freekick_crossed']  # take_onã¯ãƒ‰ãƒªãƒ–ãƒ«ã§ã®ä»•æ›ã‘

success_rates_list = []
print("ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æˆåŠŸç‡ç‰¹å¾´é‡ã‚’è¨ˆç®—ä¸­...")

for (match_id, player_id), group in tqdm(relevant_actions.groupby(['match_id', 'player_id']), desc="Calculating success rates"):
    row_data = {'match_id': match_id, 'player_id': player_id}

    for action_type in action_types_with_result:
        type_actions = group[group['type_name'] == action_type] # å¯¾è±¡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º

        if len(type_actions) > 0:
            success_count = len(type_actions[type_actions['result_name'] == 'success'])
            total_count = len(type_actions)

            # æˆåŠŸç‡ã‚’è¨ˆç®—
            success_rate = success_count / total_count
            row_data[f'{action_type}_success_rate'] = success_rate
        else:
            # è©²å½“ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒãªã„å ´åˆã¯0
            row_data[f'{action_type}_success_rate'] = 0

    success_rates_list.append(row_data)

success_rates = pd.DataFrame(success_rates_list)

print(f"ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {success_rates.shape}")
display(success_rates.head(3))

# %% trusted=true
# è¿½åŠ ï¼ˆãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ãƒãƒ¼ã‚¸ã®ç›´å‰ï¼‰: é«˜åº¦ç‰¹å¾´é‡ã®ãƒãƒ¼ã‚¸
def _merge_many(df, parts):
    for part in parts:
        if part is None or (hasattr(part, "empty") and part.empty):
            continue
        df = df.merge(part, on=["match_id", "player_id"], how="left")
    return df

train_df = _merge_many(
    train_df,
    [
        nstep_block,
        second_assist,
        sca1,
        sca2,
        gca1,
        gca2,
        pass_geom,
        pass_latency,
        xpass_risk,
        # ğŸ†• æ–°ç‰¹å¾´é‡
        time_based_features,
        zone_based_features,
        network_centrality_features,
        extended_chain_features,
        dynamic_positioning_features,
    ],
)

test_df = _merge_many(
    test_df,
    [
        nstep_block,
        second_assist,
        sca1,
        sca2,
        gca1,
        gca2,
        pass_geom,
        pass_latency,
        xpass_risk,
        # ğŸ†• æ–°ç‰¹å¾´é‡
        time_based_features,
        zone_based_features,
        network_centrality_features,
        extended_chain_features,
        dynamic_positioning_features,
    ],
)

# Fill NA and typesï¼ˆã“ã“ã§æ¬ æã‚’æ½°ã™ï¼‰
count_cols = ["second_assist_count", "SCA_1", "SCA_2", "GCA_1", "GCA_2"]
for col in count_cols:
    if col in train_df.columns:
        train_df[col] = train_df[col].fillna(0).astype(int)
        test_df[col] = test_df[col].fillna(0).astype(int)

num_cols = [
    "nstep_to_shot",
    "nstep_xt_delta",
    "pass_dist_mean",
    "pass_dist_max",
    "to_goal_angle_abs_mean",
    "to_goal_dist_mean",
    "pass_to_shot_latency_mean",
    "pass_to_shot_latency_min",
    "risk_creativity_sum",
    "xpass_mean",
    "xpass_min",
    "pass_success_minus_xpass",
    "xpass_deep_mean",
    "xpass_box_mean",
]
for col in num_cols:
    if col in train_df.columns:
        train_df[col] = train_df[col].fillna(0.0)
        test_df[col] = test_df[col].fillna(0.0)

# ğŸ†• æ–°ç‰¹å¾´é‡ã®æ¬ æå€¤å‡¦ç†
new_feature_cols = [
    # æ™‚é–“å¸¯åˆ¥
    "first_half_actions", "second_half_actions", "final_15min_actions",
    "early_10min_actions", "time_weighted_intensity",
    # ã‚¾ãƒ¼ãƒ³åˆ¥
    "defensive_zone_actions", "middle_zone_actions", "attacking_zone_actions",
    "halfspace_left_actions", "halfspace_right_actions", "central_corridor_actions",
    "final_third_penetrations", "box_entries",
    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¸­å¿ƒæ€§
    "betweenness_centrality", "closeness_centrality", "degree_centrality",
    "pass_receiver_diversity", "unique_pass_partners",
    # æ‹¡å¼µé€£é–
    "longchain_to_shot", "longchain_xt_delta",
    # å‹•çš„ãƒã‚¸ã‚·ãƒ§ãƒ‹ãƒ³ã‚°
    "position_variance_x", "position_variance_y", "position_range_x",
    "position_range_y", "avg_action_distance",
]
for col in new_feature_cols:
    if col in train_df.columns:
        train_df[col] = train_df[col].fillna(0.0)
    if col in test_df.columns:
        test_df[col] = test_df[col].fillna(0.0)

print("Advanced features merged (early).")
print(f"ğŸ†• æ–°ç‰¹å¾´é‡ {len(new_feature_cols)}å€‹ã‚’è¿½åŠ ã—ã¾ã—ãŸ")


# %% trusted=true
# è¿½åŠ ï¼ˆall_featureså®šç¾©ã‚»ãƒ«ã®ç›´å¾Œã‚’æƒ³å®šï¼‰: all_features ã«é«˜åº¦ç‰¹å¾´é‡ã‚’å«ã‚ã‚‹
advanced_candidates = [
    "second_assist_count",
    "SCA_1",
    "SCA_2",
    "GCA_1",
    "GCA_2",
    "nstep_to_shot",
    "nstep_xt_delta",
    "pass_dist_mean",
    "pass_dist_max",
    "to_goal_angle_abs_mean",
    "to_goal_dist_mean",
    "pass_to_shot_latency_mean",
    "pass_to_shot_latency_min",
    "risk_creativity_sum",
    "xpass_mean",
    "xpass_min",
    "pass_success_minus_xpass",
    "xpass_deep_mean",
    "xpass_box_mean",
    # æ™‚ç³»åˆ—ãƒˆãƒ¬ãƒ³ãƒ‰
    "xAG_expanding_mean",
    "xAG_rolling3_mean",
    "xAG_diff_prev",
    # ğŸ†• æ–°ç‰¹å¾´é‡
    "first_half_actions", "second_half_actions", "final_15min_actions",
    "early_10min_actions", "time_weighted_intensity",
    "defensive_zone_actions", "middle_zone_actions", "attacking_zone_actions",
    "halfspace_left_actions", "halfspace_right_actions", "central_corridor_actions",
    "final_third_penetrations", "box_entries",
    "betweenness_centrality", "closeness_centrality", "degree_centrality",
    "pass_receiver_diversity", "unique_pass_partners",
    "longchain_to_shot", "longchain_xt_delta",
    "position_variance_x", "position_variance_y", "position_range_x",
    "position_range_y", "avg_action_distance",
]
advanced_features = [c for c in advanced_candidates if c in train_df.columns]
try:
    all_features = list(dict.fromkeys(all_features + advanced_features))
except NameError:
    _advanced_features_pending = advanced_features
print(f"è¿½åŠ ã•ã‚ŒãŸé«˜åº¦ç‰¹å¾´é‡: {len(advanced_features)}å€‹ (ğŸ†•æ–°ç‰¹å¾´é‡å«ã‚€)")


# %% colab={"base_uri": "https://localhost:8080/", "height": 310} id="yjC2SnrIiuhW" outputId="ac96bf5f-28cf-4e2f-a395-3c52f5b32736" trusted=true
# ä½ç½®ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡
# ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ä¸Šã§ã®æ´»å‹•ã‚¨ãƒªã‚¢ã‚’åˆ†æã—ã€æ”»æ’ƒçš„ãªé¸æ‰‹ã‚’è­˜åˆ¥

print("ä½ç½®ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ã‚’è¨ˆç®—ä¸­...")

# ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’3ã¤ã®ã‚¨ãƒªã‚¢ã«åˆ†å‰²ï¼ˆxåº§æ¨™ãƒ™ãƒ¼ã‚¹ï¼‰
def categorize_position(x):
    """xåº§æ¨™ã‹ã‚‰ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚¨ãƒªã‚¢ã‚’åˆ¤å®š"""
    if x < 35:
        return 'defensive'  # å®ˆå‚™çš„ã‚¨ãƒªã‚¢
    elif x < 70:
        return 'midfield'   # ä¸­ç›¤ã‚¨ãƒªã‚¢
    else:
        return 'attacking'  # æ”»æ’ƒçš„ã‚¨ãƒªã‚¢

# å„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚¨ãƒªã‚¢ã‚’åˆ¤å®š
relevant_actions['start_zone'] = relevant_actions['start_x'].apply(categorize_position)

# ã‚¾ãƒ¼ãƒ³åˆ¥ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°ã‚’é›†è¨ˆ
zone_actions = (
    relevant_actions
    .pivot_table(
        index=['match_id', 'player_id'],
        columns='start_zone',
        values='period_id',
        aggfunc='count',
        fill_value=0
    )
    .add_prefix('zone_')
    .add_suffix('_actions')
    .reset_index()
)

# å„ã‚¾ãƒ¼ãƒ³ã§ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ¯”ç‡ã‚’è¨ˆç®—
zone_actions['total_actions'] = (
    zone_actions.get('zone_defensive_actions', 0) +
    zone_actions.get('zone_midfield_actions', 0) +
    zone_actions.get('zone_attacking_actions', 0)
)

zone_actions['zone_attacking_actions_ratio'] = np.where(
    zone_actions['total_actions'] > 0,
    zone_actions.get('zone_attacking_actions', 0) / zone_actions['total_actions'],
    0
)

zone_actions['zone_midfield_actions_ratio'] = np.where(
    zone_actions['total_actions'] > 0,
    zone_actions.get('zone_midfield_actions', 0) / zone_actions['total_actions'],
    0
)

zone_actions['zone_defensive_actions_ratio'] = np.where(
    zone_actions['total_actions'] > 0,
    zone_actions.get('zone_defensive_actions', 0) / zone_actions['total_actions'],
    0
)
zone_actions = zone_actions.drop(columns=['total_actions'])

print(f"\nã‚¾ãƒ¼ãƒ³åˆ¥ã‚¢ã‚¯ã‚·ãƒ§ãƒ³çµ±è¨ˆ:")
for zone in ['defensive', 'midfield', 'attacking']:
    col_name = f'zone_{zone}_actions'
    if col_name in zone_actions.columns:
        mean_val = zone_actions[col_name].mean()
        print(f"  {zone:10s}ã‚¨ãƒªã‚¢: å¹³å‡ {mean_val:.1f} ã‚¢ã‚¯ã‚·ãƒ§ãƒ³")

print(f"\nä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {zone_actions.shape}")
display(zone_actions.head(3))

# %% colab={"base_uri": "https://localhost:8080/", "height": 219} id="W2m7WIjGiuhW" outputId="87b1e9c5-034e-4850-89e4-65e627031806" trusted=true
# æ™‚é–“æ­£è¦åŒ–ç‰¹å¾´é‡
# å‡ºå ´æ™‚é–“ã«ã‚ˆã‚‹å½±éŸ¿ã‚’æ’é™¤ã—ã€å…¬å¹³ãªæ¯”è¼ƒã‚’å¯èƒ½ã«ã™ã‚‹

print("æ™‚é–“æ­£è¦åŒ–ç‰¹å¾´é‡ã‚’è¨ˆç®—ä¸­...")

per_minute_features = match_player_stats.copy()

# å…¨ä½“ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°ã®æ­£è¦åŒ–
per_minute_features['action_count_per_minute'] = np.where(
    per_minute_features['minutes_played'] > 0,
    per_minute_features['action_count'] / per_minute_features['minutes_played'],
    0
)

# ã‚´ãƒ¼ãƒ«æ•°ã‚’ãƒãƒ¼ã‚¸ãƒ»ã‚¼ãƒ­åŸ‹ã‚
per_minute_features = per_minute_features.merge(
    match_player_goals,
    on=['match_id', 'player_id'],
    how='left'
)
per_minute_features['goal_count'] = per_minute_features['goal_count'].fillna(0)

# ã‚´ãƒ¼ãƒ«æ•°ã®æ­£è¦åŒ–
per_minute_features['goal_count_per_minute'] = np.where(
    per_minute_features['minutes_played'] > 0,
    per_minute_features['goal_count'] / per_minute_features['minutes_played'],
    0
)

# ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—åˆ¥ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°ã‚’ãƒãƒ¼ã‚¸ãƒ»ã‚¼ãƒ­åŸ‹ã‚
per_minute_features = per_minute_features.merge(
    action_type_stats,
    on=['match_id', 'player_id'],
    how='left'
)
action_type_cols = [col for col in per_minute_features.columns if col.startswith('type_') and col.endswith('_count')] # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—åˆ¥ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°ã®åˆ—
for col in action_type_cols:
    per_minute_features[col] = per_minute_features[col].fillna(0)

# ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—åˆ¥ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°ã®æ­£è¦åŒ–
for col in action_type_cols:
    new_col_name = col.replace('_count', '_count_per_minute')
    per_minute_features[new_col_name] = np.where(
        per_minute_features['minutes_played'] > 0,
        per_minute_features[col] / per_minute_features['minutes_played'],
        0
    )

# æ–°è¦ä½œæˆã—ãŸåˆ—ã®ã¿ã«çµã‚Šè¾¼ã¿
per_minute_cols = [col for col in per_minute_features.columns if col.endswith('_per_minute')]
per_minute_features = per_minute_features[['match_id', 'player_id'] + per_minute_cols]

print(f"\nä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {per_minute_features.shape}")
display(per_minute_features.head(3))

# %% colab={"base_uri": "https://localhost:8080/", "height": 199} id="5m47SqPMiuhW" outputId="5ceb8fa8-cc5a-478a-bb4b-7be3075b35f1" trusted=true
# æ”»æ’ƒ/å®ˆå‚™ãƒãƒ©ãƒ³ã‚¹ç‰¹å¾´é‡
# é¸æ‰‹ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ«ã‚’å®šé‡åŒ–ã—ã€æ”»æ’ƒçš„ãªé¸æ‰‹ã‚’è­˜åˆ¥

print("æ”»æ’ƒ/å®ˆå‚™ãƒãƒ©ãƒ³ã‚¹ç‰¹å¾´é‡ã‚’è¨ˆç®—ä¸­...")

# æ”»æ’ƒ/å®ˆå‚™ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®å®šç¾©
offensive_actions = ['shot', 'pass', 'cross', 'take_on', 'dribble']
defensive_actions = ['tackle', 'interception', 'clearance']

# å„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®åˆ†é¡ã‚’ä»˜ä¸
def categorize_ad(action):
    if action in offensive_actions:
        return 'offensive'
    elif action in defensive_actions:
        return 'defensive'
    else:
        return None

relevant_actions['action_type'] = relevant_actions['type_name'].apply(categorize_ad)

# æ”»å®ˆåˆ¥ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°ã‚’é›†è¨ˆ
offense_defense_balance = (
    relevant_actions
    .pivot_table(
        index=['match_id', 'player_id'],
        columns='action_type',
        values='period_id',
        aggfunc='count',
        fill_value=0
    )
    .add_prefix('type_')
    .add_suffix('_actions')
    .reset_index()
)

# æ”»å®ˆãƒãƒ©ãƒ³ã‚¹æŒ‡æ¨™ã‚’è¨ˆç®—
offense_defense_balance['total_actions'] = (
    offense_defense_balance.get('type_offensive_actions', 0) +
    offense_defense_balance.get('type_defensive_actions', 0)
)

offense_defense_balance['type_offensive_action_ratio'] = np.where(
    offense_defense_balance['total_actions'] > 0,
    offense_defense_balance['type_offensive_actions'] / offense_defense_balance['total_actions'],
    0
)

offense_defense_balance = offense_defense_balance.drop(columns=['total_actions'])

print(f"\nä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {offense_defense_balance.shape}")
display(offense_defense_balance.head(3))

# %% colab={"base_uri": "https://localhost:8080/", "height": 199} id="sLKXukn5pIUx" outputId="3c3f0531-8bd0-4e66-9c7e-dacfa3a0b86a" trusted=true
# æ™‚ç³»åˆ—è¦ç´ ã‚’åŠ å‘³ã—ãŸç‰¹å¾´é‡
# xAGã®å®šç¾©ã‚’è€ƒãˆã‚‹ã¨ã€ãƒ‘ã‚¹ã—ãŸå‘³æ–¹ã®ã‚·ãƒ¥ãƒ¼ãƒˆãŒå¤šã„ã»ã©xAGã¯é«˜ããªã‚‹
# ãã“ã§ã€æ¬¡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚·ãƒ¥ãƒ¼ãƒˆã§ã‚ã‚‹ãƒ‘ã‚¹ã®æ•°ã‚’é¸æ‰‹-è©¦åˆã”ã¨ã«é›†è¨ˆã™ã‚‹

print("æ¬¡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚·ãƒ¥ãƒ¼ãƒˆã®ãƒ‘ã‚¹æ•°ã‚’è¨ˆç®—ä¸­...")

# ç›´å¾Œã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—ã‚’ã‚·ãƒ•ãƒˆã§ä»˜ä¸
relevant_actions = relevant_actions.sort_values(['match_id', 'period_id', 'time_seconds'])  # å¿µã®ç‚ºã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ™‚é–“ã§ã‚½ãƒ¼ãƒˆ
relevant_actions["next_type"] = relevant_actions.groupby("match_id")["type_name"].shift(-1)

# pass â†’ shot ã¨ãªã£ã¦ã„ã‚‹è¡Œã‚’æŠ½å‡º
pass_to_shot = relevant_actions[
    (relevant_actions["type_name"] == "pass") &
    (relevant_actions["next_type"] == "shot")
]

# match_id, player_idã”ã¨ã«ã‚«ã‚¦ãƒ³ãƒˆ
pass_leads_to_shot = (
    pass_to_shot.groupby(["match_id", "player_id"])
    .size()
    .reset_index(name="pass_leads_to_shot")
)

print(f"\nä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {pass_leads_to_shot.shape}")
display(pass_leads_to_shot.head(3))

# %% trusted=true
# ãƒ—ãƒ­ã‚°ãƒ¬ãƒƒã‚·ãƒ–/ãƒ‡ã‚£ãƒ¼ãƒ—ç³»ã®ç‰¹å¾´é‡
print("ãƒ—ãƒ­ã‚°ãƒ¬ãƒƒã‚·ãƒ–/ãƒ‡ã‚£ãƒ¼ãƒ—ç³»ç‰¹å¾´é‡ã‚’è¨ˆç®—ä¸­...")

PASS_PROGRESSIVE_TYPES = {"pass", "cross", "freekick_crossed", "corner_crossed"}
CARRY_PROGRESSIVE_TYPES = {"carry", "dribble", "take_on"}

progressive_pass_actions = relevant_actions[
    relevant_actions["type_name"].isin(PASS_PROGRESSIVE_TYPES)
].copy()

if not progressive_pass_actions.empty:
    dx = (progressive_pass_actions["end_x"] - progressive_pass_actions["start_x"]).fillna(0.0)
    dy = (progressive_pass_actions["end_y"] - progressive_pass_actions["start_y"]).fillna(0.0)
else:
    dx = pd.Series(dtype=float)
    dy = pd.Series(dtype=float)

progressive_pass_actions["delta_x"] = dx
progressive_pass_actions["delta_total"] = np.hypot(dx, dy)
progressive_pass_actions["is_completed"] = progressive_pass_actions["result_name"] == "success"

FINAL_THIRD_X = 70.0
DEEP_COMPLETION_X = 85.0
PENALTY_AREA_X = 88.0
PROGRESS_ADVANCE_MIN = 10.0

progressive_pass_actions["is_progressive"] = (
    (progressive_pass_actions["delta_x"] >= PROGRESS_ADVANCE_MIN)
    | (
        (progressive_pass_actions["start_x"] < FINAL_THIRD_X)
        & (progressive_pass_actions["end_x"] >= FINAL_THIRD_X)
    )
    | (progressive_pass_actions["end_x"] >= DEEP_COMPLETION_X)
)

progressive_pass_actions["progressive_attempt"] = progressive_pass_actions["is_progressive"].astype(int)
progressive_pass_actions["progressive_success"] = (
    progressive_pass_actions["is_progressive"] & progressive_pass_actions["is_completed"]
).astype(int)
progressive_pass_actions["progressive_distance"] = np.where(
    progressive_pass_actions["is_progressive"],
    progressive_pass_actions["delta_total"],
    0.0,
)

progressive_pass_actions["is_final_third_entry"] = (
    progressive_pass_actions["is_completed"]
    & (progressive_pass_actions["start_x"] < FINAL_THIRD_X)
    & (progressive_pass_actions["end_x"] >= FINAL_THIRD_X)
)
progressive_pass_actions["is_deep_completion"] = (
    progressive_pass_actions["is_completed"]
    & (progressive_pass_actions["end_x"] >= DEEP_COMPLETION_X)
)
progressive_pass_actions["is_penalty_area_entry"] = (
    progressive_pass_actions["is_completed"]
    & (progressive_pass_actions["end_x"] >= PENALTY_AREA_X)
)

pass_progressive_features = (
    progressive_pass_actions.groupby(["match_id", "player_id"]).agg(
        progressive_pass_count=("progressive_attempt", "sum"),
        progressive_pass_success=("progressive_success", "sum"),
        progressive_pass_distance_total=("progressive_distance", "sum"),
        final_third_entry_count=("is_final_third_entry", "sum"),
        deep_completion_count=("is_deep_completion", "sum"),
        penalty_area_entry_count=("is_penalty_area_entry", "sum"),
    )
    .reset_index()
)

if "progressive_pass_count" in pass_progressive_features:
    pass_progressive_features["progressive_pass_success_rate"] = np.where(
        pass_progressive_features["progressive_pass_count"] > 0,
        pass_progressive_features["progressive_pass_success"]
        / pass_progressive_features["progressive_pass_count"],
        0.0,
    )
    pass_progressive_features["progressive_pass_distance_mean"] = np.where(
        pass_progressive_features["progressive_pass_count"] > 0,
        pass_progressive_features["progressive_pass_distance_total"]
        / pass_progressive_features["progressive_pass_count"],
        0.0,
    )
else:
    pass_progressive_features["progressive_pass_success_rate"] = []
    pass_progressive_features["progressive_pass_distance_mean"] = []

carry_actions = relevant_actions[
    relevant_actions["type_name"].isin(CARRY_PROGRESSIVE_TYPES)
].copy()

if not carry_actions.empty:
    carry_actions["end_x"] = carry_actions["end_x"].fillna(carry_actions["start_x"])
    carry_actions["end_y"] = carry_actions["end_y"].fillna(carry_actions["start_y"])
    carry_dx = (carry_actions["end_x"] - carry_actions["start_x"]).fillna(0.0)
    carry_dy = (carry_actions["end_y"] - carry_actions["start_y"]).fillna(0.0)
    carry_actions["delta_total"] = np.hypot(carry_dx, carry_dy)
    carry_actions["delta_x"] = carry_dx
    carry_actions["is_success"] = carry_actions["result_name"] == "success"
    carry_actions["is_progressive"] = (
        (carry_actions["delta_x"] >= 5.0)
        | (
            (carry_actions["start_x"] < FINAL_THIRD_X)
            & (carry_actions["end_x"] >= FINAL_THIRD_X)
        )
    )
    carry_actions["progressive_carry_attempt"] = carry_actions["is_progressive"].astype(int)
    carry_actions["progressive_carry_success"] = (
        carry_actions["is_progressive"] & carry_actions["is_success"]
    ).astype(int)
    carry_actions["progressive_carry_distance"] = np.where(
        carry_actions["is_progressive"], carry_actions["delta_total"], 0.0
    )

    carry_progressive_features = (
        carry_actions.groupby(["match_id", "player_id"]).agg(
            progressive_carry_count=("progressive_carry_attempt", "sum"),
            progressive_carry_success=("progressive_carry_success", "sum"),
            progressive_carry_distance_total=("progressive_carry_distance", "sum"),
        )
        .reset_index()
    )

    carry_progressive_features["progressive_carry_success_rate"] = np.where(
        carry_progressive_features["progressive_carry_count"] > 0,
        carry_progressive_features["progressive_carry_success"]
        / carry_progressive_features["progressive_carry_count"],
        0.0,
    )
    carry_progressive_features["progressive_carry_distance_mean"] = np.where(
        carry_progressive_features["progressive_carry_count"] > 0,
        carry_progressive_features["progressive_carry_distance_total"]
        / carry_progressive_features["progressive_carry_count"],
        0.0,
    )
else:
    carry_progressive_features = pd.DataFrame(
        columns=[
            "match_id",
            "player_id",
            "progressive_carry_count",
            "progressive_carry_success",
            "progressive_carry_distance_total",
            "progressive_carry_success_rate",
            "progressive_carry_distance_mean",
        ]
    )

progressive_features = pass_progressive_features.merge(
    carry_progressive_features,
    on=["match_id", "player_id"],
    how="outer",
).fillna(0.0)

print(f"ä½œæˆã—ãŸãƒ—ãƒ­ã‚°ãƒ¬ãƒƒã‚·ãƒ–ç³»ç‰¹å¾´é‡: {progressive_features.shape}")
display(progressive_features.head(3))


# %% trusted=true
# å­¦ç¿’å‹ xT (Expected Threat) ç‰¹å¾´é‡
print("å­¦ç¿’å‹xT (value iteration) ç‰¹å¾´é‡ã‚’è¨ˆç®—ä¸­...")

# ã‚°ãƒªãƒƒãƒ‰å®šç¾© (16x12)
GRID_X_EDGES = np.linspace(0, 105, 17)
GRID_Y_EDGES = np.linspace(0, 68, 13)
NUM_X = len(GRID_X_EDGES) - 1
NUM_Y = len(GRID_Y_EDGES) - 1
NUM_ZONES = NUM_X * NUM_Y

def map_to_zone(x_array: np.ndarray, y_array: np.ndarray) -> np.ndarray:
    """Map coordinates to xT grid zone indices (0-191)."""
    x_idx = np.clip(np.digitize(x_array, GRID_X_EDGES) - 1, 0, NUM_X - 1)
    y_idx = np.clip(np.digitize(y_array, GRID_Y_EDGES) - 1, 0, NUM_Y - 1)
    return (y_idx * NUM_X + x_idx).astype(int)

# åˆ©ç”¨ã™ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç¨®åˆ¥
distribution_actions = {
    "pass", "cross", "throw_in", "corner_crossed", "freekick_crossed",
    "carry", "take_on", "dribble", "goal_kick", "clearance"
}
shot_actions = {"shot", "shot_penalty", "shot_freekick"}

# å­¦ç¿’ç”¨xTã¯trainã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ã¿ã‚’ä½¿ç”¨ï¼ˆãƒªãƒ¼ã‚¯é˜²æ­¢ï¼‰
train_match_ids_xt = set(train_df["match_id"])
train_actions = relevant_actions[relevant_actions["match_id"].isin(train_match_ids_xt)].copy()

# åº§æ¨™æ¬ æã‚’ã‚¼ãƒ­åŸ‹ã‚ã—ã¦ã‚¾ãƒ¼ãƒ³ç®—å‡ºç”¨ã«æº–å‚™ï¼ˆtrainã®ã¿ï¼‰
start_x_train = train_actions["start_x"].fillna(0).to_numpy()
start_y_train = train_actions["start_y"].fillna(0).to_numpy()
start_zones_train = map_to_zone(start_x_train, start_y_train)

transition_counts = np.zeros((NUM_ZONES, NUM_ZONES), dtype=np.float64)
shot_counts = np.zeros(NUM_ZONES, dtype=np.float64)
goal_counts = np.zeros(NUM_ZONES, dtype=np.float64)
ball_loss_counts = np.zeros(NUM_ZONES, dtype=np.float64)

# ã‚·ãƒ§ãƒƒãƒˆé–¢é€£çµ±è¨ˆï¼ˆtrainã®ã¿ï¼‰
shot_mask_train = train_actions["type_name"].isin(shot_actions)
if shot_mask_train.any():
    shot_zones_train = start_zones_train[shot_mask_train.to_numpy()]
    shot_counts += np.bincount(shot_zones_train, minlength=NUM_ZONES)
    goal_flags_train = train_actions.loc[shot_mask_train, "result_name"].eq("success").to_numpy(dtype=np.float64)
    goal_counts += np.bincount(shot_zones_train, weights=goal_flags_train, minlength=NUM_ZONES)

# ãƒ‘ã‚¹ãƒ»ã‚­ãƒ£ãƒªãƒ¼ç­‰ã®ãƒã‚¼ãƒƒã‚·ãƒ§ãƒ³é·ç§»çµ±è¨ˆï¼ˆtrainã®ã¿ï¼‰
move_mask_train = train_actions["type_name"].isin(distribution_actions)
if move_mask_train.any():
    move_actions_train = train_actions.loc[move_mask_train].copy()
    move_start_zones_train = map_to_zone(
        move_actions_train["start_x"].fillna(0).to_numpy(),
        move_actions_train["start_y"].fillna(0).to_numpy(),
    )
    move_success_train = move_actions_train["result_name"].eq("success").to_numpy()

    if (~move_success_train).any():
        ball_loss_counts += np.bincount(move_start_zones_train[~move_success_train], minlength=NUM_ZONES)

    valid_success_idx_train = move_success_train & move_actions_train["end_x"].notna().to_numpy() & move_actions_train["end_y"].notna().to_numpy()
    if valid_success_idx_train.any():
        success_start_zones_train = move_start_zones_train[valid_success_idx_train]
        success_end_zones_train = map_to_zone(
            move_actions_train.loc[valid_success_idx_train, "end_x"].to_numpy(),
            move_actions_train.loc[valid_success_idx_train, "end_y"].to_numpy(),
        )
        np.add.at(transition_counts, (success_start_zones_train, success_end_zones_train), 1.0)

# xTä¾¡å€¤åå¾©ï¼ˆtrainçµ±è¨ˆã§æ¨å®šï¼‰
transition_totals = transition_counts.sum(axis=1)
total_counts = transition_totals + shot_counts + ball_loss_counts
safe_totals = np.where(total_counts == 0, 1.0, total_counts)

transition_probs = np.divide(
    transition_counts,
    safe_totals[:, None],
    out=np.zeros_like(transition_counts),
    where=safe_totals[:, None] > 0,
)
shot_prob = shot_counts / safe_totals
goal_given_shot = np.divide(
    goal_counts,
    shot_counts,
    out=np.zeros_like(goal_counts),
    where=shot_counts > 0,
)
immediate_reward = shot_prob * goal_given_shot

gamma = 0.95
xt_values = immediate_reward.copy()
max_iterations = 500
for iteration in range(max_iterations):
    updated = immediate_reward + gamma * transition_probs.dot(xt_values)
    max_delta = np.max(np.abs(updated - xt_values))
    xt_values = updated
    if max_delta < 1e-6:
        break
else:
    iteration += 1  # åæŸã—ãªã‹ã£ãŸå ´åˆã®ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿

print(f"å­¦ç¿’å‹xT value iteration: {iteration + 1} step(s), max_delta={max_delta:.2e}")
print(f"xTå€¤ã®ç¯„å›²: min={xt_values.min():.5f}, max={xt_values.max():.5f}")

# ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ™ãƒ¼ã‚¹ã®xTç‰¹å¾´é‡ä»˜ä¸ï¼ˆtrainã§å­¦ç¿’ã—ãŸxt_valuesã‚’å…¨è¡Œã¸é©ç”¨ï¼‰
# å…¨è¡Œï¼ˆtrain+testï¼‰ã®é–‹å§‹ã‚¾ãƒ¼ãƒ³ã‚’ç®—å‡º
start_x_all = relevant_actions["start_x"].fillna(0).to_numpy()
start_y_all = relevant_actions["start_y"].fillna(0).to_numpy()
start_zones_all = map_to_zone(start_x_all, start_y_all)

end_x = relevant_actions["end_x"].to_numpy()
end_y = relevant_actions["end_y"].to_numpy()
has_end_coords = np.isfinite(end_x) & np.isfinite(end_y)
end_zones_all = np.zeros(len(relevant_actions), dtype=int)
if has_end_coords.any():
    end_zones_all[has_end_coords] = map_to_zone(end_x[has_end_coords], end_y[has_end_coords])

start_values = xt_values[start_zones_all]
end_values = np.zeros(len(relevant_actions), dtype=np.float64)
end_values[has_end_coords] = xt_values[end_zones_all[has_end_coords]]

success_flag = relevant_actions["result_name"].eq("success").astype(int).to_numpy()
end_values_on_success = np.where(success_flag == 1, end_values, 0.0)

relevant_actions["xt_learned_start"] = start_values
relevant_actions["xt_learned_end"] = end_values
relevant_actions["xt_learned_delta"] = end_values_on_success - start_values
relevant_actions["xt_learned_positive_delta"] = np.clip(relevant_actions["xt_learned_delta"], 0.0, None)
relevant_actions["xt_learned_success"] = success_flag
relevant_actions["xt_learned_end_on_success"] = np.where(success_flag == 1, end_values, np.nan)
relevant_actions["xt_learned_delta_on_success"] = np.where(success_flag == 1, relevant_actions["xt_learned_delta"], np.nan)

xt_learned_features = (
    relevant_actions.groupby(["match_id", "player_id"])
    .agg(
        xt_learned_start_mean=("xt_learned_start", "mean"),
        xt_learned_start_max=("xt_learned_start", "max"),
        xt_learned_delta_sum=("xt_learned_delta", "sum"),
        xt_learned_delta_mean=("xt_learned_delta", "mean"),
        xt_learned_positive_delta_sum=("xt_learned_positive_delta", "sum"),
        xt_learned_positive_delta_mean=("xt_learned_positive_delta", "mean"),
        xt_learned_success_rate=("xt_learned_success", "mean"),
        xt_learned_action_count=("xt_learned_success", "count"),
        xt_learned_end_success_mean=("xt_learned_end_on_success", "mean"),
        xt_learned_delta_success_mean=("xt_learned_delta_on_success", "mean"),
    )
    .reset_index()
)

xt_learned_feature_cols = [col for col in xt_learned_features.columns if col not in {"match_id", "player_id"}]

train_df = train_df.merge(xt_learned_features, on=["match_id", "player_id"], how="left")
test_df = test_df.merge(xt_learned_features, on=["match_id", "player_id"], how="left")

train_df[xt_learned_feature_cols] = train_df[xt_learned_feature_cols].fillna(0.0)
test_df[xt_learned_feature_cols] = test_df[xt_learned_feature_cols].fillna(0.0)

train_df["xt_learned_action_count"] = train_df["xt_learned_action_count"].astype(int)
test_df["xt_learned_action_count"] = test_df["xt_learned_action_count"].astype(int)

print("å­¦ç¿’å‹xTç‰¹å¾´é‡ï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰:")
display(xt_learned_features.head(3))



# %% [markdown] id="hHxhgz7ypDWn"
# ## Possession-Level Progression Features
#
# Learned xT highlights forward threat, so we aggregate possession speed and directness as complementary signals.
#

# %% trusted=true
print("Calculating possession progression features...")

pos_actions = (
    relevant_actions
    .reset_index(drop=False)
    .sort_values(["match_id", "period_id", "time_seconds", "index"], kind="mergesort")
    .rename(columns={"index": "action_index"})
    .copy()
)
pos_actions["time_seconds"] = pos_actions["time_seconds"].fillna(0.0)
pos_actions["team_id"] = pos_actions["team_id"].fillna("unknown_team")
pos_actions["new_match"] = pos_actions["match_id"].ne(pos_actions["match_id"].shift())
pos_actions["same_team_prev"] = pos_actions["team_id"].eq(pos_actions["team_id"].shift())
pos_actions["prev_success"] = pos_actions["result_name"].shift().eq("success")
pos_actions["time_diff"] = pos_actions.groupby("match_id")["time_seconds"].diff().fillna(0.0)
pos_actions["new_possession"] = (
    pos_actions["new_match"]
    | (~pos_actions["same_team_prev"].fillna(False))
    | (~pos_actions["prev_success"].fillna(True))
    | (pos_actions["time_diff"] > 15.0)
)
pos_actions.loc[pos_actions.index[0], "new_possession"] = True
pos_actions["possession_id"] = pos_actions["new_possession"].cumsum().astype(int)
pos_actions["possession_event_index"] = pos_actions.groupby("possession_id").cumcount() + 1

pos_group = pos_actions.groupby("possession_id").agg(
    match_id=("match_id", "first"),
    team_id=("team_id", "first"),
    start_x=("start_x", "first"),
    start_y=("start_y", "first"),
    end_x=("end_x", "last"),
    end_y=("end_y", "last"),
    start_time=("time_seconds", "first"),
    end_time=("time_seconds", "last"),
    action_count=("player_id", "count"),
    unique_players=("player_id", "nunique"),
    xt_positive_sum=("xt_learned_positive_delta", "sum"),
    xt_delta_sum=("xt_learned_delta", "sum"),
).reset_index()

pos_group["duration"] = (pos_group["end_time"] - pos_group["start_time"]).clip(lower=1.0)
pos_group["delta_x"] = pos_group["end_x"] - pos_group["start_x"]
pos_group["delta_y"] = pos_group["end_y"] - pos_group["start_y"]
pos_group["ground_distance"] = np.sqrt(np.square(pos_group["delta_x"]) + np.square(pos_group["delta_y"]))
pos_group["directness"] = np.divide(
    pos_group["delta_x"],
    pos_group["ground_distance"],
    out=np.zeros_like(pos_group["delta_x"]),
    where=pos_group["ground_distance"] > 0,
)
pos_group["speed_x"] = pos_group["delta_x"] / pos_group["duration"]
pos_group["speed_ground"] = pos_group["ground_distance"] / pos_group["duration"]
pos_group["xt_positive_per_second"] = np.divide(
    pos_group["xt_positive_sum"],
    pos_group["duration"],
    out=np.zeros_like(pos_group["xt_positive_sum"]),
    where=pos_group["duration"] > 0,
)

final_third_threshold = 70.0
final_third_steps = (
    pos_actions[pos_actions["end_x"].ge(final_third_threshold)]
    .groupby("possession_id")["possession_event_index"]
    .min()
)
final_third_times = (
    pos_actions[pos_actions["end_x"].ge(final_third_threshold)]
    .groupby("possession_id")["time_seconds"]
    .min()
)
pos_group["final_third_entry_step"] = pos_group["possession_id"].map(final_third_steps)
pos_group["final_third_entry_flag"] = pos_group["final_third_entry_step"].notna().astype(float)
pos_group["final_third_entry_time"] = pos_group["possession_id"].map(final_third_times)
pos_group["time_to_final_third"] = (
    pos_group["final_third_entry_time"] - pos_group["start_time"]
).where(pos_group["final_third_entry_flag"] > 0)

pos_player = (
    pos_actions[["match_id", "player_id", "possession_id"]]
    .drop_duplicates()
    .merge(
        pos_group[[
            "possession_id",
            "duration",
            "ground_distance",
            "directness",
            "speed_x",
            "speed_ground",
            "xt_positive_per_second",
            "xt_positive_sum",
            "xt_delta_sum",
            "action_count",
            "final_third_entry_flag",
            "final_third_entry_step",
            "time_to_final_third",
        ]],
        on="possession_id",
        how="left",
    )
)

player_pos_features = pos_player.groupby(["match_id", "player_id"]).agg(
    possession_count=("possession_id", "nunique"),
    possession_duration_mean=("duration", "mean"),
    possession_ground_distance_mean=("ground_distance", "mean"),
    possession_directness_mean=("directness", "mean"),
    possession_speed_x_mean=("speed_x", "mean"),
    possession_speed_ground_mean=("speed_ground", "mean"),
    possession_xt_positive_per_second_mean=("xt_positive_per_second", "mean"),
    possession_xt_positive_sum=("xt_positive_sum", "sum"),
    possession_xt_delta_sum=("xt_delta_sum", "sum"),
    possession_actions_per_pos_mean=("action_count", "mean"),
    possession_final_third_rate=("final_third_entry_flag", "mean"),
    possession_final_third_step_mean=("final_third_entry_step", "mean"),
    possession_time_to_final_third_mean=("time_to_final_third", "mean"),
).reset_index()

possession_feature_cols = [
    col for col in player_pos_features.columns if col not in {"match_id", "player_id"}
]
numeric_possession_cols = [col for col in possession_feature_cols if col != "possession_count"]

train_df = train_df.merge(player_pos_features, on=["match_id", "player_id"], how="left")
test_df = test_df.merge(player_pos_features, on=["match_id", "player_id"], how="left")

train_df["possession_count"] = train_df["possession_count"].fillna(0).astype(int)
test_df["possession_count"] = test_df["possession_count"].fillna(0).astype(int)

if numeric_possession_cols:
    train_df[numeric_possession_cols] = train_df[numeric_possession_cols].fillna(0.0)
    test_df[numeric_possession_cols] = test_df[numeric_possession_cols].fillna(0.0)

print("Possession progression features added:", len(possession_feature_cols))


# %% [markdown]
# ## Pass Network Features
#
# Network-centric statistics to capture player roles within possession flow.
#

# %% trusted=true
print("Calculating pass network features...")

sorted_actions = (
    relevant_actions
    .reset_index(drop=False)
    .sort_values(["match_id", "period_id", "time_seconds", "index"], kind="mergesort")
    .rename(columns={"index": "action_index"})
    .copy()
)

sorted_actions["next_player_id"] = sorted_actions.groupby("match_id")["player_id"].shift(-1)
sorted_actions["next_team_id"] = sorted_actions.groupby("match_id")["team_id"].shift(-1)

success_pass_mask = (
    (sorted_actions["type_name"] == "pass")
    & sorted_actions["result_name"].eq("success")
    & sorted_actions["next_team_id"].notna()
    & sorted_actions["next_team_id"].eq(sorted_actions["team_id"])
)

pass_edges = sorted_actions.loc[success_pass_mask, [
    "match_id",
    "team_id",
    "player_id",
    "next_player_id",
    "start_x",
    "start_y",
    "end_x",
    "end_y"
]].copy()

pass_edges = pass_edges.dropna(subset=["player_id", "next_player_id"])
pass_edges["player_id"] = pass_edges["player_id"].astype(str)
pass_edges["next_player_id"] = pass_edges["next_player_id"].astype(str)

pass_edges["pass_distance"] = np.sqrt(
    (pass_edges["end_x"] - pass_edges["start_x"]) ** 2 +
    (pass_edges["end_y"] - pass_edges["start_y"]) ** 2
)
pass_edges["lateral_shift"] = pass_edges["end_y"] - pass_edges["start_y"]
pass_edges["switch_flag"] = pass_edges["lateral_shift"].abs() >= 20.0

player_edge_stats = (
    pass_edges.groupby(["match_id", "player_id"])
    .agg(
        pass_net_attempts=("next_player_id", "count"),
        pass_net_avg_distance=("pass_distance", "mean"),
        pass_net_switch_rate=("switch_flag", "mean"),
    )
    .reset_index()
)

receiver_stats = (
    pass_edges.groupby(["match_id", "next_player_id"])
    .size()
    .reset_index(name="pass_net_receive_count")
    .rename(columns={"next_player_id": "player_id"})
)

centrality_records = []
for (match_id, team_id), group in pass_edges.groupby(["match_id", "team_id"]):
    players = set(group["player_id"]) | set(group["next_player_id"])
    if not players:
        continue

    G = nx.DiGraph()
    for src, tgt in group[["player_id", "next_player_id"]].itertuples(index=False):
        if G.has_edge(src, tgt):
            G[src][tgt]["weight"] += 1.0
        else:
            G.add_edge(src, tgt, weight=1.0)

    for node in players:
        if node not in G:
            G.add_node(node)

    out_degree = dict(G.out_degree(weight="weight"))
    in_degree = dict(G.in_degree(weight="weight"))
    try:
        betweenness = nx.betweenness_centrality(G, weight="weight", normalized=True)
    except Exception:
        betweenness = {node: 0.0 for node in players}

    undirected = G.to_undirected()
    if undirected.number_of_edges() > 0:
        clustering = nx.clustering(undirected, weight="weight")
    else:
        clustering = {node: 0.0 for node in players}

    for node in players:
        centrality_records.append({
            "match_id": match_id,
            "player_id": node,
            "pass_net_out_degree": out_degree.get(node, 0.0),
            "pass_net_in_degree": in_degree.get(node, 0.0),
            "pass_net_betweenness": betweenness.get(node, 0.0),
            "pass_net_clustering": clustering.get(node, 0.0),
        })

centrality_df = pd.DataFrame(centrality_records)
if centrality_df.empty:
    centrality_df = pd.DataFrame(columns=[
        "match_id",
        "player_id",
        "pass_net_out_degree",
        "pass_net_in_degree",
        "pass_net_betweenness",
        "pass_net_clustering",
    ])

if player_edge_stats.empty:
    player_edge_stats = pd.DataFrame(columns=[
        "match_id",
        "player_id",
        "pass_net_attempts",
        "pass_net_avg_distance",
        "pass_net_switch_rate",
    ])

if receiver_stats.empty:
    receiver_stats = pd.DataFrame(columns=[
        "match_id",
        "player_id",
        "pass_net_receive_count",
    ])

pass_network_features = (
    centrality_df
    .merge(player_edge_stats, on=["match_id", "player_id"], how="outer")
    .merge(receiver_stats, on=["match_id", "player_id"], how="outer")
)

if not pass_network_features.empty:
    numeric_cols = [
        col for col in pass_network_features.columns
        if col not in {"match_id", "player_id"}
    ]
    pass_network_features[numeric_cols] = pass_network_features[numeric_cols].fillna(0.0)

pass_network_feature_cols = [
    col for col in pass_network_features.columns if col not in {"match_id", "player_id"}
]

train_df = train_df.merge(pass_network_features, on=["match_id", "player_id"], how="left")
test_df = test_df.merge(pass_network_features, on=["match_id", "player_id"], how="left")

if pass_network_feature_cols:
    train_df[pass_network_feature_cols] = train_df[pass_network_feature_cols].fillna(0.0)
    test_df[pass_network_feature_cols] = test_df[pass_network_feature_cols].fillna(0.0)

print("Pass network features added:", len(pass_network_feature_cols))


# %% [markdown]
#
# ## è¡Œç‚ºã‚¿ã‚¤ãƒ—åˆ¥ eÎ”xT ç‰¹å¾´é‡
#
# å­¦ç¿’æ¸ˆã¿xTã«åŸºã¥ãç©ºé–“ä¾¡å€¤ã¨è¡Œç‚ºã‚¿ã‚¤ãƒ—åˆ¥ã®æˆåŠŸç¢ºç‡ãƒ¢ãƒ‡ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã€ãƒªã‚¹ã‚¯èª¿æ•´ã•ã‚ŒãŸæœŸå¾…xTå¢—åˆ† (eÎ”xT) ã‚’ç®—å‡ºã—ã¾ã™ã€‚
#

# %% trusted=true

print("è¡Œç‚ºã‚¿ã‚¤ãƒ—åˆ¥ xPass ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã¦ã„ã¾ã™...")

xpass_action_groups = {
    "pass": ["pass"],
    "cross": ["cross"],
    "carry": ["carry"],
    "dribble": ["dribble", "take_on"],
    "free_kick": ["freekick_crossed"],
    "corner": ["corner_crossed"],
}

train_match_ids = set(train_df["match_id"])

if "action_index" not in relevant_actions.columns:
    relevant_actions = relevant_actions.copy()
    relevant_actions["action_index"] = np.arange(len(relevant_actions))

if "xpass_prob" not in relevant_actions.columns:
    relevant_actions["xpass_prob"] = np.nan
if "xpass_action_group" not in relevant_actions.columns:
    relevant_actions["xpass_action_group"] = pd.NA

relevant_actions["is_train_action"] = relevant_actions["match_id"].isin(train_match_ids)

relevant_actions["end_x_filled"] = relevant_actions["end_x"].fillna(relevant_actions["start_x"])
relevant_actions["end_y_filled"] = relevant_actions["end_y"].fillna(relevant_actions["start_y"])
relevant_actions["delta_x"] = (relevant_actions["end_x_filled"] - relevant_actions["start_x"]).fillna(0.0)
relevant_actions["delta_y"] = (relevant_actions["end_y_filled"] - relevant_actions["start_y"]).fillna(0.0)
relevant_actions["distance"] = np.hypot(relevant_actions["delta_x"], relevant_actions["delta_y"])
relevant_actions["abs_delta_y"] = relevant_actions["delta_y"].abs()

for col in ["xt_learned_start", "xt_learned_delta", "xt_learned_positive_delta"]:
    if col in relevant_actions.columns:
        relevant_actions[col] = relevant_actions[col].fillna(0.0)

xpass_predictions = []
xpass_training_summary = []
xpass_calibration_records = []

xpass_numeric_candidates = [
    "start_x",
    "start_y",
    "end_x_filled",
    "end_y_filled",
    "delta_x",
    "delta_y",
    "distance",
    "abs_delta_y",
    "time_seconds",
    "minutes_played",
    "period_id",
    "is_home",
    "xt_learned_start",
    "xt_learned_delta",
    "xt_learned_positive_delta",
]

xpass_categorical_candidates = [
    "team_name_short",
    "bodypart_name",
    "competition",
    "match_venue",
]

for action_group, action_names in xpass_action_groups.items():
    subset_idx = relevant_actions["type_name"].isin(action_names)
    action_subset = relevant_actions.loc[subset_idx].copy()

    if action_subset.empty:
        continue

    action_subset["is_success"] = action_subset["result_name"].eq("success").astype(int)
    action_subset["is_home"] = action_subset["is_home"].fillna(False).astype(int)

    numeric_candidates_local = list(xpass_numeric_candidates)
    if "is_starter" in action_subset.columns:
        action_subset["is_starter"] = action_subset["is_starter"].fillna(False).astype(int)
        numeric_candidates_local.append("is_starter")

    if "minutes_played" in action_subset.columns:
        action_subset["minutes_played"] = action_subset["minutes_played"].fillna(0.0)

    categorical_features = [col for col in xpass_categorical_candidates if col in action_subset.columns]
    for col in categorical_features:
        action_subset[col] = action_subset[col].fillna("missing").astype("category")

    numeric_features = [col for col in numeric_candidates_local if col in action_subset.columns]
    used_features = numeric_features + categorical_features

    train_subset = action_subset[action_subset["is_train_action"]].copy()
    test_subset = action_subset[~action_subset["is_train_action"]].copy()

    if train_subset.empty or train_subset["match_id"].nunique() < 2:
        fallback = float(train_subset["is_success"].mean()) if len(train_subset) else 0.5
        fallback = float(np.clip(fallback, 1e-4, 1 - 1e-4))
        action_subset.loc[train_subset.index, "xpass_prob"] = fallback
        action_subset.loc[test_subset.index, "xpass_prob"] = fallback
        action_subset["xpass_action_group"] = action_group
        relevant_actions.loc[action_subset.index, "xpass_prob"] = action_subset["xpass_prob"]
        relevant_actions.loc[action_subset.index, "xpass_action_group"] = action_subset["xpass_action_group"]
        xpass_predictions.append(
            action_subset[
                [
                    "match_id",
                    "player_id",
                    "action_index",
                    "xpass_action_group",
                    "xpass_prob",
                    "xt_learned_delta",
                    "xt_learned_start",
                    "is_train_action",
                ]
            ]
        )
        xpass_training_summary.append(
            {
                "action_type": action_group,
                "train_actions": len(train_subset),
                "test_actions": len(test_subset),
                "success_rate": float(train_subset["is_success"].mean()) if len(train_subset) else np.nan,
            }
        )
        continue

    n_splits = min(5, max(2, train_subset["match_id"].nunique()))
    gkf = GroupKFold(n_splits=n_splits)

    params = {
        "objective": "binary",
        "metric": "binary_logloss",
        "learning_rate": 0.03,
        "num_leaves": 25,
        "feature_fraction": 0.7,
        "bagging_fraction": 0.7,
        "bagging_freq": 1,
        "min_data_in_leaf": 64,
        "min_gain_to_split": 0.01,
        "lambda_l1": 0.1,
        "lambda_l2": 0.1,
        "seed": SEED,
        "verbose": -1,
    }

    oof_preds = np.zeros(len(train_subset), dtype=float)
    test_preds = np.zeros(len(test_subset), dtype=float) if len(test_subset) else None
    models = []

    for fold, (tr_idx, val_idx) in enumerate(gkf.split(train_subset, groups=train_subset["match_id"])):
        X_tr = train_subset.iloc[tr_idx][used_features]
        y_tr = train_subset.iloc[tr_idx]["is_success"]
        X_val = train_subset.iloc[val_idx][used_features]
        y_val = train_subset.iloc[val_idx]["is_success"]

        train_ds = lgb.Dataset(
            X_tr,
            label=y_tr,
            categorical_feature=categorical_features or None,
            free_raw_data=False,
        )
        val_ds = lgb.Dataset(
            X_val,
            label=y_val,
            reference=train_ds,
            categorical_feature=categorical_features or None,
            free_raw_data=False,
        )

        model = lgb.train(
            params,
            train_ds,
            valid_sets=[val_ds],
            num_boost_round=800,
            callbacks=[lgb.early_stopping(80), lgb.log_evaluation(0)],
        )
        models.append(model)
        fold_pred = model.predict(X_val, num_iteration=model.best_iteration)
        oof_preds[val_idx] = fold_pred
        if len(test_subset):
            test_preds += model.predict(test_subset[used_features], num_iteration=model.best_iteration)

    if len(test_subset):
        test_preds = test_preds / len(models)

    # Plattã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆfold-aware to prevent leakageï¼‰
    if len(np.unique(train_subset["is_success"])) > 1:
        try:
            from sklearn.linear_model import LogisticRegression

            # Fold-aware calibration for OOF predictions to prevent data leakage
            calibrated_oof_preds = np.zeros(len(train_subset), dtype=float)
            
            for fold_idx, (tr_idx, val_idx) in enumerate(gkf.split(train_subset, groups=train_subset["match_id"])):
                # Fit calibration model on TRAINING fold only (exclude validation fold)
                calib_model = LogisticRegression(max_iter=1000, random_state=42)
                calib_model.fit(
                    oof_preds[tr_idx].reshape(-1, 1),
                    train_subset.iloc[tr_idx]["is_success"].to_numpy()
                )
                
                # Apply calibration to VALIDATION fold
                calibrated_oof_preds[val_idx] = calib_model.predict_proba(
                    oof_preds[val_idx].reshape(-1, 1)
                )[:, 1]
            
            oof_preds = calibrated_oof_preds
            
            # For test predictions: use ALL training data (this is correct)
            if test_preds is not None:
                final_calib = LogisticRegression(max_iter=1000, random_state=42)
                final_calib.fit(oof_preds.reshape(-1, 1), train_subset["is_success"].to_numpy())
                test_preds = final_calib.predict_proba(test_preds.reshape(-1, 1))[:, 1]
                
        except Exception as exc:
            print(f"Calibration failed for {action_group}: {exc}")

    oof_preds = np.clip(oof_preds, 1e-4, 1 - 1e-4)
    if len(test_subset):
        test_preds = np.clip(test_preds, 1e-4, 1 - 1e-4)

    train_mean = float(train_subset["is_success"].mean()) if len(train_subset) else 0.5
    fallback = float(np.clip(train_mean, 1e-4, 1 - 1e-4))

    action_subset.loc[train_subset.index, "xpass_prob"] = oof_preds
    if len(test_subset):
        action_subset.loc[test_subset.index, "xpass_prob"] = test_preds
    else:
        action_subset.loc[test_subset.index, "xpass_prob"] = fallback

    action_subset["xpass_action_group"] = action_group

    relevant_actions.loc[action_subset.index, "xpass_prob"] = action_subset["xpass_prob"]
    relevant_actions.loc[action_subset.index, "xpass_action_group"] = action_subset["xpass_action_group"]

    xpass_predictions.append(
        action_subset[
            [
                "match_id",
                "player_id",
                "action_index",
                "xpass_action_group",
                "xpass_prob",
                "xt_learned_delta",
                "xt_learned_start",
                "is_train_action",
            ]
        ]
    )

    xpass_training_summary.append(
        {
            "action_type": action_group,
            "train_actions": len(train_subset),
            "test_actions": len(test_subset),
            "success_rate": float(train_subset["is_success"].mean()) if len(train_subset) else np.nan,
        }
    )

    cal_df = train_subset[["is_success"]].copy()
    cal_df["pred"] = oof_preds
    cal_df["action_type"] = action_group
    try:
        unique_pred = np.unique(np.round(cal_df["pred"], 6))
        n_bins = min(10, max(4, len(unique_pred)))
        cal_df["bucket"] = pd.qcut(cal_df["pred"], q=n_bins, duplicates="drop")
        agg = cal_df.groupby(["action_type", "bucket"], observed=True).agg(
            pred_mean=("pred", "mean"),
            success_rate=("is_success", "mean"),
            count=("is_success", "size"),
        ).reset_index()
        xpass_calibration_records.append(agg)
    except ValueError:
        pass

if xpass_predictions:
    xpass_predictions_df = pd.concat(xpass_predictions, ignore_index=True)
else:
    xpass_predictions_df = pd.DataFrame(
        columns=[
            "match_id",
            "player_id",
            "action_index",
            "xpass_action_group",
            "xpass_prob",
            "xt_learned_delta",
            "xt_learned_start",
            "is_train_action",
        ]
    )

xpass_training_summary_df = pd.DataFrame(xpass_training_summary)
if not xpass_training_summary_df.empty:
    display(xpass_training_summary_df.sort_values("action_type"))

if xpass_calibration_records:
    calibration_df = pd.concat(xpass_calibration_records, ignore_index=True)
    display(calibration_df)



# %% colab={"base_uri": "https://localhost:8080/", "height": 977} id="nzaeWhMpbDlr" outputId="f99d35a4-fc96-4bcc-f176-50c7235075ef" trusted=true
# [exp0027] 5åˆ†å‰²ã®StratifiedGroupKFoldã‚’è¨­å®šï¼ˆmatch_idã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ– + æ­£ä¾‹ç‡ã§å±¤åŒ–ï¼‰
# scripts/cv.py ã®é–¢æ•°ã‚’ä½¿ç”¨
from scripts.cv import make_stratified_group_folds

train_df["fold"] = make_stratified_group_folds(
    train_df, 
    y_col='xAG', 
    threshold=0.1, 
    n_splits=5, 
    n_bins=5, 
    seed=42
)

# é–¢æ•°ã¯0-indexedã®foldã‚’è¿”ã™ã®ã§ã€1-indexedã«å¤‰æ›ï¼ˆæ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¨ã®äº’æ›æ€§ã®ãŸã‚ï¼‰
train_df["fold"] = train_df["fold"] + 1

# çµæœã‚’å¯è¦–åŒ– (é€šå¸¸ã®GroupKFoldã¨åŒã˜å½¢å¼)
gkf = GroupKFold(n_splits=5)

# xAGè»¸ã®ã‚¹ã‚±ãƒ¼ãƒ«ã¯å…±é€šåŒ–ã—ã¦è¦‹ã‚„ã™ãã™ã‚‹
x_min, x_max = train_df["xAG"].min(), train_df["xAG"].max()
xAG_vals = np.arange(x_min, x_max + 0.1, 0.1).round(1)

# å›³: å„foldã”ã¨ã« 3ã‚«ãƒ©ãƒ ï¼ˆTrainåˆ†å¸ƒ, Valåˆ†å¸ƒ, match_idãƒ™ãƒ³å›³ï¼‰
fig, axes = plt.subplots(nrows=5, ncols=3, figsize=(18, 18), sharey=False, sharex=False)

for fold_num in range(1, 6):
    i = fold_num - 1
    val_mask = train_df["fold"] == fold_num
    trn_mask = train_df["fold"] != fold_num
    
    # foldåˆ—ã¯æ—¢ã«ã‚»ãƒƒãƒˆæ¸ˆã¿ (make_stratified_group_foldsã§è¨­å®š)

    # train/val ã® xAG åˆ†å¸ƒã‚’å–å¾—ï¼ˆå…±é€šã‚¹ã‚±ãƒ¼ãƒ«ã«ãƒªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼‰
    trn_counts = (
        train_df[trn_mask]["xAG"].value_counts().sort_index()
        .reindex(xAG_vals, fill_value=0)
    )
    val_counts = (
        train_df[val_mask]["xAG"].value_counts().sort_index()
        .reindex(xAG_vals, fill_value=0)
    )

    # å·¦åˆ—: å„foldã®trainãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ
    ax_train = axes[i, 0]
    ax_train.bar(trn_counts.index, trn_counts.values, width=0.08, color="steelblue")
    ax_train.set_title(f"Fold {i+1} - Train xAG åˆ†å¸ƒ")
    ax_train.set_xlabel("xAG")
    ax_train.set_ylabel("é »åº¦")

    # ä¸­åˆ—: å„foldã®validationãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ
    ax_val = axes[i, 1]
    ax_val.bar(val_counts.index, val_counts.values, width=0.08, color="orange")
    ax_val.set_title(f"Fold {i+1} - Val xAG åˆ†å¸ƒ")
    ax_val.set_xlabel("xAG")
    ax_val.set_ylabel("é »åº¦")

    # å³åˆ—: match_idã®ãƒ™ãƒ³å›³ï¼ˆTrain vs Valï¼‰
    ax_venn = axes[i, 2]
    trn_match_ids = set(train_df[trn_mask]["match_id"])
    val_match_ids = set(train_df[val_mask]["match_id"])
    v = venn2(
        [trn_match_ids, val_match_ids],
        set_labels=(f"Train match_id (n={len(trn_match_ids)})",
                    f"Val match_id (n={len(val_match_ids)})"),
        ax=ax_venn
    )
    ax_venn.set_title(f"Fold {i+1} - match_id ã®é‡ãªã‚Š")

# xè»¸ã‚’å…±é€šã‚¹ã‚±ãƒ¼ãƒ«ã«æƒãˆã‚‹ï¼ˆåˆ†å¸ƒå›³ã®2ã‚«ãƒ©ãƒ ã«é©ç”¨ï¼‰
for i in range(5):
    for j in [0, 1]:
        ax = axes[i, j]
        ax.set_xlim(x_min - 0.05, x_max + 0.05)  # ç«¯ã‚’å°‘ã—ä½™è£•æŒãŸã›ã‚‹
        ax.set_xticks(xAG_vals[::2])  # ãƒ©ãƒ™ãƒ«ã®æ•°ã‚’é–“å¼•ã

plt.tight_layout()
plt.show()

# %% trusted=true
# [exp0027] å±¤åŒ–CVã®å“è³ªæ¤œè¨¼: å„foldã®æ­£ä¾‹ç‡ã‚’ç¢ºèª
print("=" * 80)
print("StratifiedGroupKFold å“è³ªæ¤œè¨¼")
print("=" * 80)

fold_stats = []
for fold_num in range(1, 6):
    val_mask = train_df["fold"] == fold_num
    trn_mask = train_df["fold"] != fold_num
    
    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµ±è¨ˆ
    val_df = train_df[val_mask]
    val_pos_rate = (val_df["xAG"] >= 0.1).mean()
    val_size = len(val_df)
    val_matches = val_df["match_id"].nunique()
    val_mean_xag = val_df["xAG"].mean()
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çµ±è¨ˆ
    trn_df = train_df[trn_mask]
    trn_pos_rate = (trn_df["xAG"] >= 0.1).mean()
    trn_size = len(trn_df)
    trn_matches = trn_df["match_id"].nunique()
    trn_mean_xag = trn_df["xAG"].mean()
    
    fold_stats.append({
        'Fold': fold_num,
        'Val_PositiveRate': val_pos_rate,
        'Train_PositiveRate': trn_pos_rate,
        'Val_Size': val_size,
        'Train_Size': trn_size,
        'Val_Matches': val_matches,
        'Train_Matches': trn_matches,
        'Val_Mean_xAG': val_mean_xag,
        'Train_Mean_xAG': trn_mean_xag
    })
    
    print(f"\nã€Fold {fold_num}ã€‘")
    print(f"  Validation: {val_size:>6} samples, {val_matches:>4} matches, "
          f"æ­£ä¾‹ç‡={val_pos_rate:.4f}, å¹³å‡xAG={val_mean_xag:.4f}")
    print(f"  Training:   {trn_size:>6} samples, {trn_matches:>4} matches, "
          f"æ­£ä¾‹ç‡={trn_pos_rate:.4f}, å¹³å‡xAG={trn_mean_xag:.4f}")

# çµ±è¨ˆã‚µãƒãƒª
fold_stats_df = pd.DataFrame(fold_stats)
print("\n" + "=" * 80)
print("ã‚µãƒãƒªãƒ¼çµ±è¨ˆ")
print("=" * 80)
print(f"æ­£ä¾‹ç‡ã®æ¨™æº–åå·® (Validation): {fold_stats_df['Val_PositiveRate'].std():.6f}")
print(f"æ­£ä¾‹ç‡ã®æ¨™æº–åå·® (Training):   {fold_stats_df['Train_PositiveRate'].std():.6f}")
print(f"å¹³å‡xAGã®æ¨™æº–åå·® (Validation): {fold_stats_df['Val_Mean_xAG'].std():.6f}")
print(f"å¹³å‡xAGã®æ¨™æº–åå·® (Training):   {fold_stats_df['Train_Mean_xAG'].std():.6f}")

# è¡¨ã¨ã—ã¦è¡¨ç¤º
print("\nè©³ç´°çµ±è¨ˆãƒ†ãƒ¼ãƒ–ãƒ«:")
display(fold_stats_df)


# %% trusted=true
print("eÎ”xTã®Î»æœ€é©åŒ–ã¨ç‰¹å¾´é‡é›†ç´„ã‚’å®Ÿè¡Œä¸­...")

import optuna
from optuna.samplers import TPESampler

edxt_feature_cols = []

if xpass_predictions_df.empty:
    print("xPassã®å¯¾è±¡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€eÎ”xTç‰¹å¾´é‡ã¯è¿½åŠ ã•ã‚Œã¾ã›ã‚“ã€‚")
else:
    xpass_predictions_df = xpass_predictions_df.copy()
    xpass_predictions_df["xt_learned_delta"] = xpass_predictions_df["xt_learned_delta"].fillna(0.0)
    xpass_predictions_df["xt_learned_start"] = xpass_predictions_df["xt_learned_start"].fillna(0.0)
    xpass_predictions_df["success_component"] = xpass_predictions_df["xpass_prob"] * xpass_predictions_df["xt_learned_delta"]
    xpass_predictions_df["fail_component_raw"] = (1.0 - xpass_predictions_df["xpass_prob"]) * xpass_predictions_df["xt_learned_start"]
    xpass_predictions_df["fail_weight"] = (1.0 - xpass_predictions_df["xpass_prob"]).fillna(0.0)
    train_actions = xpass_predictions_df[xpass_predictions_df["is_train_action"]].copy()
    global_start_mean = float(train_actions["xt_learned_start"].mean()) if not train_actions.empty else 0.0
    global_start_std = float(train_actions["xt_learned_start"].std(ddof=0)) if not train_actions.empty else 0.0
    if global_start_std < 1e-6:
        global_start_std = 1.0
    global_start_median = float(train_actions["xt_learned_start"].median()) if not train_actions.empty else 0.0
    global_start_mad = float((train_actions["xt_learned_start"] - global_start_median).abs().median()) if not train_actions.empty else 0.0
    if global_start_mad < 1e-6:
        global_start_mad = 1.0
    # ============================================================
    # FIX ISSUE 1: Fold-aware normalization to prevent data leakage
    # Compute group statistics from OTHER folds only for each validation fold
    # ============================================================
    # Initialize columns for fold-aware statistics
    xpass_predictions_df["fail_group_mean"] = global_start_mean
    xpass_predictions_df["fail_group_std"] = global_start_std
    xpass_predictions_df["fail_group_median"] = global_start_median
    xpass_predictions_df["fail_group_mad"] = global_start_mad
    # For training data: use fold-aware statistics (exclude current fold)
    train_actions_with_fold = train_actions.merge(
        train_df[["match_id", "player_id", "fold"]],
        on=["match_id", "player_id"],
        how="left"
    )
    for fold in train_actions_with_fold["fold"].dropna().unique():
        # Get actions from OTHER folds (not current fold)
        other_folds_actions = train_actions_with_fold[train_actions_with_fold["fold"] != fold]
        if other_folds_actions.empty:
            continue
        # Compute statistics from other folds only
        fold_group_mean = other_folds_actions.groupby("xpass_action_group")["xt_learned_start"].mean()
        fold_group_std = other_folds_actions.groupby("xpass_action_group")["xt_learned_start"].std(ddof=0)
        fold_group_median = other_folds_actions.groupby("xpass_action_group")["xt_learned_start"].median()
        fold_group_mad = other_folds_actions.groupby("xpass_action_group")["xt_learned_start"].apply(
            lambda s: (s - s.median()).abs().median()
        )
        # Apply to current fold's validation data
        # Create the mask properly
        fold_match_player = train_df[train_df["fold"] == fold][["match_id", "player_id"]].copy()
        fold_match_player["_in_fold"] = True
        xpass_with_fold = xpass_predictions_df.merge(
            fold_match_player,
            on=["match_id", "player_id"],
            how="left"
        )
        current_fold_mask = (xpass_with_fold["is_train_action"]) & (xpass_with_fold["_in_fold"] == True)
        if current_fold_mask.sum() > 0:
            xpass_predictions_df.loc[current_fold_mask, "fail_group_mean"] = (
                xpass_predictions_df.loc[current_fold_mask, "xpass_action_group"]
                .map(fold_group_mean)
                .fillna(global_start_mean)
            )
            xpass_predictions_df.loc[current_fold_mask, "fail_group_std"] = (
                xpass_predictions_df.loc[current_fold_mask, "xpass_action_group"]
                .map(fold_group_std)
                .fillna(global_start_std)
            )
            xpass_predictions_df.loc[current_fold_mask, "fail_group_median"] = (
                xpass_predictions_df.loc[current_fold_mask, "xpass_action_group"]
                .map(fold_group_median)
                .fillna(global_start_median)
            )
            xpass_predictions_df.loc[current_fold_mask, "fail_group_mad"] = (
                xpass_predictions_df.loc[current_fold_mask, "xpass_action_group"]
                .map(fold_group_mad)
                .fillna(global_start_mad)
            )
    # For test data: use statistics from ALL training data
    test_mask = ~xpass_predictions_df["is_train_action"]
    if test_mask.sum() > 0:
        group_mean_all = train_actions.groupby("xpass_action_group")["xt_learned_start"].mean()
        group_std_all = train_actions.groupby("xpass_action_group")["xt_learned_start"].std(ddof=0)
        group_median_all = train_actions.groupby("xpass_action_group")["xt_learned_start"].median()
        group_mad_all = train_actions.groupby("xpass_action_group")["xt_learned_start"].apply(
            lambda s: (s - s.median()).abs().median()
        )
        xpass_predictions_df.loc[test_mask, "fail_group_mean"] = (
            xpass_predictions_df.loc[test_mask, "xpass_action_group"]
            .map(group_mean_all)
            .fillna(global_start_mean)
        )
        xpass_predictions_df.loc[test_mask, "fail_group_std"] = (
            xpass_predictions_df.loc[test_mask, "xpass_action_group"]
            .map(group_std_all)
            .fillna(global_start_std)
        )
        xpass_predictions_df.loc[test_mask, "fail_group_median"] = (
            xpass_predictions_df.loc[test_mask, "xpass_action_group"]
            .map(group_median_all)
            .fillna(global_start_median)
        )
        xpass_predictions_df.loc[test_mask, "fail_group_mad"] = (
            xpass_predictions_df.loc[test_mask, "xpass_action_group"]
            .map(group_mad_all)
            .fillna(global_start_mad)
        )
    # Ensure minimum std and mad to avoid division by zero
    xpass_predictions_df["fail_group_std"] = xpass_predictions_df["fail_group_std"].where(
        xpass_predictions_df["fail_group_std"] > 1e-6, global_start_std
    )
    xpass_predictions_df["fail_group_mad"] = xpass_predictions_df["fail_group_mad"].where(
        xpass_predictions_df["fail_group_mad"] > 1e-6, global_start_mad
    )
    # Validation: Ensure no NaN values remain
    assert xpass_predictions_df["fail_group_mean"].isna().sum() == 0, "fail_group_mean has NaN values"
    assert xpass_predictions_df["fail_group_std"].isna().sum() == 0, "fail_group_std has NaN values"
    assert xpass_predictions_df["fail_group_median"].isna().sum() == 0, "fail_group_median has NaN values"
    assert xpass_predictions_df["fail_group_mad"].isna().sum() == 0, "fail_group_mad has NaN values"
    print(f"âœ“ Fold-aware normalization applied: {len(train_actions_with_fold['fold'].dropna().unique())} folds processed")
    pass_train = train_actions[train_actions["xpass_action_group"] == "pass"]
    if not pass_train.empty:
        pass_reference = np.sort(pass_train["xt_learned_start"].to_numpy())
    else:
        pass_reference = None
    def _quantile_center(values: np.ndarray, reference: np.ndarray) -> np.ndarray:
        if reference is None or len(reference) == 0:
            return np.zeros_like(values, dtype=float)
        ranks = np.searchsorted(reference, values, side="left")
        quantiles = (ranks + 0.5) / len(reference)
        quantiles = np.clip(quantiles, 1e-6, 1 - 1e-6)
        return quantiles - 0.5
    xpass_predictions_df["fail_component_scaled"] = 0.0
    pass_mask = xpass_predictions_df["xpass_action_group"] == "pass"
    if pass_mask.any():
        if pass_reference is not None and len(pass_reference) > 10:
            centered = _quantile_center(xpass_predictions_df.loc[pass_mask, "xt_learned_start"].to_numpy(), pass_reference)
        else:
            centered = (
                (xpass_predictions_df.loc[pass_mask, "xt_learned_start"] - xpass_predictions_df.loc[pass_mask, "fail_group_median"]) / xpass_predictions_df.loc[pass_mask, "fail_group_mad"]
            ).to_numpy()
        xpass_predictions_df.loc[pass_mask, "fail_component_scaled"] = xpass_predictions_df.loc[pass_mask, "fail_weight"].to_numpy() * centered
    non_pass_mask = ~pass_mask
    if non_pass_mask.any():
        centered = (
            xpass_predictions_df.loc[non_pass_mask, "xt_learned_start"].to_numpy() - xpass_predictions_df.loc[non_pass_mask, "fail_group_mean"].to_numpy()
        ) / xpass_predictions_df.loc[non_pass_mask, "fail_group_std"].to_numpy()
        xpass_predictions_df.loc[non_pass_mask, "fail_component_scaled"] = xpass_predictions_df.loc[non_pass_mask, "fail_weight"].to_numpy() * centered
    train_actions = xpass_predictions_df[xpass_predictions_df["is_train_action"]].copy()
    train_meta = train_df[["match_id", "player_id", "fold", "xAG"]].copy()
    fold_labels = sorted(train_df["fold"].unique())
    def _weighted_rmse_local(y_true, y_pred):
        weights = make_sample_weight(y_true)
        return float(np.sqrt(np.mean(weights * (y_true - y_pred) ** 2) + 1e-9))
    aggregated_components = (
        train_actions.groupby(["match_id", "player_id", "xpass_action_group"])
        .agg(
            success_sum=("success_component", "sum"),
            fail_sum_scaled=("fail_component_scaled", "sum"),
            fail_sum_raw=("fail_component_raw", "sum"),
            fail_weight_sum=("fail_weight", "sum"),
            action_count=("success_component", "count"),
        )
        .reset_index()
    )
    lambda_per_type = {}
    lambda_meta = []
    lambda_bounds = {
        "pass": (-0.3, 0.6),
        "cross": (-0.6, 2.0),
        "dribble": (-0.6, 2.0),
        "carry": (-0.6, 2.0),
        "corner": (-1.5, 3.0),
        "free_kick": (-1.5, 3.0),
    }
    for action_group in sorted(xpass_predictions_df["xpass_action_group"].dropna().unique()):
        type_df = aggregated_components[aggregated_components["xpass_action_group"] == action_group].copy()
        type_df = type_df.merge(train_meta, on=["match_id", "player_id"], how="left")
        type_df = type_df.dropna(subset=["xAG", "fold"])
        if type_df.empty:
            lambda_per_type[action_group] = 0.0
            continue
        lam_low, lam_high = lambda_bounds.get(action_group, (-1.0, 2.0))
        def objective(trial):
            lam = trial.suggest_float("lambda", lam_low, lam_high)
            preds = type_df["success_sum"] - lam * type_df["fail_sum_scaled"]
            scores = []
            for fold in fold_labels:
                fold_mask = type_df["fold"] == fold
                if not fold_mask.any():
                    continue
                scores.append(
                    _weighted_rmse_local(
                        type_df.loc[fold_mask, "xAG"].to_numpy(),
                        preds.loc[fold_mask].to_numpy(),
                    )
                )
            return float(np.mean(scores)) if scores else 1.0
        study = optuna.create_study(direction="minimize", sampler=TPESampler(seed=SEED))
        study.optimize(objective, n_trials=35, show_progress_bar=False)
        best_lambda = float(study.best_params["lambda"])
        lambda_per_type[action_group] = best_lambda
        lambda_meta.append(
            {
                "action_type": action_group,
                "lambda": best_lambda,
                "lambda_min": lam_low,
                "lambda_max": lam_high,
                "train_rows": int(type_df.shape[0]),
                "actions_per_player_mean": float(type_df["action_count"].mean()),
                "fail_scaled_mean": float(type_df["fail_sum_scaled"].mean()),
            }
        )
    lambda_meta_df = pd.DataFrame(lambda_meta)
    if not lambda_meta_df.empty:
        display(lambda_meta_df.sort_values("lambda"))
        print("Î»åˆ†å¸ƒçµ±è¨ˆ:")
        display(lambda_meta_df["lambda"].describe())
    for action_group, lam_value in lambda_per_type.items():
        type_actions = xpass_predictions_df[xpass_predictions_df["xpass_action_group"] == action_group].copy()
        if type_actions.empty:
            continue
        type_actions["edxt_value_scaled"] = type_actions["success_component"] - lam_value * type_actions["fail_component_scaled"]
        type_actions["edxt_positive_scaled"] = np.clip(type_actions["edxt_value_scaled"], 0.0, None)
        type_actions["edxt_value"] = type_actions["success_component"] - lam_value * type_actions["fail_component_raw"]
        type_actions["edxt_positive"] = np.clip(type_actions["edxt_value"], 0.0, None)
        agg_train = (
            type_actions[type_actions["is_train_action"]]
            .groupby(["match_id", "player_id"])
            .agg(
                edxt_sum=("edxt_value", "sum"),
                edxt_mean=("edxt_value", "mean"),
                edxt_max=("edxt_value", "max"),
                edxt_positive_sum=("edxt_positive", "sum"),
                edxt_positive_mean=("edxt_positive", "mean"),
                edxt_scaled_sum=("edxt_value_scaled", "sum"),
                edxt_scaled_mean=("edxt_value_scaled", "mean"),
                edxt_scaled_max=("edxt_value_scaled", "max"),
                edxt_scaled_positive_sum=("edxt_positive_scaled", "sum"),
                edxt_scaled_positive_mean=("edxt_positive_scaled", "mean"),
                edxt_count=("edxt_value", "count"),
                success_sum=("success_component", "sum"),
                fail_sum_raw=("fail_component_raw", "sum"),
                fail_sum_scaled=("fail_component_scaled", "sum"),
                fail_weight_sum=("fail_weight", "sum"),
            )
            .reset_index()
        )
        agg_test = (
            type_actions[~type_actions["is_train_action"]]
            .groupby(["match_id", "player_id"])
            .agg(
                edxt_sum=("edxt_value", "sum"),
                edxt_mean=("edxt_value", "mean"),
                edxt_max=("edxt_value", "max"),
                edxt_positive_sum=("edxt_positive", "sum"),
                edxt_positive_mean=("edxt_positive", "mean"),
                edxt_scaled_sum=("edxt_value_scaled", "sum"),
                edxt_scaled_mean=("edxt_value_scaled", "mean"),
                edxt_scaled_max=("edxt_value_scaled", "max"),
                edxt_scaled_positive_sum=("edxt_positive_scaled", "sum"),
                edxt_scaled_positive_mean=("edxt_positive_scaled", "mean"),
                edxt_count=("edxt_value", "count"),
                success_sum=("success_component", "sum"),
                fail_sum_raw=("fail_component_raw", "sum"),
                fail_sum_scaled=("fail_component_scaled", "sum"),
                fail_weight_sum=("fail_weight", "sum"),
            )
            .reset_index()
        )
        col_map = {
            "edxt_sum": f"{action_group}_edxt_sum",
            "edxt_mean": f"{action_group}_edxt_mean",
            "edxt_max": f"{action_group}_edxt_max",
            "edxt_positive_sum": f"{action_group}_edxt_positive_sum",
            "edxt_positive_mean": f"{action_group}_edxt_positive_mean",
            "edxt_scaled_sum": f"{action_group}_edxt_scaled_sum",
            "edxt_scaled_mean": f"{action_group}_edxt_scaled_mean",
            "edxt_scaled_max": f"{action_group}_edxt_scaled_max",
            "edxt_scaled_positive_sum": f"{action_group}_edxt_scaled_positive_sum",
            "edxt_scaled_positive_mean": f"{action_group}_edxt_scaled_positive_mean",
            "edxt_count": f"{action_group}_edxt_count",
            "success_sum": f"{action_group}_edxt_success_sum",
            "fail_sum_raw": f"{action_group}_edxt_fail_sum",
            "fail_sum_scaled": f"{action_group}_edxt_fail_scaled_sum",
            "fail_weight_sum": f"{action_group}_fail_weight_sum",
        }
        train_df = train_df.merge(agg_train.rename(columns=col_map), on=["match_id", "player_id"], how="left")
        test_df = test_df.merge(agg_test.rename(columns=col_map), on=["match_id", "player_id"], how="left")
        for new_col in col_map.values():
            if new_col not in train_df.columns:
                train_df[new_col] = 0.0
            if new_col not in test_df.columns:
                test_df[new_col] = 0.0
            train_df[new_col] = train_df[new_col].fillna(0.0)
            test_df[new_col] = test_df[new_col].fillna(0.0)
        edxt_feature_cols.extend(col_map.values())
    openplay_groups = {"pass", "cross", "dribble", "carry"}
    setpiece_groups = {"corner", "free_kick"}
    def _sum_columns(df, cols, new_col):
        found = [col for col in cols if col in df.columns]
        if found:
            df[new_col] = df[found].sum(axis=1)
        else:
            df[new_col] = 0.0
    setpiece_scaled_cols = [f"{g}_edxt_scaled_positive_sum" for g in setpiece_groups]
    openplay_scaled_cols = [f"{g}_edxt_scaled_positive_sum" for g in openplay_groups]
    setpiece_raw_cols = [f"{g}_edxt_positive_sum" for g in setpiece_groups]
    openplay_raw_cols = [f"{g}_edxt_positive_sum" for g in openplay_groups]
    for df in (train_df, test_df):
        _sum_columns(df, setpiece_scaled_cols, "setpiece_edxt_scaled_positive_sum")
        _sum_columns(df, openplay_scaled_cols, "openplay_edxt_scaled_positive_sum")
        _sum_columns(df, setpiece_raw_cols, "setpiece_edxt_positive_sum")
        _sum_columns(df, openplay_raw_cols, "openplay_edxt_positive_sum")
        scaled_denom = df["setpiece_edxt_scaled_positive_sum"] + df["openplay_edxt_scaled_positive_sum"]
        raw_denom = df["setpiece_edxt_positive_sum"] + df["openplay_edxt_positive_sum"]
        df["setpiece_edxt_scaled_ratio"] = np.where(scaled_denom > 0, df["setpiece_edxt_scaled_positive_sum"] / scaled_denom, 0.0)
        df["setpiece_edxt_raw_ratio"] = np.where(raw_denom > 0, df["setpiece_edxt_positive_sum"] / raw_denom, 0.0)
    edxt_feature_cols.extend([
        "setpiece_edxt_scaled_positive_sum",
        "openplay_edxt_scaled_positive_sum",
        "setpiece_edxt_positive_sum",
        "openplay_edxt_positive_sum",
        "setpiece_edxt_scaled_ratio",
        "setpiece_edxt_raw_ratio",
    ])
    edxt_feature_cols = sorted(dict.fromkeys(edxt_feature_cols))


# %% trusted=true

print("ãƒãƒ¼ãƒ æ–‡è„ˆç‰¹å¾´é‡ã‚’è¿½åŠ ã—ã¦ã„ã¾ã™...")

team_context_feature_cols = []
team_base_candidates = [
    col
    for col in train_df.columns
    if col.startswith("xt_learned_") or col.startswith("possession_xt_") or col.endswith("_edxt_sum")
]
team_base_columns = [col for col in team_base_candidates if np.issubdtype(train_df[col].dtype, np.number)]

if team_base_columns:
    for df in (train_df, test_df):
        for col in team_base_columns:
            team_sum = df.groupby(["match_id", "Squad"])[col].transform("sum")
            sum_col = f"{col}_team_sum"
            share_col = f"{col}_team_share"
            lopo_col = f"{col}_team_lopo"
            df[sum_col] = team_sum
            df[share_col] = np.where(team_sum != 0, df[col] / team_sum, 0.0)
            df[lopo_col] = team_sum - df[col]
            df[sum_col] = df[sum_col].fillna(0.0)
            df[share_col] = df[share_col].fillna(0.0)
            df[lopo_col] = df[lopo_col].fillna(0.0)
            if df is train_df:
                team_context_feature_cols.extend([sum_col, share_col, lopo_col])

    team_context_feature_cols = sorted(dict.fromkeys(team_context_feature_cols))
else:
    team_context_feature_cols = []


# %% trusted=true

print("ãƒªãƒ¼ã‚°åˆ¥ã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ç‰¹å¾´ã‚’ä½œæˆã—ã¦ã„ã¾ã™...")

comp_interaction_feature_cols = []
comp_cross_base_features = [
    col
    for col in [
        "xt_learned_delta_sum",
        "xt_learned_positive_delta_sum",
        "possession_xt_positive_sum",
        "possession_xt_delta_sum",
        "possession_speed_ground_mean",
    ]
    if col in train_df.columns
]

if comp_cross_base_features:
    comp_dummy_train = pd.get_dummies(train_df["Comp"].astype(str), prefix="comp_gate", dtype=float)
    comp_dummy_test = pd.get_dummies(test_df["Comp"].astype(str), prefix="comp_gate", dtype=float)
    comp_dummy_train, comp_dummy_test = comp_dummy_train.align(comp_dummy_test, join="outer", axis=1, fill_value=0.0)
    comp_dummy_test = comp_dummy_test[comp_dummy_train.columns]

    train_base = train_df[comp_cross_base_features].fillna(0.0)
    test_base = test_df[comp_cross_base_features].fillna(0.0)

    for base_col in comp_cross_base_features:
        train_values = train_base[base_col].to_numpy()
        test_values = test_base[base_col].to_numpy()
        for comp_col in comp_dummy_train.columns:
            feat_name = f"{base_col}__{comp_col}"
            train_df[feat_name] = train_values * comp_dummy_train[comp_col].to_numpy()
            test_df[feat_name] = test_values * comp_dummy_test[comp_col].to_numpy()
            comp_interaction_feature_cols.append(feat_name)

    comp_interaction_feature_cols = sorted(dict.fromkeys(comp_interaction_feature_cols))
else:
    comp_interaction_feature_cols = []


# %% [markdown] id="A7xHPcUziuhW"
# ## ç‰¹å¾´é‡ã®çµ±åˆ
#
# ä½œæˆã—ãŸå…¨ã¦ã®ç‰¹å¾´é‡ã‚’çµ±åˆã—ã€train/testãƒ‡ãƒ¼ã‚¿ã«ãƒãƒ¼ã‚¸ã—ã¾ã™ã€‚

# %% colab={"base_uri": "https://localhost:8080/"} id="oiPaa-Dz6Gvj" outputId="cc523b94-b8ff-47fd-b87c-8ced83eb52a0" trusted=true
# å¿œç”¨ç‰¹å¾´é‡ã‚’train/testã¸ãƒãƒ¼ã‚¸
train_df = (
    train_df
    .merge(success_rates, on=['match_id', 'player_id'], how='left')
    .merge(zone_actions, on=['match_id', 'player_id'], how='left')
    .merge(per_minute_features, on=['match_id', 'player_id'], how='left')
    .merge(offense_defense_balance, on=['match_id', 'player_id'], how='left')
    .merge(pass_leads_to_shot, on=['match_id', 'player_id'], how='left')
    .merge(progressive_features, on=['match_id', 'player_id'], how='left')
)

train_df['pass_leads_to_shot'] = train_df['pass_leads_to_shot'].fillna(0)

progressive_cols = [col for col in progressive_features.columns if col not in ['match_id', 'player_id']]
for col in progressive_cols:
    train_df[col] = train_df[col].fillna(0.0)


test_df = (
    test_df
    .merge(success_rates, on=['match_id', 'player_id'], how='left')
    .merge(zone_actions, on=['match_id', 'player_id'], how='left')
    .merge(per_minute_features, on=['match_id', 'player_id'], how='left')
    .merge(offense_defense_balance, on=['match_id', 'player_id'], how='left')
    .merge(pass_leads_to_shot, on=['match_id', 'player_id'], how='left')
    .merge(progressive_features, on=['match_id', 'player_id'], how='left')
)

test_df['pass_leads_to_shot'] = test_df['pass_leads_to_shot'].fillna(0)
for col in progressive_cols:
    test_df[col] = test_df[col].fillna(0.0)

print(f"ãƒãƒ¼ã‚¸å¾Œã®trainãƒ‡ãƒ¼ã‚¿shape: {train_df.shape}")
print(f"ãƒãƒ¼ã‚¸å¾Œã®testãƒ‡ãƒ¼ã‚¿shape: {test_df.shape}")


# %% [markdown] id="FHzc1BjgZYRR"
# ## ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³åˆ†å‰²
#
# åˆå›ã®baselineã§ã¯ã€å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«åˆ†å‰²ã™ã‚‹KFoldã‚’ç”¨ã„ã¾ã—ãŸãŒã€ä»Šå›ã¯ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã«åˆã‚ã›ãŸåˆ¥ã®åˆ†å‰²æ–¹æ³•ã‚’è¡Œã„ã¾ã™ã€‚
#
# EDAã§ç¢ºèªã—ãŸã‚ˆã†ã«ã€ä»Šå›ã¯trainãƒ‡ãƒ¼ã‚¿ã¨testãƒ‡ãƒ¼ã‚¿ã«ã¤ã„ã¦ã¯ã€match_idã®é‡ãªã‚Šã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
# ã™ãªã‚ã¡ã€testãƒ‡ãƒ¼ã‚¿ã‚’äºˆæ¸¬ã™ã‚‹ã¨ãã«ã¯ã€ã“ã‚Œã¾ã§è¦‹ãŸã“ã¨ã®ãªã„è©¦åˆã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦äºˆæ¸¬ã‚’ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
# ã“ã®çŠ¶æ³ã‚’trainãƒ‡ãƒ¼ã‚¿å†…éƒ¨ã§ã®Cross Validationã§ã‚‚ãªã‚‹ã¹ãå†ç¾ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã€å®Ÿéš›ã®ã‚¿ã‚¹ã‚¯ã«è¿‘ã„çŠ¶æ³ã§æ­£ã—ã„è©•ä¾¡ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚
#
# ã“ã“ã§ã¯ã€GroupKFoldã‚’ç”¨ã„ã¦ã€trainãƒ‡ãƒ¼ã‚¿ã‚’match_idãŒè¢«ã‚‰ãªã„ã‚ˆã†ã«5åˆ†å‰²ã—ã¾ã™ã€‚ã“ã†ã™ã‚‹ã“ã¨ã§ã€å„foldã§ã®trainãƒ‡ãƒ¼ã‚¿ã¨validãƒ‡ãƒ¼ã‚¿ã®match_idãŒé‡ãªã‚‰ãªããªã‚Šã¾ã™ã€‚
#

# %% [markdown] id="EqDZiepKaIf3"
# ## ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™

# %% trusted=true

# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç‰¹å¾´é‡ã®ä½œæˆ
print("ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç‰¹å¾´é‡ã‚’ä½œæˆä¸­...")

# SquadÃ—Opponentã®äº¤äº’ä½œç”¨ç‰¹å¾´ã‚’ä½œæˆ
train_df["Squad_x_Opponent"] = train_df["Squad"].astype(str) + "_vs_" + train_df["Opponent"].astype(str)
test_df["Squad_x_Opponent"] = test_df["Squad"].astype(str) + "_vs_" + test_df["Opponent"].astype(str)

target_encoding_cols = ["player_id", "Squad", "Opponent", "Squad_x_Opponent"]
global_mean = train_df["xAG"].mean()
smoothing = 10.0
fold_labels = sorted(train_df["fold"].unique())

for col in target_encoding_cols:
    enc_col = f"{col}_target_enc"
    train_df[enc_col] = np.nan

    for fold in fold_labels:
        trn = train_df[train_df["fold"] != fold]
        val_mask = train_df["fold"] == fold

        stats = trn.groupby(col)["xAG"].agg(["sum", "count"])
        stats["encoding"] = (stats["sum"] + global_mean * smoothing) / (stats["count"] + smoothing)

        train_df.loc[val_mask, enc_col] = train_df.loc[val_mask, col].map(stats["encoding"]).fillna(global_mean)

    overall_stats = train_df.groupby(col)["xAG"].agg(["sum", "count"])
    overall_stats["encoding"] = (overall_stats["sum"] + global_mean * smoothing) / (overall_stats["count"] + smoothing)

    test_df[enc_col] = test_df[col].map(overall_stats["encoding"]).fillna(global_mean)

    missing_train = train_df[enc_col].isna().sum()
    missing_test = test_df[enc_col].isna().sum()

    if missing_train > 0:
        train_df.loc[train_df[enc_col].isna(), enc_col] = global_mean
    if missing_test > 0:
        test_df.loc[test_df[enc_col].isna(), enc_col] = global_mean

    print(f"  {col}: train missing {int(missing_train)}, test missing {int(missing_test)}")

print("ãƒªãƒ¼ã‚°/ãƒãƒ¼ãƒ ãƒã‚¤ã‚¢ã‚¹èª¿æ•´ç‰¹å¾´ã‚’è¨ˆç®—ä¸­...")

train_df["Comp_target_enc"] = np.nan
comp_smoothing = 5.0

for fold in fold_labels:
    trn = train_df[train_df["fold"] != fold]
    val_mask = train_df["fold"] == fold

    comp_stats = trn.groupby("Comp")["xAG"].agg(["sum", "count"])
    comp_stats["encoding"] = (comp_stats["sum"] + global_mean * comp_smoothing) / (comp_stats["count"] + comp_smoothing)

    train_df.loc[val_mask, "Comp_target_enc"] = train_df.loc[val_mask, "Comp"].map(comp_stats["encoding"]).fillna(global_mean)

comp_overall_stats = train_df.groupby("Comp")["xAG"].agg(["sum", "count"])
comp_overall_stats["encoding"] = (comp_overall_stats["sum"] + global_mean * comp_smoothing) / (comp_overall_stats["count"] + comp_smoothing)

test_df["Comp_target_enc"] = test_df["Comp"].map(comp_overall_stats["encoding"]).fillna(global_mean)

train_df["Comp_target_enc"] = train_df["Comp_target_enc"].fillna(global_mean)

test_df["Comp_target_enc"] = test_df["Comp_target_enc"].fillna(global_mean)

train_df["Squad_comp_residual"] = train_df["Squad_target_enc"] - train_df["Comp_target_enc"]
test_df["Squad_comp_residual"] = test_df["Squad_target_enc"] - test_df["Comp_target_enc"]

train_df["Squad_global_residual"] = train_df["Squad_target_enc"] - global_mean
test_df["Squad_global_residual"] = test_df["Squad_target_enc"] - global_mean

# diff between Squad and Opponent TE for matchup effect
train_df["Squad_vs_opponent_gap"] = train_df["Squad_target_enc"] - train_df["Opponent_target_enc"]
test_df["Squad_vs_opponent_gap"] = test_df["Squad_target_enc"] - test_df["Opponent_target_enc"]

print("  Squad_comp_residual ãªã©ã®æ–°ç‰¹å¾´ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚")



# %% colab={"base_uri": "https://localhost:8080/"} id="507qeMbXaIf3" outputId="bc0b39f4-fe13-4491-8f88-71c960669942" trusted=true

# å„ç‰¹å¾´é‡ã‚°ãƒ«ãƒ¼ãƒ—ã®å®šç¾©
base_features = ["age", "action_count", "avg_x", "avg_y", "minutes_played", "goal_count"]
categorical_features = ["Comp", "Squad", "Venue"]
action_type_features = [col for col in train_df.columns if (col.startswith('type_')) and (col.endswith('_count'))]
success_rate_features = [
    col for col in train_df.columns
    if col.endswith('_success_rate') and not col.startswith('progressive_')
]
zone_features = [col for col in train_df.columns if col.startswith('zone_')]
per_minute_features = [col for col in train_df.columns if col.endswith('_per_minute')]
ad_balance_features = ['type_offensive_actions', 'type_defensive_actions', 'type_offensive_action_ratio']
sequencial_features = ['pass_leads_to_shot']
progressive_feature_cols = [
    col for col in train_df.columns
    if col.startswith('progressive_')
    or col in ['deep_completion_count', 'final_third_entry_count', 'penalty_area_entry_count']
]
xt_cols = [col for col in train_df.columns if col.startswith('xt_')]
target_encoding_features = [f"{col}_target_enc" for col in ["player_id", "Squad", "Opponent", "Squad_x_Opponent"]]
extra_bias_features = ["Comp_target_enc", "Squad_comp_residual", "Squad_global_residual", "Squad_vs_opponent_gap"]
possession_features = [col for col in train_df.columns if col.startswith('possession_')]
pass_network_features = [col for col in train_df.columns if col.startswith('pass_net_')]
edxt_features = sorted(globals().get('edxt_feature_cols', []))
team_context_features = sorted(globals().get('team_context_feature_cols', []))
comp_interaction_features = sorted(globals().get('comp_interaction_feature_cols', []))

all_features = (
    base_features
    + categorical_features
    + action_type_features
    + success_rate_features
    + zone_features
    + per_minute_features
    + ad_balance_features
    + sequencial_features
    + progressive_feature_cols
    + xt_cols
    + target_encoding_features
    + extra_bias_features
    + possession_features
    + pass_network_features
    + edxt_features
    + team_context_features
    + comp_interaction_features
)

all_features = list(dict.fromkeys(all_features))

# è¿½åŠ : é«˜åº¦ç‰¹å¾´é‡ã‚’ all_features ã«å«ã‚ã‚‹ï¼ˆå­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ä½œæˆã®å‰ã«å®Ÿæ–½ï¼‰
advanced_candidates = [
    "second_assist_count",
    "SCA_1",
    "SCA_2",
    "GCA_1",
    "GCA_2",
    "nstep_to_shot",
    "nstep_xt_delta",
    "pass_dist_mean",
    "pass_dist_max",
    "to_goal_angle_abs_mean",
    "to_goal_dist_mean",
    "pass_to_shot_latency_mean",
    "pass_to_shot_latency_min",
    "risk_creativity_sum",
    "xpass_mean",
    "xpass_min",
    "pass_success_minus_xpass",
    "xpass_deep_mean",
    "xpass_box_mean",
    # æ™‚ç³»åˆ—ãƒˆãƒ¬ãƒ³ãƒ‰
    "xAG_expanding_mean",
    "xAG_rolling3_mean",
    "xAG_diff_prev",
    # ğŸ†• æ–°ç‰¹å¾´é‡ï¼ˆEXP0025ï¼‰
    "first_half_actions", "second_half_actions", "final_15min_actions",
    "early_10min_actions", "time_weighted_intensity",
    "defensive_zone_actions", "middle_zone_actions", "attacking_zone_actions",
    "halfspace_left_actions", "halfspace_right_actions", "central_corridor_actions",
    "final_third_penetrations", "box_entries",
    "betweenness_centrality", "closeness_centrality", "degree_centrality",
    "pass_receiver_diversity", "unique_pass_partners",
    "longchain_to_shot", "longchain_xt_delta",
    "position_variance_x", "position_variance_y", "position_range_x",
    "position_range_y", "avg_action_distance",
]
advanced_features = [c for c in advanced_candidates if c in train_df.columns]
if advanced_features:
    all_features = list(dict.fromkeys(all_features + advanced_features))
    print(f"è¿½åŠ ã•ã‚ŒãŸé«˜åº¦ç‰¹å¾´é‡ï¼ˆå­¦ç¿’å‰åæ˜ ï¼‰: {len(advanced_features)}å€‹ (ğŸ†•æ–°ç‰¹å¾´é‡25å€‹å«ã‚€)")

# ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã«ã¤ã„ã¦ã¯ã€åˆ—ã®å‹ã‚’ã€Œcategoryã€ã«å¤‰æ›´ã—ã¦ãŠã
for col in categorical_features:
    train_df[col] = train_df[col].astype("category")
    test_df[col] = test_df[col].astype("category")

print(f"  - ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡æ•°: {len(all_features)}å€‹")
print(f"  - åŸºæœ¬ç‰¹å¾´é‡: {len(base_features)}å€‹")
print(f"  - ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡: {len(categorical_features)}å€‹")
print(f"  - ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç‰¹å¾´é‡(type_*_count): {len(action_type_features)}å€‹")
print(f"  - æˆåŠŸç‡ç³»: {len(success_rate_features)}å€‹")
print(f"  - ã‚¾ãƒ¼ãƒ³ç³»: {len(zone_features)}å€‹")
print(f"  - per_minuteç³»: {len(per_minute_features)}å€‹")
print(f"  - æ”»å®ˆãƒãƒ©ãƒ³ã‚¹ç³»: {len(ad_balance_features)}å€‹")
print(f"  - æ™‚ç³»åˆ—ç³»: {len(sequencial_features)}å€‹")
print(f"  - ãƒ—ãƒ­ã‚°ãƒ¬ãƒƒã‚·ãƒ–ç³»: {len(progressive_feature_cols)}å€‹")
print(f"  - xTç³»: {len(xt_cols)}å€‹")
print(f"  - ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç³»: {len(target_encoding_features)}å€‹")
print(f"  - ãƒã‚¼ãƒƒã‚·ãƒ§ãƒ³é€²æ”»ç³»: {len(possession_features)}å€‹")
print(f"  - ãƒ‘ã‚¹ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç³»: {len(pass_network_features)}å€‹")
print(f"  - eÎ”xTç³»: {len(edxt_features)}å€‹")
print(f"  - ãƒãƒ¼ãƒ æ–‡è„ˆç³»: {len(team_context_features)}å€‹")
print(f"  - ãƒªãƒ¼ã‚°ç›¸äº’ä½œç”¨ç³»: {len(comp_interaction_features)}å€‹")


# %% colab={"base_uri": "https://localhost:8080/"} id="sUaVQAMtR2wk" outputId="a5af6982-d1c3-4d7d-f5aa-a51963962df1" trusted=true
# ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
X_train = train_df[all_features + ["fold"]]
y_train = train_df["xAG"]
X_test = test_df[all_features]

print(f"\nãƒ¢ãƒ‡ãƒ«å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: X_train {X_train.shape}, y_train {y_train.shape}, X_test {X_test.shape}")

# %% [markdown] id="cCk52KvKaIf3"
# ## ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆLightGBMï¼‰
#
#
#

# %% [markdown] id="fBFkOqtLhraM"
# ç‰¹å¾´é‡ã®æ•°ã‚‚å¢—ãˆã¦ãŠã‚Šã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯æ¢ç´¢ã—ã¦ã¿ãªã„ã¨åˆ†ã‹ã‚Šã¾ã›ã‚“ã€‚
#
# ã“ã“ã§ã¯ã€ã‚ˆã‚ŠåŠ¹ç‡çš„ã«è‰¯ã•ãã†ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¦‹ã¤ã‘ã‚‰ã‚Œã‚‹Optunaã‚’ç”¨ã„ã¦æœ€é©åŒ–ã‚’è¡Œã„ã¾ã™ã€‚
# Optunaã¯æ¢ç´¢ç©ºé–“ã‹ã‚‰è©¦è¡Œå›æ•°ã”ã¨ã«å€™è£œã‚’ææ¡ˆã—ã€è‰¯ã‹ã£ãŸè©¦è¡Œã®æƒ…å ±ã‚’æ´»ã‹ã—ãªãŒã‚‰æ¬¡ã®æ¢ç´¢ã«åæ˜ ã•ã›ã‚‹ãƒ™ã‚¤ã‚ºçš„æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆTPEã‚µãƒ³ãƒ—ãƒ©ãƒ¼ï¼‰ã‚’åˆ©ç”¨ã§ãã‚‹ãŸã‚ã€ç·å½“ãŸã‚Šã®GridSearchã‚ˆã‚Šã‚‚å°‘ãªã„è©¦è¡Œã§è‰¯ã„çµæœã«è¾¿ã‚Šç€ãã‚„ã™ã„ã®ãŒãƒ¡ãƒªãƒƒãƒˆã§ã™ã€‚
# ï¼ˆå‚è€ƒ: https://zenn.dev/robes/articles/d53ff6d665650f ï¼‰


# %% colab={"base_uri": "https://localhost:8080/"} id="ngpAJwwnaIf3" outputId="321e83a6-aee6-4b0a-88c6-199be978c2c2" trusted=true
import optuna
from optuna.samplers import TPESampler

# Optunaã§æœ€é©åŒ–ã—ãªã„ãƒ™ãƒ¼ã‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
base_params = {
    "objective": "regression_l2",
    "boosting_type": "gbdt",
    "random_state": SEED,
    "verbosity": -1,
    "force_col_wise": True,
    "bagging_fraction": 0.8,
    "bagging_freq": 1,
    "feature_fraction": 0.8,
}

# Optunaã§æ¢ç´¢ã™ã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¦‚è¦
optuna_search_space = {
    "num_leaves": (10, 64),
    "learning_rate": (0.01, 0.1),
    "min_child_samples": (10, 50),
}

print("Optunaç”¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šå®Œäº†")
print(f"æ¢ç´¢å¯¾è±¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {list(optuna_search_space.keys())}")

# %% colab={"base_uri": "https://localhost:8080/"} id="-hkjQJvTvhMO" outputId="61949fbb-9b97-43cd-9bfc-e704053383ba" trusted=true
try:
    base_dir = Path(__file__).resolve().parent
except NameError:  # __file__ ã¯ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å®Ÿè¡Œæ™‚ã«ã¯å®šç¾©ã•ã‚Œãªã„
    base_dir = Path.cwd()

log_dir = base_dir / "logs"
log_dir.mkdir(parents=True, exist_ok=True)


def save_training_run(
    cv_scores,
    oof_score,
    optuna_summary,
    best_params,
    log_directory: Path,
    log_prefix: str = "host_baseline_002",
):
    """Persist CV metrics to reusable JSON/text logs."""

    timestamp = datetime.now().astimezone().isoformat(timespec="seconds")

    metrics_payload = {
        "run_timestamp": timestamp,
        "cv": {
            "scores": [float(score) for score in cv_scores],
            "mean": float(np.mean(cv_scores)),
            "std": float(np.std(cv_scores)),
        },
        "per_fold": {f"fold_{idx + 1}": float(score) for idx, score in enumerate(cv_scores)},
        "oof_rmse": float(oof_score),
        "optuna": optuna_summary,
        "best_params": {key: (float(val) if isinstance(val, (np.floating, np.integer)) else val)
                         for key, val in best_params.items()},
    }

    metrics_path = log_directory / f"{log_prefix}_metrics.json"
    metrics_path.write_text(json.dumps(metrics_payload, ensure_ascii=False, indent=2), encoding="utf-8")

    log_lines = [
        f"[{timestamp}] {log_prefix}",
        f"  CV mean: {metrics_payload['cv']['mean']:.4f}",
        f"  CV std: {metrics_payload['cv']['std']:.4f}",
        f"  OOF RMSE: {metrics_payload['oof_rmse']:.4f}",
    ]
    for idx, score in enumerate(cv_scores, start=1):
        log_lines.append(f"  Fold {idx}: {score:.4f}")

    log_lines.append(
        "  Optuna best trial: "
        f"{optuna_summary['best_trial_number']} (CV mean {optuna_summary['best_cv_value']:.6f}, "
        f"fold1 RMSE {optuna_summary['fold1_val_rmse']:.6f})"
    )

    log_path = log_directory / f"{log_prefix}_training.log"
    with log_path.open("a", encoding="utf-8") as fp:
        fp.write("\n".join(log_lines) + "\n")


# Optunaã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
print("Optunaã«ã‚ˆã‚‹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™...")

# Fold 1ã‚’æ¤œè¨¼ç”¨ã«ç¢ºä¿ï¼ˆå¾Œã§ã‚¹ã‚³ã‚¢ç¢ºèªã«åˆ©ç”¨ï¼‰
trn_mask = train_df["fold"] != 1
val_mask = train_df["fold"] == 1

X_tr = train_df.loc[trn_mask, all_features].copy()
X_val = train_df.loc[val_mask, all_features].copy()
y_tr = y_train.loc[trn_mask].copy()
y_val = y_train.loc[val_mask].copy()

def objective(trial):
    params = base_params.copy()
    params.update({
        "num_leaves": trial.suggest_int("num_leaves", *optuna_search_space["num_leaves"]),
        "learning_rate": trial.suggest_float("learning_rate", *optuna_search_space["learning_rate"], log=True),
        "min_child_samples": trial.suggest_int("min_child_samples", *optuna_search_space["min_child_samples"]),
    })

    cv_scores = []
    for fold in range(1, 4):  # è¨ˆç®—é‡ã‚’æŠ‘ãˆã‚‹ãŸã‚Fold1~3ã§CV
        trn_mask_cv = train_df["fold"] != fold
        val_mask_cv = train_df["fold"] == fold

        X_tr_cv = train_df.loc[trn_mask_cv, all_features].copy()
        X_val_cv = train_df.loc[val_mask_cv, all_features].copy()
        y_tr_cv = y_train.loc[trn_mask_cv].copy()
        y_val_cv = y_train.loc[val_mask_cv].copy()

        train_weights_cv = make_sample_weight(y_tr_cv)
        val_weights_cv = make_sample_weight(y_val_cv)
        train_data = lgb.Dataset(X_tr_cv, label=y_tr_cv, weight=train_weights_cv)
        val_data = lgb.Dataset(X_val_cv, label=y_val_cv, weight=val_weights_cv)

        model = lgb.train(
            params,
            train_data,
            valid_sets=[val_data],
            feval=weighted_rmse_feval,
            num_boost_round=1000,
            callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)],
        )

        preds = model.predict(X_val_cv, num_iteration=model.best_iteration)
        cv_scores.append(weighted_rmse(y_val_cv, preds))

    return float(np.mean(cv_scores))

optuna.logging.set_verbosity(optuna.logging.WARNING)
study = optuna.create_study(direction="minimize", sampler=TPESampler(seed=SEED))
study.optimize(objective, n_trials=30, show_progress_bar=True)

best_params = study.best_trial.params.copy()
best_lgbm_params = base_params.copy()
best_lgbm_params.update(best_params)

# Fold1ã§ã®ã‚¹ã‚³ã‚¢ã‚’å†ç¢ºèª
train_weights = make_sample_weight(y_tr)
val_weights = make_sample_weight(y_val)
train_data = lgb.Dataset(X_tr, label=y_tr, weight=train_weights)
val_data = lgb.Dataset(X_val, label=y_val, weight=val_weights)
best_model = lgb.train(
    best_lgbm_params,
    train_data,
    valid_sets=[train_data, val_data],
    valid_names=["train", "val"],
    feval=weighted_rmse_feval,
    num_boost_round=1000,
    callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)],
)

val_pred = best_model.predict(X_val, num_iteration=best_model.best_iteration)
val_score = weighted_rmse(y_val, val_pred)

print("=== Optunaãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµæœ ===")
print(f"æœ€è‰¯Trialç•ªå·: {study.best_trial.number}")
print(f"å¹³å‡CV RMSE: {study.best_value:.6f}")
print(f"Fold1 Validation RMSE: {val_score:.6f}")
print("æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
for param, value in best_params.items():
    print(f"  {param}: {value}")

# %% colab={"base_uri": "https://localhost:8080/"} id="f1o2kC3wsi0n" outputId="2e156a42-4637-433a-f6db-76205a1e3b21" trusted=true
# æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã®5-Fold Cross Validation
print("æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã®5-Fold Cross Validationã‚’é–‹å§‹...")

# å˜èª¿æ€§åˆ¶ç´„ã®è¨­å®š
# xAGã¨å˜èª¿å¢—åŠ é–¢ä¿‚ã«ã‚ã‚‹ç‰¹å¾´é‡ã‚’é¸å®š
monotone_increase_features = [
    # ãƒ—ãƒ­ã‚°ãƒ¬ãƒƒã‚·ãƒ–ç³»ï¼ˆæ”»æ’ƒçš„ãªå‰é€²ãƒ—ãƒ¬ãƒ¼ â†’ xAGå¢—åŠ ï¼‰
    'progressive_pass_count',
    'progressive_pass_success',
    'progressive_pass_distance_total',
    'progressive_pass_distance_mean',
    'progressive_carry_count',
    'progressive_carry_success',
    'progressive_carry_distance_total',
    'progressive_carry_distance_mean',
    'deep_completion_count',  # ãƒ‡ã‚£ãƒ¼ãƒ—ã‚¾ãƒ¼ãƒ³åˆ°é”æ•°
    'final_third_entry_count',  # ãƒ•ã‚¡ã‚¤ãƒŠãƒ«ã‚µãƒ¼ãƒ‰é€²å…¥æ•°
    'penalty_area_entry_count',  # ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚¨ãƒªã‚¢é€²å…¥æ•°

    # ã‚·ãƒ¥ãƒ¼ãƒˆãƒ»ã‚´ãƒ¼ãƒ«ç³»ï¼ˆå‰µé€ æ€§ã®æŒ‡æ¨™ï¼‰
    'goal_count',  # ã‚´ãƒ¼ãƒ«æ•°
    'pass_leads_to_shot',  # ãƒ‘ã‚¹â†’ã‚·ãƒ§ãƒƒãƒˆã®é€£é–

    # æ”»æ’ƒçš„ã‚¾ãƒ¼ãƒ³æ´»å‹•ï¼ˆå‰ç·šã§ã®ãƒ—ãƒ¬ãƒ¼ â†’ xAGå¢—åŠ ï¼‰
    'zone_attacking_actions',  # æ”»æ’ƒã‚¾ãƒ¼ãƒ³ã§ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°
    'zone_attacking_actions_ratio',  # æ”»æ’ƒã‚¾ãƒ¼ãƒ³æ¯”ç‡

    # æ”»æ’ƒçš„ãƒãƒ©ãƒ³ã‚¹
    'type_offensive_actions',  # æ”»æ’ƒã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ•°
    'type_offensive_action_ratio',  # æ”»æ’ƒã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ¯”ç‡
]

missing_monotone_features = [feat for feat in monotone_increase_features if feat not in all_features]
if missing_monotone_features:
    print("å˜èª¿æ€§åˆ¶ç´„å¯¾è±¡ã¨ã—ã¦æŒ‡å®šã—ãŸã‚‚ã®ã®ã€ç‰¹å¾´é‡ä¸€è¦§ã«å­˜åœ¨ã—ãªã„åˆ—ãŒã‚ã‚Šã¾ã™:")
    for feat in missing_monotone_features:
        print(f"  - {feat}")

applied_monotone_features = [feat for feat in monotone_increase_features if feat in all_features]

# all_featureså†…ã§ã®å„ç‰¹å¾´é‡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—ã—ã€å˜èª¿æ€§ãƒ™ã‚¯ãƒˆãƒ«ã‚’æ§‹ç¯‰
monotone_constraints = [0] * len(all_features)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯åˆ¶ç´„ãªã—(0)
for feat in applied_monotone_features:
    idx = all_features.index(feat)
    monotone_constraints[idx] = 1  # å˜èª¿å¢—åŠ åˆ¶ç´„

print(f"\nå˜èª¿æ€§åˆ¶ç´„ã‚’é©ç”¨: {len(applied_monotone_features)}å€‹ã®ç‰¹å¾´é‡")
print("å˜èª¿å¢—åŠ åˆ¶ç´„ã‚’é©ç”¨ã—ãŸç‰¹å¾´é‡:")
for feat in applied_monotone_features:
    print(f"  - {feat}")

# LightGBMå­¦ç¿’ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šï¼ˆOptunaã§æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
lgbm_params = {
    "objective": "regression_l2",
    "boosting_type": "gbdt",
    "random_state": SEED,
    "verbosity": -1,
    "force_col_wise": True,
    "bagging_fraction": 0.8,
    "bagging_freq": 1,
    "feature_fraction": 0.8,
    "monotone_constraints": monotone_constraints,  # å˜èª¿æ€§åˆ¶ç´„ã‚’è¿½åŠ 
    "monotone_constraints_method": "advanced",  # advanced methodã‚’ä½¿ç”¨
}

# Optunaã®æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸
lgbm_params.update(best_params)

print(f"\nä½¿ç”¨ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {lgbm_params}")

# 5-Foldã§ã®ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆæœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä½¿ç”¨ï¼‰
oof_preds = np.zeros(len(X_train))
cv_scores = []
models = []
feature_importance = pd.DataFrame()

# Training the models on the entire training data
for fold in range(5):
    print(f"=== Fold {fold + 1} ===")

    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    trn_mask = train_df["fold"] != fold+1
    val_mask = train_df["fold"] == fold+1

    X_tr = train_df.loc[trn_mask, all_features].copy()
    X_val = train_df.loc[val_mask, all_features].copy()
    y_tr = y_train.loc[trn_mask].copy()
    y_val = y_train.loc[val_mask].copy()

    # LightGBMãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    train_weights = make_sample_weight(y_tr)
    val_weights = make_sample_weight(y_val)
    train_data = lgb.Dataset(X_tr, label=y_tr, weight=train_weights)
    val_data = lgb.Dataset(X_val, label=y_val, weight=val_weights, reference=train_data)

    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆæœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä½¿ç”¨ï¼‰
    model = lgb.train(
        lgbm_params,
        train_data,
        valid_sets=[train_data, val_data],
        valid_names=["train", "val"],
        feval=weighted_rmse_feval,
        num_boost_round=1000,
        callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)]
    )

    # validationãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹äºˆæ¸¬ã€ã‚¹ã‚³ã‚¢ç®—å‡º
    y_pred_val = model.predict(X_val, num_iteration=model.best_iteration)
    oof_preds[val_mask] = y_pred_val
    score = weighted_rmse(y_val, y_pred_val)
    cv_scores.append(score)
    models.append(model)  # ã“ã®foldã®ãƒ¢ãƒ‡ãƒ«ã‚’modelsã«æ ¼ç´

    print(f"Fold {fold + 1} RMSE: {score:.4f}")

    # ç‰¹å¾´é‡é‡è¦åº¦ç®—å‡º
    fold_importance = pd.DataFrame({
        "feature": all_features,
        "importance": model.feature_importance(importance_type="gain"),
        "fold": fold + 1
    })
    feature_importance = pd.concat([feature_importance, fold_importance], axis=0)

cv_mean = float(np.mean(cv_scores))
cv_std = float(np.std(cv_scores))

print("=== Cross Validation Results (Optimized Parameters) ===")
print(f"CV RMSE: {cv_mean:.4f} (+/- {cv_std * 2:.4f})")
for i, score in enumerate(cv_scores):
    print(f"Fold {i + 1}: {score:.4f}")

# OOFäºˆæ¸¬ã®ã‚¹ã‚³ã‚¢ç®—å‡º
oof_score = weighted_rmse(y_train, oof_preds)
print(f"OOF RMSE: {oof_score:.4f}")

optuna_summary = {
    "best_trial_number": int(study.best_trial.number),
    "best_cv_value": float(study.best_value),
    "fold1_val_rmse": float(val_score),
}

save_training_run(
    cv_scores=cv_scores,
    oof_score=oof_score,
    optuna_summary=optuna_summary,
    best_params=best_params,
    log_directory=log_dir,
)

print(f"ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {log_dir / 'host_baseline_002_metrics.json'}")
print(f"ãƒ­ã‚°ã‚’è¿½è¨˜ã—ã¾ã—ãŸ: {log_dir / 'host_baseline_002_training.log'}")


# %% [markdown]
# ## CatBoost ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
#
# CatBoostã‚’ä½¿ç”¨ã—ã¦ã€LGBMã¨ç›¸è£œçš„ãªãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚
#
# **CatBoostã®ç‰¹å¾´:**
# - **Ordered Target Statistics**: ãƒªãƒ¼ã‚¯ã‚’å›é¿ã—ãªãŒã‚‰ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’åŠ¹æœçš„ã«å‡¦ç†
# - **é«˜æ¬¡äº¤äº’ä½œç”¨**: ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°é–“ã®è¤‡é›‘ãªäº¤äº’ä½œç”¨ã‚’è‡ªå‹•çš„ã«å­¦ç¿’
# - **å¯¾ç§°æœ¨æ§‹é€ **: ã‚ˆã‚Šå®‰å®šã—ãŸäºˆæ¸¬ã‚’å®Ÿç¾
#
# **å®Ÿè£…æ–¹é‡:**
# - ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
# - LGBMã¨ç•°ãªã‚‹seed/åˆ—ã‚µãƒ–ã‚»ãƒƒãƒˆã§å¤šæ§˜æ€§ã‚’ç¢ºä¿
# - å˜èª¿æ€§åˆ¶ç´„ã¯å¿…è¦æœ€å°é™ã«æŠ‘åˆ¶
# - é‡ã¿ä»˜ãRMSEã«å¯¾å¿œã—ãŸã‚µãƒ³ãƒ—ãƒ«é‡ã¿ã‚’ä½¿ç”¨

# %% trusted=true
# CatBoostç”¨ã®ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡ã‚’æ˜ç¤ºçš„ã«å®šç¾©
catboost_categorical_features = ['Comp', 'Squad', 'Venue']

# CatBoostã§ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ï¼ˆLGBMã¨åŒã˜ç‰¹å¾´é‡ã‚’ä½¿ç”¨ï¼‰
catboost_features = all_features.copy()

print(f"CatBoostã§ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡æ•°: {len(catboost_features)}å€‹")
print(f"ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡: {catboost_categorical_features}")

# ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
cat_features_idx = [catboost_features.index(col) for col in catboost_categorical_features if col in catboost_features]
print(f"ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {cat_features_idx}")

# %% trusted=true
# CatBoostç”¨ã®Optunaãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
print("CatBoost ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’é–‹å§‹...")

def objective_catboost(trial):
    """
    CatBoostç”¨ã®Optunaç›®çš„é–¢æ•°
    LGBMã¨ç›¸è£œæ€§ã‚’æŒãŸã›ã‚‹ãŸã‚ã€ç•°ãªã‚‹æ¢ç´¢ç©ºé–“ã‚’è¨­å®š
    """
    params = {
        'iterations': 1000,
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),
        'depth': trial.suggest_int('depth', 4, 10),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),
        'random_strength': trial.suggest_float('random_strength', 0.1, 10.0, log=True),
        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),
        'border_count': trial.suggest_int('border_count', 32, 255),
        'loss_function': 'RMSE',
        'eval_metric': 'RMSE',
        'random_seed': SEED + 100,  # LGBMã¨ç•°ãªã‚‹seedã§å¤šæ§˜æ€§ç¢ºä¿
        'verbose': False,
        'early_stopping_rounds': 100,
        'cat_features': cat_features_idx,
    }
    
    # Fold 1ã®ã¿ã§è©•ä¾¡ï¼ˆé«˜é€ŸåŒ–ã®ãŸã‚ï¼‰
    fold = 0
    trn_mask = train_df['fold'] != fold + 1
    val_mask = train_df['fold'] == fold + 1
    
    X_tr = train_df.loc[trn_mask, catboost_features].copy()
    X_val = train_df.loc[val_mask, catboost_features].copy()
    y_tr = y_train.loc[trn_mask].copy()
    y_val = y_train.loc[val_mask].copy()
    
    # ã‚µãƒ³ãƒ—ãƒ«é‡ã¿ã‚’è¨ˆç®—
    train_weights = make_sample_weight(y_tr)
    
    # CatBoost Poolä½œæˆ
    train_pool = cb.Pool(
        data=X_tr,
        label=y_tr,
        weight=train_weights,
        cat_features=cat_features_idx
    )
    val_pool = cb.Pool(
        data=X_val,
        label=y_val,
        cat_features=cat_features_idx
    )
    
    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    model = cb.CatBoostRegressor(**params)
    model.fit(
        train_pool,
        eval_set=val_pool,
        verbose=False,
        plot=False
    )
    
    # äºˆæ¸¬ã¨ã‚¹ã‚³ã‚¢è¨ˆç®—
    y_pred_val = model.predict(X_val)
    val_score = weighted_rmse(y_val, y_pred_val)
    
    return val_score

# Optunaæœ€é©åŒ–å®Ÿè¡Œ
catboost_study = optuna.create_study(
    direction='minimize',
    sampler=TPESampler(seed=SEED)
)

catboost_study.optimize(
    objective_catboost,
    n_trials=30,  # LGBMã¨åŒæ§˜ã«30è©¦è¡Œ
    show_progress_bar=True
)

catboost_best_params = catboost_study.best_params
print(f"\nCatBoostæœ€é©åŒ–å®Œäº†")
print(f"Best trial: {catboost_study.best_trial.number}")
print(f"Best validation RMSE: {catboost_study.best_value:.4f}")
print(f"Best parameters: {catboost_best_params}")

# %% trusted=true
# CatBoostç”¨ã®å˜èª¿æ€§åˆ¶ç´„è¨­å®š
# LGBMã‚ˆã‚Šã‚‚åˆ¶ç´„ã‚’ç·©ã‚ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®å¤šæ§˜æ€§ã‚’ç¢ºä¿
print("CatBoostå˜èª¿æ€§åˆ¶ç´„ã‚’è¨­å®šä¸­...")

# æœ€ã‚‚é‡è¦ãªç‰¹å¾´é‡ã®ã¿ã«å˜èª¿æ€§åˆ¶ç´„ã‚’é©ç”¨ï¼ˆLGBMã‚ˆã‚Šå°‘ãªã‚ï¼‰
catboost_monotone_increase_features = [
    'goal_count',  # ã‚´ãƒ¼ãƒ«æ•°
    'pass_leads_to_shot',  # ãƒ‘ã‚¹â†’ã‚·ãƒ§ãƒƒãƒˆ
    'progressive_pass_count',  # ãƒ—ãƒ­ã‚°ãƒ¬ãƒƒã‚·ãƒ–ãƒ‘ã‚¹æ•°
    'deep_completion_count',  # ãƒ‡ã‚£ãƒ¼ãƒ—ã‚¾ãƒ¼ãƒ³åˆ°é”
    'penalty_area_entry_count',  # ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚¨ãƒªã‚¢é€²å…¥
    'zone_attacking_actions',  # æ”»æ’ƒã‚¾ãƒ¼ãƒ³ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
]

# ç‰¹å¾´é‡ãŒå­˜åœ¨ã™ã‚‹ã‚‚ã®ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
applied_catboost_monotone_features = [
    feat for feat in catboost_monotone_increase_features 
    if feat in catboost_features
]

# CatBoostã®å˜èª¿æ€§åˆ¶ç´„å½¢å¼: æ–‡å­—åˆ—ã®ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š
# å½¢å¼: "feature_idx1,feature_idx2,..." ã§æ–¹å‘ã‚’æŒ‡å®š
# ã¾ãŸã¯å„ç‰¹å¾´é‡ã«å¯¾ã—ã¦ (idx, direction) ã®ã‚¿ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆ
# ã“ã“ã§ã¯ã€ç‰¹å¾´é‡åãƒ™ãƒ¼ã‚¹ã§è¾æ›¸å½¢å¼ã‚’ä½¿ç”¨

# CatBoostã®å˜èª¿æ€§åˆ¶ç´„: å…¨ç‰¹å¾´é‡ã«å¯¾ã™ã‚‹åˆ¶ç´„ãƒªã‚¹ãƒˆï¼ˆ0=åˆ¶ç´„ãªã—, 1=å¢—åŠ , -1=æ¸›å°‘ï¼‰
catboost_monotone_constraints = [0] * len(catboost_features)

for feat in applied_catboost_monotone_features:
    if feat in catboost_features:
        idx = catboost_features.index(feat)
        catboost_monotone_constraints[idx] = 1  # å˜èª¿å¢—åŠ åˆ¶ç´„

print(f"å˜èª¿æ€§åˆ¶ç´„ã‚’é©ç”¨: {len(applied_catboost_monotone_features)}å€‹ã®ç‰¹å¾´é‡")
print(f"  (LGBMã®{len(applied_monotone_features)}å€‹ã‹ã‚‰å‰Šæ¸›ã—ã€ãƒ¢ãƒ‡ãƒ«ã®å¤šæ§˜æ€§ã‚’ç¢ºä¿)")
for feat in applied_catboost_monotone_features:
    print(f"  - {feat}")

# %% trusted=true
# CatBoostã§ã®5-Fold Cross Validation
print("\nCatBoostã§ã®5-Fold Cross Validationã‚’é–‹å§‹...")

# æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ™ãƒ¼ã‚¹ã«è¨­å®š
catboost_params = {
    'iterations': 1000,
    'loss_function': 'RMSE',
    'eval_metric': 'RMSE',
    'random_seed': SEED + 100,  # LGBMã¨ç•°ãªã‚‹seed
    'verbose': False,
    'early_stopping_rounds': 100,
    'cat_features': cat_features_idx,
    'monotone_constraints': catboost_monotone_constraints,  # å˜èª¿æ€§åˆ¶ç´„
}

# Optunaã®æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸
catboost_params.update(catboost_best_params)

print(f"ä½¿ç”¨ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {catboost_params}")

# CVçµæœã‚’æ ¼ç´
catboost_oof_preds = np.zeros(len(X_train))
catboost_cv_scores = []
catboost_models = []
catboost_feature_importance = pd.DataFrame()

for fold in range(5):
    print(f"\n=== CatBoost Fold {fold + 1} ===")
    
    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    trn_mask = train_df['fold'] != fold + 1
    val_mask = train_df['fold'] == fold + 1
    
    X_tr = train_df.loc[trn_mask, catboost_features].copy()
    X_val = train_df.loc[val_mask, catboost_features].copy()
    y_tr = y_train.loc[trn_mask].copy()
    y_val = y_train.loc[val_mask].copy()
    
    # ã‚µãƒ³ãƒ—ãƒ«é‡ã¿ã‚’è¨ˆç®—
    train_weights = make_sample_weight(y_tr)
    
    # CatBoost Poolä½œæˆ
    train_pool = cb.Pool(
        data=X_tr,
        label=y_tr,
        weight=train_weights,  # é‡ã¿ä»˜ãRMSEå¯¾å¿œ
        cat_features=cat_features_idx
    )
    val_pool = cb.Pool(
        data=X_val,
        label=y_val,
        cat_features=cat_features_idx
    )
    
    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    model = cb.CatBoostRegressor(**catboost_params)
    model.fit(
        train_pool,
        eval_set=val_pool,
        verbose=100,
        plot=False
    )
    
    # äºˆæ¸¬ã¨ã‚¹ã‚³ã‚¢è¨ˆç®—
    y_pred_val = model.predict(X_val)
    catboost_oof_preds[val_mask] = y_pred_val
    score = weighted_rmse(y_val, y_pred_val)
    catboost_cv_scores.append(score)
    catboost_models.append(model)
    
    print(f"Fold {fold + 1} Weighted RMSE: {score:.4f}")
    
    # ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—
    fold_importance = pd.DataFrame({
        'feature': catboost_features,
        'importance': model.get_feature_importance(),
        'fold': fold + 1
    })
    catboost_feature_importance = pd.concat(
        [catboost_feature_importance, fold_importance], 
        axis=0
    )

# CVçµæœã®ã‚µãƒãƒªãƒ¼
catboost_cv_mean = float(np.mean(catboost_cv_scores))
catboost_cv_std = float(np.std(catboost_cv_scores))

print("\n=== CatBoost Cross Validation Results ===")
print(f"CV Weighted RMSE: {catboost_cv_mean:.4f} (+/- {catboost_cv_std * 2:.4f})")
for i, score in enumerate(catboost_cv_scores):
    print(f"Fold {i + 1}: {score:.4f}")

# OOFäºˆæ¸¬ã®ã‚¹ã‚³ã‚¢
catboost_oof_score = weighted_rmse(y_train, catboost_oof_preds)
print(f"\nCatBoost OOF Weighted RMSE: {catboost_oof_score:.4f}")

# LGBMã¨ã®æ¯”è¼ƒ
print(f"\n--- ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ ---")
print(f"LightGBM OOF RMSE: {oof_score:.4f}")
print(f"CatBoost OOF RMSE: {catboost_oof_score:.4f}")
print(f"å·®åˆ†: {catboost_oof_score - oof_score:+.4f}")

# %% [markdown]
# ## ãƒ¢ãƒ‡ãƒ«ãƒ–ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆLightGBM + CatBoostï¼‰
#
# LightGBMã¨CatBoostã®äºˆæ¸¬ã‚’åŠ é‡å¹³å‡ã§ãƒ–ãƒ¬ãƒ³ãƒ‰ã—ã¾ã™ã€‚
#
# **ãƒ–ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æˆ¦ç•¥:**
# - OOFäºˆæ¸¬ã§ãƒ–ãƒ¬ãƒ³ãƒ‰æ¯”ç‡ã‚’æœ€é©åŒ–
# - ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã§æœ€é©ãªé‡ã¿ã‚’æ¢ç´¢
# - LGBMã¨CatBoostã®ç›¸è£œæ€§ã‚’æ´»ç”¨

# %% trusted=true
# OOFäºˆæ¸¬ã‚’ä½¿ã£ã¦ãƒ–ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æ¯”ç‡ã‚’æœ€é©åŒ–
print("ãƒ–ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æ¯”ç‡ã‚’æœ€é©åŒ–ä¸­...")

best_blend_weight = 0.5
best_blend_score = float('inf')

# ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã§æœ€é©ãªé‡ã¿ã‚’æ¢ç´¢
for lgb_weight in np.arange(0.0, 1.01, 0.05):
    catboost_weight = 1.0 - lgb_weight
    
    # ãƒ–ãƒ¬ãƒ³ãƒ‰äºˆæ¸¬
    blended_oof = lgb_weight * oof_preds + catboost_weight * catboost_oof_preds
    
    # ã‚¹ã‚³ã‚¢è¨ˆç®—
    blend_score = weighted_rmse(y_train, blended_oof)
    
    if blend_score < best_blend_score:
        best_blend_score = blend_score
        best_blend_weight = lgb_weight

best_catboost_weight = 1.0 - best_blend_weight

print(f"\n=== æœ€é©ãƒ–ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æ¯”ç‡ ===")
print(f"LightGBMé‡ã¿: {best_blend_weight:.2f}")
print(f"CatBoosté‡ã¿: {best_catboost_weight:.2f}")
print(f"\n--- ã‚¹ã‚³ã‚¢æ¯”è¼ƒ ---")
print(f"LightGBM OOF: {oof_score:.4f}")
print(f"CatBoost OOF: {catboost_oof_score:.4f}")
print(f"ãƒ–ãƒ¬ãƒ³ãƒ‰OOF: {best_blend_score:.4f}")
print(f"\nLGBMã‹ã‚‰ã®æ”¹å–„: {best_blend_score - oof_score:+.4f}")
print(f"CatBoostã‹ã‚‰ã®æ”¹å–„: {best_blend_score - catboost_oof_score:+.4f}")

# %% [markdown]
# ## ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹æ¨è«–ï¼ˆãƒ–ãƒ¬ãƒ³ãƒ‰ãƒ¢ãƒ‡ãƒ«ï¼‰

# %% trusted=true
# CatBoostã§ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹æ¨è«–
print("CatBoostã§ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹æ¨è«–ã‚’å®Ÿè¡Œä¸­...")

catboost_test_preds = np.zeros(len(X_test))

for fold, model in enumerate(catboost_models):
    fold_preds = model.predict(test_df[catboost_features])
    catboost_test_preds += fold_preds / 5
    print(f"Fold {fold + 1}: äºˆæ¸¬å®Œäº†")

print(f"\nCatBoostäºˆæ¸¬çµ±è¨ˆ:")
print(f"  Mean: {catboost_test_preds.mean():.4f}")
print(f"  Std: {catboost_test_preds.std():.4f}")
print(f"  Min: {catboost_test_preds.min():.4f}")
print(f"  Max: {catboost_test_preds.max():.4f}")

# %% trusted=true
# LightGBMã§ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹æ¨è«–ï¼ˆæ—¢å­˜ã®modelsã‚’ä½¿ç”¨ï¼‰
print("LightGBMã§ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹æ¨è«–ã‚’å®Ÿè¡Œä¸­...")

lgbm_test_preds = np.zeros(len(X_test))

for fold, model in enumerate(models):
    fold_preds = model.predict(X_test, num_iteration=model.best_iteration)
    lgbm_test_preds += fold_preds / 5
    print(f"Fold {fold + 1}: äºˆæ¸¬å®Œäº†")

print(f"\nLightGBMäºˆæ¸¬çµ±è¨ˆ:")
print(f"  Mean: {lgbm_test_preds.mean():.4f}")
print(f"  Std: {lgbm_test_preds.std():.4f}")
print(f"  Min: {lgbm_test_preds.min():.4f}")
print(f"  Max: {lgbm_test_preds.max():.4f}")

# %% trusted=true
# æœ€é©ãªé‡ã¿ã§ãƒ–ãƒ¬ãƒ³ãƒ‰äºˆæ¸¬ã‚’ä½œæˆ
print("\nãƒ–ãƒ¬ãƒ³ãƒ‰äºˆæ¸¬ã‚’ä½œæˆä¸­...")

blended_test_preds = (
    best_blend_weight * lgbm_test_preds + 
    best_catboost_weight * catboost_test_preds
)

print(f"\nãƒ–ãƒ¬ãƒ³ãƒ‰äºˆæ¸¬çµ±è¨ˆ:")
print(f"  Mean: {blended_test_preds.mean():.4f}")
print(f"  Std: {blended_test_preds.std():.4f}")
print(f"  Min: {blended_test_preds.min():.4f}")
print(f"  Max: {blended_test_preds.max():.4f}")

# äºˆæ¸¬å€¤ã®åˆ†å¸ƒã‚’å¯è¦–åŒ–
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

axes[0].hist(lgbm_test_preds, bins=50, alpha=0.7, color='blue', edgecolor='black')
axes[0].set_title('LightGBMäºˆæ¸¬åˆ†å¸ƒ', fontsize=14)
axes[0].set_xlabel('xAGäºˆæ¸¬å€¤')
axes[0].set_ylabel('é »åº¦')
axes[0].grid(alpha=0.3)

axes[1].hist(catboost_test_preds, bins=50, alpha=0.7, color='green', edgecolor='black')
axes[1].set_title('CatBoostäºˆæ¸¬åˆ†å¸ƒ', fontsize=14)
axes[1].set_xlabel('xAGäºˆæ¸¬å€¤')
axes[1].set_ylabel('é »åº¦')
axes[1].grid(alpha=0.3)

axes[2].hist(blended_test_preds, bins=50, alpha=0.7, color='purple', edgecolor='black')
axes[2].set_title('ãƒ–ãƒ¬ãƒ³ãƒ‰äºˆæ¸¬åˆ†å¸ƒ', fontsize=14)
axes[2].set_xlabel('xAGäºˆæ¸¬å€¤')
axes[2].set_ylabel('é »åº¦')
axes[2].grid(alpha=0.3)

plt.tight_layout()
plt.show()

# ãƒ¢ãƒ‡ãƒ«é–“ã®ç›¸é–¢ã‚’ç¢ºèª
correlation = np.corrcoef(lgbm_test_preds, catboost_test_preds)[0, 1]
print(f"\nLGBMã¨CatBoostã®äºˆæ¸¬ç›¸é–¢: {correlation:.4f}")
print(f"(ç›¸é–¢ãŒä½ã„ã»ã©ã€ãƒ–ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã«ã‚ˆã‚‹æ”¹å–„åŠ¹æœãŒå¤§ãã„)")

# %% trusted=true
# [exp0028] Isotonicæ ¡æ­£: æ­£é ˜åŸŸï¼ˆy >= 0.1ï¼‰ã®ã¿ã‚’æ ¡æ­£
print("=" * 80)
print("Isotonicæ ¡æ­£ã‚’é©ç”¨ä¸­...")
print("=" * 80)

from scripts.calibration import fit_isotonic_positive, apply_isotonic_positive

# OOFäºˆæ¸¬ã§æ ¡æ­£ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
print("\n1. OOFäºˆæ¸¬ã§Isotonicãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ä¸­...")
iso_model = fit_isotonic_positive(
    y_oof_true=y_train.values,
    y_oof_pred=blended_oof_preds,  # ãƒ–ãƒ¬ãƒ³ãƒ‰å¾Œã®OOFäºˆæ¸¬
    threshold=0.1,
    pos_weight=5.0
)

# OOFäºˆæ¸¬ã‚’æ ¡æ­£ã—ã¦è©•ä¾¡
print("\n2. OOFäºˆæ¸¬ã‚’æ ¡æ­£ä¸­...")
calibrated_oof_preds = apply_isotonic_positive(
    blended_oof_preds,
    iso_model,
    threshold=0.1
)

# æ ¡æ­£å‰å¾Œã®ã‚¹ã‚³ã‚¢æ¯”è¼ƒ
before_rmse = weighted_rmse(y_train.values, blended_oof_preds)
after_rmse = weighted_rmse(y_train.values, calibrated_oof_preds)

print(f"\nã€æ ¡æ­£åŠ¹æœã€‘")
print(f"  æ ¡æ­£å‰ wRMSE: {before_rmse:.6f}")
print(f"  æ ¡æ­£å¾Œ wRMSE: {after_rmse:.6f}")
print(f"  æ”¹å–„é‡: {before_rmse - after_rmse:.6f} ({(before_rmse - after_rmse) / before_rmse * 100:.2f}%)")

# æ­£é ˜åŸŸãƒ»è² é ˜åŸŸåˆ¥ã®åˆ†æ
from sklearn.metrics import mean_squared_error

pos_mask = y_train.values >= 0.1
neg_mask = y_train.values < 0.1

print(f"\nã€æ­£é ˜åŸŸï¼ˆy >= 0.1ï¼‰ã€‘")
print(f"  ã‚µãƒ³ãƒ—ãƒ«æ•°: {pos_mask.sum()}")
print(f"  æ ¡æ­£å‰ RMSE: {np.sqrt(mean_squared_error(y_train[pos_mask], blended_oof_preds[pos_mask])):.6f}")
print(f"  æ ¡æ­£å¾Œ RMSE: {np.sqrt(mean_squared_error(y_train[pos_mask], calibrated_oof_preds[pos_mask])):.6f}")

print(f"\nã€è² é ˜åŸŸï¼ˆy < 0.1ï¼‰ã€‘")
print(f"  ã‚µãƒ³ãƒ—ãƒ«æ•°: {neg_mask.sum()}")
print(f"  æ ¡æ­£å‰ RMSE: {np.sqrt(mean_squared_error(y_train[neg_mask], blended_oof_preds[neg_mask])):.6f}")
print(f"  æ ¡æ­£å¾Œ RMSE: {np.sqrt(mean_squared_error(y_train[neg_mask], calibrated_oof_preds[neg_mask])):.6f}")

# 3. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«é©ç”¨
print("\n3. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«æ ¡æ­£ã‚’é©ç”¨ä¸­...")
calibrated_test_preds = apply_isotonic_positive(
    blended_test_preds,
    iso_model,
    threshold=0.1
)

print(f"\nã€ãƒ†ã‚¹ãƒˆäºˆæ¸¬ã®çµ±è¨ˆã€‘")
print(f"  æ ¡æ­£å‰:")
print(f"    Mean: {blended_test_preds.mean():.6f}")
print(f"    Std:  {blended_test_preds.std():.6f}")
print(f"    Min:  {blended_test_preds.min():.6f}")
print(f"    Max:  {blended_test_preds.max():.6f}")
print(f"  æ ¡æ­£å¾Œ:")
print(f"    Mean: {calibrated_test_preds.mean():.6f}")
print(f"    Std:  {calibrated_test_preds.std():.6f}")
print(f"    Min:  {calibrated_test_preds.min():.6f}")
print(f"    Max:  {calibrated_test_preds.max():.6f}")

# 4. ãƒ–ãƒ¬ãƒ³ãƒ‰äºˆæ¸¬ã‚’æ ¡æ­£ç‰ˆã«ç½®ãæ›ãˆ
blended_test_preds = calibrated_test_preds
blended_oof_preds = calibrated_oof_preds

print("\nâœ… Isotonicæ ¡æ­£å®Œäº†")


# %% [markdown] id="Sap_9i9DaIf3"
# ## ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹æ¨è«–

# %% colab={"base_uri": "https://localhost:8080/"} id="3Xs1lGqfaIf4" outputId="47eceb84-34b8-480a-dfd5-85eebb203bc7" trusted=true
# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ï¼ˆ5ãƒ¢ãƒ‡ãƒ«ã®å¹³å‡ï¼‰ on Test Data
test_preds = np.zeros(len(X_test))

for model in models:
    pred = model.predict(X_test, num_iteration=model.best_iteration)
    test_preds += pred

test_preds /= len(models)


print(f"\n=== Test Set Predictions ===")
print(f"äºˆæ¸¬xAGç¯„å›²: {test_preds.min():.3f} ã€œ {test_preds.max():.3f}")

# test_dfã«äºˆæ¸¬çµæœã‚’è¿½åŠ 
test_df['predicted_xAG'] = test_preds

# %% [markdown] id="yL_4w1qWaIf4"
# ## äºˆæ¸¬çµæœã®åˆ†æ

# %% colab={"base_uri": "https://localhost:8080/", "height": 998} id="KLKnE3KajdZp" outputId="d9b9cad3-dedc-458d-aad3-e23986d7e5a3" trusted=true
# ç‰¹å¾´é‡é‡è¦åº¦ã®å¹³å‡è¨ˆç®—
feature_importance_mean = feature_importance.groupby('feature')['importance'].agg(['mean', 'std']).reset_index()
feature_importance_mean = feature_importance_mean.sort_values('mean', ascending=False)

# å¯è¦–åŒ–
plt.figure(figsize=(10, 8))
sns.barplot(data=feature_importance_mean.head(15), x='mean', y='feature')
plt.title('Top 15 Feature Importance (Weighted RMSE Baseline xAG Model)')
plt.xlabel('Importance')
plt.tight_layout()
plt.show()

print("ç‰¹å¾´é‡é‡è¦åº¦ Top 10:")
print(feature_importance_mean.head(10))

# %% colab={"base_uri": "https://localhost:8080/", "height": 607} id="kGIeBEIPaIf4" outputId="92e7fb77-a19e-45f3-c778-c010f56a2adf" trusted=true
# train, testäºˆæ¸¬å€¤ã®åˆ†å¸ƒã‚’å¯è¦–åŒ–
plt.figure(figsize=(8, 6))
sns.histplot(oof_preds, stat='density', kde=True, alpha=0.2, label='OOFäºˆæ¸¬', linewidth=0)
sns.histplot(test_preds, stat='density', kde=True, alpha=0.2, label='Testäºˆæ¸¬', linewidth=0)

# trainæ­£è§£å€¤ã®åˆ†å¸ƒã‚’å¯è¦–åŒ–
vc = y_train.value_counts().sort_index()
heights = vc / vc.sum() / 0.1 # æ£’ã‚°ãƒ©ãƒ•ã®é«˜ã•ã‚’density ã«åˆã‚ã›ã‚‹
plt.bar(vc.index, heights, width=0.03, alpha=0.6, label='OOFæ­£è§£', align='center')

plt.xlabel('xAG')
plt.ylabel('å¯†åº¦')
plt.xlim(0, 1)
plt.title('xAGäºˆæ¸¬å€¤ã®åˆ†å¸ƒï¼ˆOOFäºˆæ¸¬ vs Testäºˆæ¸¬ vs OOFæ­£è§£ï¼‰')
plt.legend()
plt.tight_layout()
plt.show()

# %% [markdown] id="61-ufcOruCGb"
# æ­£è§£ãŒ0.0ã®ãƒ‡ãƒ¼ã‚¿ã®é‡ã¿ä»˜ã‘ãŒå°ã•ã„ãŸã‚ã€å…¨ä½“çš„ã«æ­£ã®å€¤ã‚’äºˆæƒ³ã™ã‚‹å‚¾å‘ãŒè¦‹ã‚‰ã‚Œã‚‹ã€‚

# %% [markdown] id="u4I2O1ntaIf4"
# ## æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ

# %% id="cbhq5ARAaIf4" trusted=true
# ãƒ–ãƒ¬ãƒ³ãƒ‰ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã§æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
submission_df['xAG'] = blended_test_preds

# æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
submission_path = log_dir / 'submission_blend_lgbm_catboost.csv'
submission_df.to_csv(submission_path, index=False)

print(f"æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {submission_path}")
print(f"\næå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®çµ±è¨ˆ:")
print(submission_df['xAG'].describe())

# å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã®æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ä¿å­˜
submission_lgbm = submission_df.copy()
submission_lgbm['xAG'] = lgbm_test_preds
submission_lgbm.to_csv(log_dir / 'submission_lgbm_only.csv', index=False)
print(f"\nLightGBMå˜ç‹¬ã®æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ä¿å­˜: {log_dir / 'submission_lgbm_only.csv'}")

submission_catboost = submission_df.copy()
submission_catboost['xAG'] = catboost_test_preds
submission_catboost.to_csv(log_dir / 'submission_catboost_only.csv', index=False)
print(f"CatBoostå˜ç‹¬ã®æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ä¿å­˜: {log_dir / 'submission_catboost_only.csv'}")

# ã‚µãƒãƒªãƒ¼è¡¨ç¤º
print("\n=== æœ€çµ‚çµæœã‚µãƒãƒªãƒ¼ ===")
print(f"LightGBM OOF RMSE: {oof_score:.4f}")
print(f"CatBoost OOF RMSE: {catboost_oof_score:.4f}")
print(f"ãƒ–ãƒ¬ãƒ³ãƒ‰OOF RMSE: {best_blend_score:.4f}")
print(f"\nãƒ–ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æ¯”ç‡:")
print(f"  LightGBM: {best_blend_weight:.2f}")
print(f"  CatBoost: {best_catboost_weight:.2f}")
print(f"\næå‡ºãƒ•ã‚¡ã‚¤ãƒ«:")
print(f"  1. submission_blend_lgbm_catboost.csv (æ¨å¥¨)")
print(f"  2. submission_lgbm_only.csv")
print(f"  3. submission_catboost_only.csv")

