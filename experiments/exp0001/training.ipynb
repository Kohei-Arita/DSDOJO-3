{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Training Notebook - exp0001\n",
    "\n",
    "**実験概要**: Baseline model with title and family features\n",
    "\n",
    "**実行環境**: \n",
    "- Google Colab (GPU)\n",
    "- LightGBM with categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontent/LIGHTBGM-TEM\"):\n",
    "    !git clone https://github.com/Kohei-Arita/LIGHTBGM-TEM.git\n",
    "\n",
    "%cd /content/LIGHTBGM-TEM/kaggle-projects/titanic/experiments/exp0001# Colab環境セットアップ\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# ランタイムタイプの確認\n",
    "!nvidia-smi\n",
    "\n",
    "# リポジトリクローン（初回のみ）\n",
    "if not os.path.exists(\"/c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 依存関係のインストール\n",
    "!pip install -r env/requirements.lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APIキーの設定（Secrets経由）\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = userdata.get(\"WANDB_API_KEY\")\n",
    "os.environ[\"KAGGLE_USERNAME\"] = userdata.get(\"KAGGLE_USERNAME\")\n",
    "os.environ[\"KAGGLE_KEY\"] = userdata.get(\"KAGGLE_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import yaml\n",
    "import json\n",
    "import wandb\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 設定読み込み\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "print(f\"実験ID: {cfg['experiment']['id']}\")\n",
    "print(f\"実験説明: {cfg['experiment']['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W&B初期化\n",
    "run = wandb.init(\n",
    "    project=cfg[\"wandb\"][\"project\"], name=cfg[\"experiment\"][\"id\"], config=cfg, tags=cfg[\"wandb\"][\"tags\"], job_type=\"train\"\n",
    ")\n",
    "\n",
    "# W&B Run URLを保存\n",
    "with open(\"wandb_run.txt\", \"w\") as f:\n",
    "    f.write(f\"{run.url}\\n{run.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Git SHA取得\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    git_sha = subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"]).decode(\"ascii\").strip()\n",
    "except:\n",
    "    git_sha = \"unknown\"\n",
    "\n",
    "with open(\"git_sha.txt\", \"w\") as f:\n",
    "    f.write(git_sha)\n",
    "\n",
    "print(f\"Git SHA: {git_sha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# データ読み込み関数\ndef load_and_preprocess_data(cfg):\n    \"\"\"データの読み込みと前処理\"\"\"\n\n    # 生データ読み込み\n    train_df = pd.read_csv(f\"{cfg['paths']['raw_dir']}/train.csv\")\n    test_df = pd.read_csv(f\"{cfg['paths']['raw_dir']}/test.csv\")\n\n    # データ結合\n    all_data = pd.concat([train_df, test_df], ignore_index=True)\n\n    # 欠損値処理\n    all_data[\"Age\"].fillna(all_data[\"Age\"].median(), inplace=True)\n    all_data[\"Fare\"].fillna(all_data[\"Fare\"].median(), inplace=True)\n    all_data[\"Embarked\"].fillna(all_data[\"Embarked\"].mode()[0], inplace=True)\n\n    # 特徴量エンジニアリング\n    # Title抽出\n    all_data[\"Title\"] = all_data[\"Name\"].str.extract(\" ([A-Za-z]+)\\.\")\n    title_mapping = {\n        \"Mr\": \"Mr\",\n        \"Mrs\": \"Mrs\",\n        \"Miss\": \"Miss\",\n        \"Master\": \"Master\",\n        \"Dr\": \"Rare\",\n        \"Rev\": \"Rare\",\n        \"Col\": \"Rare\",\n        \"Major\": \"Rare\",\n        \"Mlle\": \"Miss\",\n        \"Countess\": \"Rare\",\n        \"Ms\": \"Mrs\",\n        \"Lady\": \"Rare\",\n        \"Jonkheer\": \"Rare\",\n        \"Don\": \"Rare\",\n        \"Dona\": \"Rare\",\n        \"Mme\": \"Mrs\",\n        \"Capt\": \"Rare\",\n        \"Sir\": \"Rare\",\n    }\n    all_data[\"Title\"] = all_data[\"Title\"].map(title_mapping).fillna(\"Rare\")\n\n    # Family features\n    all_data[\"FamilySize\"] = all_data[\"SibSp\"] + all_data[\"Parch\"] + 1\n    all_data[\"IsAlone\"] = (all_data[\"FamilySize\"] == 1).astype(int)\n\n    # Age bands\n    all_data[\"AgeBand\"] = pd.cut(\n        all_data[\"Age\"], bins=[0, 12, 18, 35, 60, 100], labels=[\"Child\", \"Teen\", \"Adult\", \"Middle\", \"Senior\"]\n    )\n\n    # Fare bands\n    all_data[\"FareBand\"] = pd.qcut(all_data[\"Fare\"], q=4, labels=[\"Low\", \"Medium\", \"High\", \"VeryHigh\"])\n\n    # 学習・テストに分離\n    train_data = all_data[: len(train_df)].copy()\n    test_data = all_data[len(train_df) :].copy()\n\n    # 特徴量とターゲットを分離\n    feature_cols = cfg[\"features\"][\"use\"]\n\n    X = train_data[feature_cols].copy()\n    y = train_data[cfg[\"data\"][\"target\"]]\n    X_test = test_data[feature_cols].copy()\n\n    # カテゴリカル変数をLightGBM用にカテゴリ型に変換\n    categorical_cols = cfg[\"data\"][\"categorical\"]\n    for col in categorical_cols:\n        if col in X.columns:\n            X[col] = X[col].astype(\"category\")\n            X_test[col] = X_test[col].astype(\"category\")\n\n    return X, y, X_test, train_data, test_data\n\n\n# データ読み込み\nX, y, X_test, train_data, test_data = load_and_preprocess_data(cfg)\n\nprint(f\"学習データ形状: {X.shape}\")\nprint(f\"テストデータ形状: {X_test.shape}\")\nprint(f\"特徴量: {list(X.columns)}\")\n\n# デバッグ: データ型を確認\nprint(f\"\\nデータ型:\")\nprint(X.dtypes)\nprint(f\"\\nカテゴリカル変数: {cfg['data']['categorical']}\")\nfor col in cfg[\"data\"][\"categorical\"]:\n    if col in X.columns:\n        print(f\"{col}: {X[col].dtype}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV分割を作成・保存\n",
    "skf = StratifiedKFold(n_splits=cfg[\"cv\"][\"n_splits\"], shuffle=cfg[\"cv\"][\"shuffle\"], random_state=cfg[\"cv\"][\"seed\"])\n",
    "\n",
    "# split_id（分割の一意識別子）を生成\n",
    "split_config = f\"stratified_{cfg['cv']['n_splits']}_{cfg['cv']['seed']}\"\n",
    "split_id = hashlib.md5(split_config.encode()).hexdigest()[:8]\n",
    "\n",
    "# CV分割をDataFrameとして保存\n",
    "cv_folds = []\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n",
    "    for idx in train_idx:\n",
    "        cv_folds.append({\"index\": idx, \"fold\": -1})  # train folds are marked as -1\n",
    "    for idx in valid_idx:\n",
    "        cv_folds.append({\"index\": idx, \"fold\": fold})\n",
    "\n",
    "cv_folds_df = pd.DataFrame(cv_folds)\n",
    "cv_folds_df[\"split_id\"] = split_id\n",
    "cv_folds_df.to_parquet(\"cv_folds.parquet\", index=False)\n",
    "\n",
    "print(f\"CV分割保存完了: {split_id}\")\n",
    "print(cv_folds_df[\"fold\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 学習ループ\noof_predictions = np.zeros(len(X))\ntest_predictions = np.zeros(len(X_test))\nfold_scores = []\nfold_models = []\nfeature_importance = pd.DataFrame()\n\nstart_time = datetime.now()\n\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n    print(f\"\\n=== Fold {fold + 1}/{cfg['cv']['n_splits']} ===\")\n\n    # データ分割\n    X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n    y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n\n    # LightGBM Dataset作成\n    # カテゴリカル特徴量のインデックスを取得\n    categorical_features = [col for col in cfg[\"data\"][\"categorical\"] if col in X.columns]\n    \n    train_dataset = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n\n    valid_dataset = lgb.Dataset(\n        X_valid, label=y_valid, categorical_feature=categorical_features, reference=train_dataset\n    )\n\n    # モデル学習\n    model = lgb.train(\n        cfg[\"lgbm\"][\"params\"],\n        train_dataset,\n        valid_sets=[train_dataset, valid_dataset],\n        valid_names=[\"train\", \"valid\"],\n        num_boost_round=cfg[\"lgbm\"][\"train\"][\"num_boost_round\"],\n        callbacks=[\n            lgb.early_stopping(cfg[\"lgbm\"][\"train\"][\"early_stopping_rounds\"], verbose=False),\n            lgb.log_evaluation(cfg[\"lgbm\"][\"train\"][\"verbose_eval\"]),\n        ],\n    )\n\n    # 予測\n    valid_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n    test_pred = model.predict(X_test, num_iteration=model.best_iteration)\n\n    # OOF予測を保存\n    oof_predictions[valid_idx] = valid_pred\n    test_predictions += test_pred / cfg[\"cv\"][\"n_splits\"]\n\n    # スコア計算\n    fold_score = roc_auc_score(y_valid, valid_pred)\n    fold_scores.append(fold_score)\n\n    print(f\"Fold {fold + 1} AUC: {fold_score:.6f}\")\n    print(f\"Best iteration: {model.best_iteration}\")\n\n    # モデル保存\n    model_path = f\"model/fold{fold}.lgb\"\n    model.save_model(model_path)\n    fold_models.append(model_path)\n\n    # Feature importance\n    fold_importance = pd.DataFrame(\n        {\"feature\": X.columns, \"importance\": model.feature_importance(importance_type=\"gain\"), \"fold\": fold}\n    )\n    feature_importance = pd.concat([feature_importance, fold_importance])\n\n    # W&B logging\n    wandb.log({f\"fold_{fold}_auc\": fold_score, f\"fold_{fold}_best_iter\": model.best_iteration})\n\ntrain_time = (datetime.now() - start_time).total_seconds()\nprint(f\"\\n=== 学習完了 ===\")\nprint(f\"OOF AUC: {roc_auc_score(y, oof_predictions):.6f}\")\nprint(f\"CV AUC: {np.mean(fold_scores):.6f} ± {np.std(fold_scores):.6f}\")\nprint(f\"学習時間: {train_time:.1f}秒\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOF予測を保存\n",
    "oof_df = pd.DataFrame(\n",
    "    {\n",
    "        \"index\": range(len(X)),\n",
    "        \"fold\": -1,  # 後でcv_foldsから正しいfoldを割り当て\n",
    "        \"y_true\": y,\n",
    "        \"y_pred\": oof_predictions,\n",
    "    }\n",
    ")\n",
    "\n",
    "# 正しいfold番号を割り当て\n",
    "for fold, (_, valid_idx) in enumerate(skf.split(X, y)):\n",
    "    oof_df.loc[valid_idx, \"fold\"] = fold\n",
    "\n",
    "oof_df.to_parquet(\"oof.parquet\", index=False)\n",
    "\n",
    "# メトリクス保存\n",
    "metrics = {\n",
    "    \"cv\": {\n",
    "        \"metric\": \"auc\",\n",
    "        \"mean\": float(np.mean(fold_scores)),\n",
    "        \"std\": float(np.std(fold_scores)),\n",
    "        \"per_fold\": [float(score) for score in fold_scores],\n",
    "    },\n",
    "    \"train_time_sec\": train_time,\n",
    "    \"best_iteration_per_fold\": [model.best_iteration for model in lgb.Booster(model_file=path) for path in fold_models],\n",
    "}\n",
    "\n",
    "with open(\"metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "# 特徴量リスト保存\n",
    "with open(\"feature_list.txt\", \"w\") as f:\n",
    "    for feature in X.columns:\n",
    "        f.write(f\"{feature}\\n\")\n",
    "\n",
    "print(\"実験成果物保存完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W&B Artifacts保存\n",
    "artifact = wandb.Artifact(f\"{cfg['experiment']['id']}-artifacts\", type=\"experiment\")\n",
    "\n",
    "# ファイルを追加\n",
    "files_to_log = [\"config.yaml\", \"cv_folds.parquet\", \"oof.parquet\", \"metrics.json\", \"feature_list.txt\", \"git_sha.txt\"]\n",
    "\n",
    "for file_path in files_to_log:\n",
    "    if Path(file_path).exists():\n",
    "        artifact.add_file(file_path)\n",
    "\n",
    "# モデルファイルを追加\n",
    "for model_path in fold_models:\n",
    "    artifact.add_file(model_path)\n",
    "\n",
    "# アーティファクトをログ\n",
    "run.log_artifact(artifact)\n",
    "\n",
    "# 最終メトリクスをログ\n",
    "wandb.log({\"cv_auc_mean\": metrics[\"cv\"][\"mean\"], \"cv_auc_std\": metrics[\"cv\"][\"std\"], \"train_time_sec\": train_time})\n",
    "\n",
    "# Feature importance plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "feature_importance_mean = feature_importance.groupby(\"feature\")[\"importance\"].mean().sort_values(ascending=False)\n",
    "sns.barplot(x=feature_importance_mean.values, y=feature_importance_mean.index)\n",
    "plt.title(\"Feature Importance (Mean across folds)\")\n",
    "plt.tight_layout()\n",
    "wandb.log({\"feature_importance\": wandb.Image(plt)})\n",
    "plt.show()\n",
    "\n",
    "wandb.finish()\n",
    "print(\"W&B logging完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提出用予測作成（テストセット）\n",
    "submission = pd.DataFrame(\n",
    "    {cfg[\"data\"][\"id\"]: test_data[cfg[\"data\"][\"id\"]], cfg[\"data\"][\"target\"]: (test_predictions > 0.5).astype(int)}\n",
    ")\n",
    "\n",
    "submission_path = f\"submissions/submission.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"提出ファイル作成完了: {submission_path}\")\n",
    "print(f\"予測分布: {submission[cfg['data']['target']].value_counts().sort_index()}\")\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実験ノート\n",
    "notes = f\"\"\"\n",
    "# 実験ノート - {cfg[\"experiment\"][\"id\"]}\n",
    "\n",
    "## 実験設定\n",
    "- 日付: {cfg[\"experiment\"][\"date\"]}\n",
    "- 説明: {cfg[\"experiment\"][\"description\"]}\n",
    "- Git SHA: {git_sha}\n",
    "\n",
    "## 結果\n",
    "- CV AUC: {np.mean(fold_scores):.6f} ± {np.std(fold_scores):.6f}\n",
    "- 学習時間: {train_time:.1f}秒\n",
    "\n",
    "## 使用特徴量\n",
    "{chr(10).join([f\"- {feat}\" for feat in X.columns])}\n",
    "\n",
    "## 所感\n",
    "- ベースラインとして良好なスコア\n",
    "- TitleとFamilySize特徴量が有効\n",
    "- 次回: ハイパーパラメータ調整\n",
    "\n",
    "## 次のアクション\n",
    "- [ ] Optunaでハイパーパラメータ最適化\n",
    "- [ ] 他のCVスキーム（GroupKFold）を試す\n",
    "- [ ] 新しい特徴量を追加\n",
    "\"\"\"\n",
    "\n",
    "with open(\"notes.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(notes)\n",
    "\n",
    "print(\"実験ノート保存完了\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}