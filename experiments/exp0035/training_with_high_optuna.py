# ---
# jupyter:
#   colab:
#     provenance: []
#   jupytext:
#     cell_metadata_filter: all
#     formats: ipynb,py:percent
#     notebook_metadata_filter: all,-jupytext.text_representation.jupytext_version
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#   kernelspec:
#     display_name: Python 3 (ipykernel)
#     language: python
#     name: python3
#   language_info:
#     codemirror_mode:
#       name: ipython
#       version: 3
#     file_extension: .py
#     mimetype: text/x-python
#     name: python
#     nbconvert_exporter: python
#     pygments_lexer: ipython3
#     version: 3.11.13
#   widgets:
#     application/vnd.jupyter.widget-state+json:
#       09a43976bef243599b2e9b64cffb91b4:
#         model_module: '@jupyter-widgets/controls'
#         model_module_version: 1.5.0
#         model_name: HBoxModel
#         state:
#           _dom_classes: []
#           _model_module: '@jupyter-widgets/controls'
#           _model_module_version: 1.5.0
#           _model_name: HBoxModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/controls'
#           _view_module_version: 1.5.0
#           _view_name: HBoxView
#           box_style: ''
#           children:
#           - IPY_MODEL_d3e34a9bb0d24577acc83d7ae4b2486d
#           - IPY_MODEL_dcffd72448ea44bf9ec8aec5b31762f6
#           - IPY_MODEL_3a83f4956d7d41938ef54f146a5cc541
#           layout: IPY_MODEL_69216dc0f0f4422e8166c4a8f8abe666
#       09a7b0f32ef942afb2963f355e80a8bc:
#         model_module: '@jupyter-widgets/controls'
#         model_module_version: 1.5.0
#         model_name: DescriptionStyleModel
#         state:
#           _model_module: '@jupyter-widgets/controls'
#           _model_module_version: 1.5.0
#           _model_name: DescriptionStyleModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/base'
#           _view_module_version: 1.2.0
#           _view_name: StyleView
#           description_width: ''
#       3a83f4956d7d41938ef54f146a5cc541:
#         model_module: '@jupyter-widgets/controls'
#         model_module_version: 1.5.0
#         model_name: HTMLModel
#         state:
#           _dom_classes: []
#           _model_module: '@jupyter-widgets/controls'
#           _model_module_version: 1.5.0
#           _model_name: HTMLModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/controls'
#           _view_module_version: 1.5.0
#           _view_name: HTMLView
#           description: ''
#           description_tooltip: null
#           layout: IPY_MODEL_5b6ef11c003d4a79b3924bc396c0ff56
#           placeholder: "\u200B"
#           style: IPY_MODEL_78773c5f4ebc47fea4077d687ab0ea0c
#           value: "\u200740041/40041\u2007[03:10&lt;00:00,\u2007234.12it/s]"
#       5b6ef11c003d4a79b3924bc396c0ff56:
#         model_module: '@jupyter-widgets/base'
#         model_module_version: 1.2.0
#         model_name: LayoutModel
#         state:
#           _model_module: '@jupyter-widgets/base'
#           _model_module_version: 1.2.0
#           _model_name: LayoutModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/base'
#           _view_module_version: 1.2.0
#           _view_name: LayoutView
#           align_content: null
#           align_items: null
#           align_self: null
#           border: null
#           bottom: null
#           display: null
#           flex: null
#           flex_flow: null
#           grid_area: null
#           grid_auto_columns: null
#           grid_auto_flow: null
#           grid_auto_rows: null
#           grid_column: null
#           grid_gap: null
#           grid_row: null
#           grid_template_areas: null
#           grid_template_columns: null
#           grid_template_rows: null
#           height: null
#           justify_content: null
#           justify_items: null
#           left: null
#           margin: null
#           max_height: null
#           max_width: null
#           min_height: null
#           min_width: null
#           object_fit: null
#           object_position: null
#           order: null
#           overflow: null
#           overflow_x: null
#           overflow_y: null
#           padding: null
#           right: null
#           top: null
#           visibility: null
#           width: null
#       5e50717143ae4771b600c2a5f73488e2:
#         model_module: '@jupyter-widgets/controls'
#         model_module_version: 1.5.0
#         model_name: ProgressStyleModel
#         state:
#           _model_module: '@jupyter-widgets/controls'
#           _model_module_version: 1.5.0
#           _model_name: ProgressStyleModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/base'
#           _view_module_version: 1.2.0
#           _view_name: StyleView
#           bar_color: null
#           description_width: ''
#       69216dc0f0f4422e8166c4a8f8abe666:
#         model_module: '@jupyter-widgets/base'
#         model_module_version: 1.2.0
#         model_name: LayoutModel
#         state:
#           _model_module: '@jupyter-widgets/base'
#           _model_module_version: 1.2.0
#           _model_name: LayoutModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/base'
#           _view_module_version: 1.2.0
#           _view_name: LayoutView
#           align_content: null
#           align_items: null
#           align_self: null
#           border: null
#           bottom: null
#           display: null
#           flex: null
#           flex_flow: null
#           grid_area: null
#           grid_auto_columns: null
#           grid_auto_flow: null
#           grid_auto_rows: null
#           grid_column: null
#           grid_gap: null
#           grid_row: null
#           grid_template_areas: null
#           grid_template_columns: null
#           grid_template_rows: null
#           height: null
#           justify_content: null
#           justify_items: null
#           left: null
#           margin: null
#           max_height: null
#           max_width: null
#           min_height: null
#           min_width: null
#           object_fit: null
#           object_position: null
#           order: null
#           overflow: null
#           overflow_x: null
#           overflow_y: null
#           padding: null
#           right: null
#           top: null
#           visibility: null
#           width: null
#       78773c5f4ebc47fea4077d687ab0ea0c:
#         model_module: '@jupyter-widgets/controls'
#         model_module_version: 1.5.0
#         model_name: DescriptionStyleModel
#         state:
#           _model_module: '@jupyter-widgets/controls'
#           _model_module_version: 1.5.0
#           _model_name: DescriptionStyleModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/base'
#           _view_module_version: 1.2.0
#           _view_name: StyleView
#           description_width: ''
#       79fcd266aa524e9b9d656b2eab1a8cd3:
#         model_module: '@jupyter-widgets/base'
#         model_module_version: 1.2.0
#         model_name: LayoutModel
#         state:
#           _model_module: '@jupyter-widgets/base'
#           _model_module_version: 1.2.0
#           _model_name: LayoutModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/base'
#           _view_module_version: 1.2.0
#           _view_name: LayoutView
#           align_content: null
#           align_items: null
#           align_self: null
#           border: null
#           bottom: null
#           display: null
#           flex: null
#           flex_flow: null
#           grid_area: null
#           grid_auto_columns: null
#           grid_auto_flow: null
#           grid_auto_rows: null
#           grid_column: null
#           grid_gap: null
#           grid_row: null
#           grid_template_areas: null
#           grid_template_columns: null
#           grid_template_rows: null
#           height: null
#           justify_content: null
#           justify_items: null
#           left: null
#           margin: null
#           max_height: null
#           max_width: null
#           min_height: null
#           min_width: null
#           object_fit: null
#           object_position: null
#           order: null
#           overflow: null
#           overflow_x: null
#           overflow_y: null
#           padding: null
#           right: null
#           top: null
#           visibility: null
#           width: null
#       d3e34a9bb0d24577acc83d7ae4b2486d:
#         model_module: '@jupyter-widgets/controls'
#         model_module_version: 1.5.0
#         model_name: HTMLModel
#         state:
#           _dom_classes: []
#           _model_module: '@jupyter-widgets/controls'
#           _model_module_version: 1.5.0
#           _model_name: HTMLModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/controls'
#           _view_module_version: 1.5.0
#           _view_name: HTMLView
#           description: ''
#           description_tooltip: null
#           layout: IPY_MODEL_79fcd266aa524e9b9d656b2eab1a8cd3
#           placeholder: "\u200B"
#           style: IPY_MODEL_09a7b0f32ef942afb2963f355e80a8bc
#           value: "Calculating\u2007success\u2007rates:\u2007100%"
#       dc043a4a5ed74b1c9b6cc8e027e5b2b6:
#         model_module: '@jupyter-widgets/base'
#         model_module_version: 1.2.0
#         model_name: LayoutModel
#         state:
#           _model_module: '@jupyter-widgets/base'
#           _model_module_version: 1.2.0
#           _model_name: LayoutModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/base'
#           _view_module_version: 1.2.0
#           _view_name: LayoutView
#           align_content: null
#           align_items: null
#           align_self: null
#           border: null
#           bottom: null
#           display: null
#           flex: null
#           flex_flow: null
#           grid_area: null
#           grid_auto_columns: null
#           grid_auto_flow: null
#           grid_auto_rows: null
#           grid_column: null
#           grid_gap: null
#           grid_row: null
#           grid_template_areas: null
#           grid_template_columns: null
#           grid_template_rows: null
#           height: null
#           justify_content: null
#           justify_items: null
#           left: null
#           margin: null
#           max_height: null
#           max_width: null
#           min_height: null
#           min_width: null
#           object_fit: null
#           object_position: null
#           order: null
#           overflow: null
#           overflow_x: null
#           overflow_y: null
#           padding: null
#           right: null
#           top: null
#           visibility: null
#           width: null
#       dcffd72448ea44bf9ec8aec5b31762f6:
#         model_module: '@jupyter-widgets/controls'
#         model_module_version: 1.5.0
#         model_name: FloatProgressModel
#         state:
#           _dom_classes: []
#           _model_module: '@jupyter-widgets/controls'
#           _model_module_version: 1.5.0
#           _model_name: FloatProgressModel
#           _view_count: null
#           _view_module: '@jupyter-widgets/controls'
#           _view_module_version: 1.5.0
#           _view_name: ProgressView
#           bar_style: success
#           description: ''
#           description_tooltip: null
#           layout: IPY_MODEL_dc043a4a5ed74b1c9b6cc8e027e5b2b6
#           max: 40041
#           min: 0
#           orientation: horizontal
#           style: IPY_MODEL_5e50717143ae4771b600c2a5f73488e2
#           value: 40041
# ---

# %% [markdown] id="HU48Dzb4iuhP"
# # xAG予測コンペ　ベースラインコード（その2）
#
# ベースラインコード（host_baseline_001.ipynb）について、特徴量の追加作成やパラメータ最適化を行った改善版コードです。

# %% id="4N4IPuA_J7sw" trusted=true
#第一回はこちら
#https://www.kaggle.com/competitions/dsdojo_1/overview

# %% [markdown] id="cdc2NNOJiuhU"
# ---
# ## セットアップ
#
#

# %% colab={"base_uri": "https://localhost:8080/"} id="GZJyn-MciuhU" outputId="eb7079dd-b265-4b57-fda9-ff61cec466b9" trusted=true
# 必要モジュールでColab環境にないものはinstall
# !pip install japanize_matplotlib
# !pip install catboost

# %% id="km_jW_2YiuhU" trusted=true
# 必要モジュールをimport
import json
from datetime import datetime
from pathlib import Path

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib_venn import venn2
import seaborn as sns
import japanize_matplotlib
import networkx as nx
import lightgbm as lgb
import catboost as cb
from sklearn.model_selection import train_test_split, KFold, GroupKFold
from tqdm.auto import tqdm
import warnings
warnings.filterwarnings('ignore')

# ランダム性を伴う処理を行うため、結果の再現性を保つにはシード値を固定しておく必要があります
SEED = 42
np.random.seed(SEED)


# %% id="ah17yN7bu-Mu" trusted=true
# コンペの評価指標に合わせた目的関数/評価関数の定義
WEIGHTED_TARGET_THRESHOLD = 0.1
WEIGHTED_POSITIVE_WEIGHT = 5.0

def make_sample_weight(y_true):
    """
    ターゲットに応じた重みベクトルを生成
    """
    y_array = np.asarray(y_true, dtype=float)
    return np.where(y_array >= WEIGHTED_TARGET_THRESHOLD, WEIGHTED_POSITIVE_WEIGHT, 1.0)

def weighted_rmse(y_true, y_pred):
    """
    重み付きRMSE評価関数
    コンペの評価指標に合わせて実装
    """
    weights = make_sample_weight(y_true)
    squared_errors = (y_true - y_pred) ** 2
    weighted_squared_errors = weights * squared_errors
    pw_rmse = np.sqrt(np.mean(weighted_squared_errors) + 1e-9)
    return float(pw_rmse)

def weighted_rmse_feval(y_pred, dtrain):
    """
    LightGBM用の重み付きRMSE評価関数
    """
    y_true = dtrain.get_label()
    weighted_rmse_value = weighted_rmse(y_true, y_pred)
    return "weighted_rmse", weighted_rmse_value, False

# %% id="v0L9gXW5iuhU" trusted=true
# 表示できるdfの行、列数を増やす
pd.set_option("display.max_rows", 100)    # 最大100行まで表示
pd.set_option("display.max_columns", 100) # 最大100列まで表示

# %% [markdown] id="29WdPR8riuhV"
# ## データ読み込み

# %% colab={"base_uri": "https://localhost:8080/", "height": 804} id="e6eZzzhRiuhV" outputId="b494a902-ca3a-4cde-d9e9-52c00f7f2c38" trusted=true
# ローカル実行用のパス設定
base_path = '../../data'
print(f"データ読み込み元パス: {base_path}")

# データ読み込み
# player_idやmatch_idの数値的大小に意味はないのでstring形式で読み込み
train_df = pd.read_csv(f"{base_path}/match_train_data.csv", dtype={"player_id": "string", "match_id": "string"})
test_df = pd.read_csv(f"{base_path}/match_test_data.csv", dtype={"player_id": "string", "match_id": "string"})
actions_df = pd.read_csv(f"{base_path}/action_data.csv", dtype={"player_id": "string", "match_id": "string"})
submission_df = pd.read_csv(f"{base_path}/sample_submission.csv")

print(f"trainデータ形状: {train_df.shape}")
display(train_df.head(3))

print(f"\ntestデータ形状: {test_df.shape}")
display(test_df.head(3))

print(f"\nアクションデータ形状: {actions_df.shape}")
display(actions_df.head(3))


# %% [markdown] id="a4Pt7U5biuhV"
# ## 特徴量エンジニアリング - 基本特徴量
#
# まず、001と同じ基本的な特徴量を作成します。

# %% colab={"base_uri": "https://localhost:8080/"} id="F3d_UMBliuhV" outputId="3eec7699-ba5f-4497-855d-8c6c40c55b20" trusted=true
# 所与のデータから簡単に計算できる年齢特徴量を追加する

# 2017/18シーズン終了時点での年齢を計算
train_df['Date'] = pd.to_datetime(train_df['Date'])
train_df['birth_date'] = pd.to_datetime(train_df['birth_date'])
train_df['age'] = (train_df['Date'] - train_df['birth_date']).dt.days / 365.25

test_df['Date'] = pd.to_datetime(test_df['Date'])
test_df['birth_date'] = pd.to_datetime(test_df['birth_date'])
test_df['age'] = (test_df['Date'] - test_df['birth_date']).dt.days / 365.25

print(f"\nマージ後のtrainデータ形状: {train_df.shape}")
print(f"\nマージ後のtestデータ形状: {test_df.shape}")

# %% colab={"base_uri": "https://localhost:8080/"} id="r_urDy8riuhV" outputId="ef6080ba-2f4d-42ed-aea8-35cffd58e420" trusted=true
# アクションデータから試合×選手レベルの特徴量を作成

# train/testに含まれる試合×選手の組み合わせを作成する
target_match_players_train = train_df[['match_id', 'player_id']].drop_duplicates()
target_match_players_test = test_df[['match_id', 'player_id']].drop_duplicates()
target_match_players = pd.concat([target_match_players_train, target_match_players_test]).drop_duplicates()

print(f"分析対象となる試合×選手: {len(target_match_players)}組")

# アクションデータのうち、train/testデータに含まれる試合×選手のアクションのみを抽出
relevant_actions = actions_df.merge(
    target_match_players,
    on=['match_id', 'player_id'],
    how='inner'
)
print(f"抽出されたアクション数: {len(relevant_actions)}件")

# %% trusted=true
# 追加（早い段階に挿入）: 高度特徴量ユーティリティの読み込み
from pathlib import Path
import sys
import importlib

try:
    ROOT = Path.cwd().resolve().parents[1]
    if str(ROOT) not in sys.path:
        sys.path.append(str(ROOT))
except Exception:
    pass

# モジュールをリロード（修正を反映させる）
from scripts import advanced_features
importlib.reload(advanced_features)

from scripts.advanced_features import (
    build_nstep_chain_features,
    build_second_assist_sca_gca,
    build_pass_geometry_and_timing,
    build_xpass_risk_features,
    add_player_trend,
    # 🆕 新特徴量関数
    build_time_based_features,
    build_zone_based_features,
    build_pass_network_centrality,
    build_extended_chain_features,
    build_dynamic_positioning_features,
    # 🆕 受け手系拡張特徴量 (exp0035 Phase 3)
    build_box_receiver_extended_features,
)
print("advanced_features imported and reloaded (early cell).")


# %% colab={"base_uri": "https://localhost:8080/", "height": 395} id="hXudsW7vjuBf" outputId="5dcd2476-bef4-4b52-a970-22614a3b7fd9" trusted=true
# 位置データについては、homeとawayで基準が異なる
# homeの場合は、x=0が自陣ゴールライン、x=105が敵陣ゴールライン、y=0が右サイドライン、y=68が左サイドラインに対応する
# awayでは逆になるため、homeの選手とawayの選手で平均的なx,yの値を比較することができない
display(relevant_actions[(relevant_actions["type_name"] == "shot") & (relevant_actions["result_name"] == "success")][["is_home", "start_x", "start_y", "end_x", "end_y"]].head())

# そこで、位置を標準化するため、awayチームの場合は、x' = 105-x, y' = 68-yに修正する
relevant_actions.loc[relevant_actions['is_home'] == False, 'start_x'] = 105 - relevant_actions.loc[relevant_actions['is_home'] == False, 'start_x']
relevant_actions.loc[relevant_actions['is_home'] == False, 'end_x'] = 105 - relevant_actions.loc[relevant_actions['is_home'] == False, 'end_x']
relevant_actions.loc[relevant_actions['is_home'] == False, 'start_y'] = 68 - relevant_actions.loc[relevant_actions['is_home'] == False, 'start_y']
relevant_actions.loc[relevant_actions['is_home'] == False, 'end_y'] = 68 - relevant_actions.loc[relevant_actions['is_home'] == False, 'end_y']

relevant_actions[(relevant_actions["type_name"] == "shot") & (relevant_actions["result_name"] == "success")][["is_home", "start_x", "start_y", "end_x", "end_y"]].head()

# %% [markdown] id="sQOHu5iUnWLA"
# is_homeの値に関係なく、ゴールした場合end_x=105となっており、位置が標準化されている

# %% colab={"base_uri": "https://localhost:8080/", "height": 162} id="5LCBi5A3joBl" outputId="935dbfa8-dd0f-44c5-e3f9-c2e8d4b26b25" trusted=true
# 基本的な統計特徴量の作成
# groupby()とagg()を組み合わせることで、列ごとに任意の集計方法を指定できる。
match_player_stats = (
    relevant_actions
    .groupby(['match_id', 'player_id'])
    .agg(
        action_count   = ('type_name', 'size'), # アクション数合計
        avg_x          = ('start_x', 'mean'), # 平均ポジション（前後方向）
        avg_y          = ('start_y', 'mean'), # 平均ポジション（左右方向）
        minutes_played = ('minutes_played', 'first')  # 出場時間
    )
    .round(2)
    .reset_index()
)

print(f"作成したデータ形状: {match_player_stats.shape}")
display(match_player_stats.head(3))

# %% trusted=true
# 追加（relevant_actions直後）: 高度特徴量計算
print("Computing advanced features from relevant_actions (early) ...")

nstep_block = build_nstep_chain_features(
    relevant_actions,
    match_col="match_id",
    player_col="player_id",
    team_col="team_id",
    type_col="type_name",
    n_steps=3,
    gamma=0.7,
)

second_assist, sca1, sca2, gca1, gca2 = build_second_assist_sca_gca(
    relevant_actions,
    match_col="match_id",
    player_col="player_id",
    team_col="team_id",
    type_col="type_name",
    result_col="result_name",
)

pass_geom, pass_latency = build_pass_geometry_and_timing(
    relevant_actions,
    match_col="match_id",
    player_col="player_id",
    type_col="type_name",
)

xpass_risk = build_xpass_risk_features(
    relevant_actions,
    match_col="match_id",
    player_col="player_id",
    type_col="type_name",
    result_col="result_name",
)

print("Advanced feature blocks created (early).")


# %% trusted=true
# 🆕 新特徴量の計算 (EXP0025追加)
print("計算中: 新特徴量 (時間帯別/ゾーン別/ネットワーク/拡張連鎖/動的ポジショニング)...")

# 1. 時間帯別パフォーマンス
time_based_features = build_time_based_features(
    relevant_actions,
    match_col="match_id",
    player_col="player_id",
    time_col="time_seconds",
    period_col="period_id"
)

# 2. ゾーン別アクション密度
zone_based_features = build_zone_based_features(
    relevant_actions,
    match_col="match_id",
    player_col="player_id"
)

# 3. パスネットワーク中心性
network_centrality_features = build_pass_network_centrality(
    relevant_actions,
    match_col="match_id",
    player_col="player_id",
    team_col="team_id",
    type_col="type_name",
    time_col="time_seconds"
)

# 4. 拡張シーケンス連鎖 (7手先)
extended_chain_features = build_extended_chain_features(
    relevant_actions,
    match_col="match_id",
    player_col="player_id",
    team_col="team_id",
    type_col="type_name",
    n_steps=7,
    gamma=0.6
)

# 5. 動的ポジショニング
dynamic_positioning_features = build_dynamic_positioning_features(
    relevant_actions,
    match_col="match_id",
    player_col="player_id"
)

print("新特徴量ブロック作成完了")
print(f"  - 時間帯別: {len(time_based_features)}行")
print(f"  - ゾーン別: {len(zone_based_features)}行")
print(f"  - ネットワーク中心性: {len(network_centrality_features)}行")
print(f"  - 拡張連鎖: {len(extended_chain_features)}行")
print(f"  - 動的ポジショニング: {len(dynamic_positioning_features)}行")

# %% colab={"base_uri": "https://localhost:8080/", "height": 162} id="RAZHAVEanr4A" outputId="a989b03e-8c48-472d-9aaf-03b24fac09bf" trusted=true
# ゴール数の集計
# type_nameにshotが含まれて、成功したアクションはゴールになる
is_shot  = relevant_actions['type_name'].isin(['shot', 'shot_freekick', 'shot_penalty'])
is_success = relevant_actions['result_name'].eq('success')
is_goal = (is_shot & is_success).astype(int)

match_player_goals = (
    relevant_actions
    .assign(is_goal=is_goal) # is_goal列を追加
    .groupby(['match_id', 'player_id'], as_index=False)['is_goal']
    .sum() # ゴールであるアクションを合計
    .rename(columns={'is_goal': 'goal_count'})
)

print(f"作成したデータ形状: {match_player_goals.shape}")
display(match_player_goals.head(3))

# %% colab={"base_uri": "https://localhost:8080/", "height": 182} id="ZHhHF5_tiuhV" outputId="dfa2bb40-8395-43e4-c7c8-afe51f28a8dd" trusted=true
# アクションタイプ数の集計
# type_name列の値ごとに数を集計する
action_type_stats = (
    relevant_actions
    .groupby(['match_id', 'player_id', 'type_name'])
    .size()
    .unstack(fill_value=0)  # type_name を列に展開、欠損は0で埋める
    .rename_axis(None, axis=1)
    .add_prefix('type_').add_suffix('_count') # 列名に接頭辞と接尾辞を追加する（type_nameがshotなら「type_shot_count」になる）
    .reset_index()
)

print(f"作成したデータ形状: {action_type_stats.shape}")
display(action_type_stats.head(3))

# %% colab={"base_uri": "https://localhost:8080/"} id="oMpQ9RL1n1h8" outputId="e3a47fbf-85f2-44ee-d77c-2acb13ee8c48" trusted=true
# ベース特徴量をtrain/testへマージ
train_df = (
    train_df
    .merge(match_player_stats, on=['match_id', 'player_id'], how='left')
    .merge(match_player_goals, on=['match_id', 'player_id'], how='left')
    .merge(action_type_stats, on=['match_id', 'player_id'], how='left')
)


test_df = (
    test_df
    .merge(match_player_stats, on=['match_id', 'player_id'], how='left')
    .merge(match_player_goals, on=['match_id', 'player_id'], how='left')
    .merge(action_type_stats, on=['match_id', 'player_id'], how='left')
)


action_type_cols = [col for col in train_df.columns if col.startswith('type_')]
stats_count_cols = ['action_count', 'minutes_played', 'goal_count']

for col in action_type_cols + stats_count_cols:
    if col in train_df.columns:
        train_df[col] = train_df[col].fillna(0)
    if col in test_df.columns:
        test_df[col] = test_df[col].fillna(0)

print(f"ベース特徴量マージ後のtrainデータshape: {train_df.shape}")
print(f"ベース特徴量マージ後のtestデータshape: {test_df.shape}")


# %% [markdown] id="vGTCBG_BiuhV"
# ## 特徴量エンジニアリング - 応用特徴量
#
# ここから、より高度な特徴量を作成していきます。各特徴量の意図と計算方法を詳しく説明します。

# %% colab={"base_uri": "https://localhost:8080/", "height": 232, "referenced_widgets": ["09a43976bef243599b2e9b64cffb91b4", "d3e34a9bb0d24577acc83d7ae4b2486d", "dcffd72448ea44bf9ec8aec5b31762f6", "3a83f4956d7d41938ef54f146a5cc541", "69216dc0f0f4422e8166c4a8f8abe666", "79fcd266aa524e9b9d656b2eab1a8cd3", "09a7b0f32ef942afb2963f355e80a8bc", "dc043a4a5ed74b1c9b6cc8e027e5b2b6", "5e50717143ae4771b600c2a5f73488e2", "5b6ef11c003d4a79b3924bc396c0ff56", "78773c5f4ebc47fea4077d687ab0ea0c"]} id="maM2mFbTiuhV" outputId="6c028ca4-a3f4-41f8-b6f5-79d096d17f19" trusted=true
# アクション成功率特徴量
# アシストに繋がる可能性を評価するため、各種アクションの成功率を計算する

# 成功率を計算するアクションタイプ
action_types_with_result = ['pass', 'shot', 'take_on', 'cross', 'corner_crossed', 'freekick_crossed']  # take_onはドリブルでの仕掛け

success_rates_list = []
print("アクション成功率特徴量を計算中...")

for (match_id, player_id), group in tqdm(relevant_actions.groupby(['match_id', 'player_id']), desc="Calculating success rates"):
    row_data = {'match_id': match_id, 'player_id': player_id}

    for action_type in action_types_with_result:
        type_actions = group[group['type_name'] == action_type] # 対象アクションを抽出

        if len(type_actions) > 0:
            success_count = len(type_actions[type_actions['result_name'] == 'success'])
            total_count = len(type_actions)

            # 成功率を計算
            success_rate = success_count / total_count
            row_data[f'{action_type}_success_rate'] = success_rate
        else:
            # 該当アクションがない場合は0
            row_data[f'{action_type}_success_rate'] = 0

    success_rates_list.append(row_data)

success_rates = pd.DataFrame(success_rates_list)

print(f"作成したデータ形状: {success_rates.shape}")
display(success_rates.head(3))

# %% trusted=true
# 追加（ベース特徴量マージの直前）: 高度特徴量のマージ
def _merge_many(df, parts):
    for part in parts:
        if part is None or (hasattr(part, "empty") and part.empty):
            continue
        df = df.merge(part, on=["match_id", "player_id"], how="left")
    return df

train_df = _merge_many(
    train_df,
    [
        nstep_block,
        second_assist,
        sca1,
        sca2,
        gca1,
        gca2,
        pass_geom,
        pass_latency,
        xpass_risk,
        # 🆕 新特徴量
        time_based_features,
        zone_based_features,
        network_centrality_features,
        extended_chain_features,
        dynamic_positioning_features,
    ],
)

test_df = _merge_many(
    test_df,
    [
        nstep_block,
        second_assist,
        sca1,
        sca2,
        gca1,
        gca2,
        pass_geom,
        pass_latency,
        xpass_risk,
        # 🆕 新特徴量
        time_based_features,
        zone_based_features,
        network_centrality_features,
        extended_chain_features,
        dynamic_positioning_features,
    ],
)

# Fill NA and types（ここで欠損を潰す）
count_cols = ["second_assist_count", "SCA_1", "SCA_2", "GCA_1", "GCA_2"]
for col in count_cols:
    if col in train_df.columns:
        train_df[col] = train_df[col].fillna(0).astype(int)
        test_df[col] = test_df[col].fillna(0).astype(int)

num_cols = [
    "nstep_to_shot",
    "nstep_xt_delta",
    "pass_dist_mean",
    "pass_dist_max",
    "to_goal_angle_abs_mean",
    "to_goal_dist_mean",
    "pass_to_shot_latency_mean",
    "pass_to_shot_latency_min",
    "risk_creativity_sum",
    "xpass_mean",
    "xpass_min",
    "pass_success_minus_xpass",
    "xpass_deep_mean",
    "xpass_box_mean",
]
for col in num_cols:
    if col in train_df.columns:
        train_df[col] = train_df[col].fillna(0.0)
        test_df[col] = test_df[col].fillna(0.0)

# 🆕 新特徴量の欠損値処理
new_feature_cols = [
    # 時間帯別
    "first_half_actions", "second_half_actions", "final_15min_actions",
    "early_10min_actions", "time_weighted_intensity",
    # ゾーン別
    "defensive_zone_actions", "middle_zone_actions", "attacking_zone_actions",
    "halfspace_left_actions", "halfspace_right_actions", "central_corridor_actions",
    "final_third_penetrations", "box_entries",
    # ネットワーク中心性
    "betweenness_centrality", "closeness_centrality", "degree_centrality",
    "pass_receiver_diversity", "unique_pass_partners",
    # 拡張連鎖
    "longchain_to_shot", "longchain_xt_delta",
    # 動的ポジショニング
    "position_variance_x", "position_variance_y", "position_range_x",
    "position_range_y", "avg_action_distance",
]
for col in new_feature_cols:
    if col in train_df.columns:
        train_df[col] = train_df[col].fillna(0.0)
    if col in test_df.columns:
        test_df[col] = test_df[col].fillna(0.0)

print("Advanced features merged (early).")
print(f"🆕 新特徴量 {len(new_feature_cols)}個を追加しました")


# %% trusted=true
# 追加（all_features定義セルの直後を想定）: all_features に高度特徴量を含める
advanced_candidates = [
    "second_assist_count",
    "SCA_1",
    "SCA_2",
    "GCA_1",
    "GCA_2",
    "nstep_to_shot",
    "nstep_xt_delta",
    "pass_dist_mean",
    "pass_dist_max",
    "to_goal_angle_abs_mean",
    "to_goal_dist_mean",
    "pass_to_shot_latency_mean",
    "pass_to_shot_latency_min",
    "risk_creativity_sum",
    "xpass_mean",
    "xpass_min",
    "pass_success_minus_xpass",
    "xpass_deep_mean",
    "xpass_box_mean",
    # 時系列トレンド
    "xAG_expanding_mean",
    "xAG_rolling3_mean",
    "xAG_diff_prev",
    # 🆕 新特徴量
    "first_half_actions", "second_half_actions", "final_15min_actions",
    "early_10min_actions", "time_weighted_intensity",
    "defensive_zone_actions", "middle_zone_actions", "attacking_zone_actions",
    "halfspace_left_actions", "halfspace_right_actions", "central_corridor_actions",
    "final_third_penetrations", "box_entries",
    "betweenness_centrality", "closeness_centrality", "degree_centrality",
    "pass_receiver_diversity", "unique_pass_partners",
    "longchain_to_shot", "longchain_xt_delta",
    "position_variance_x", "position_variance_y", "position_range_x",
    "position_range_y", "avg_action_distance",
]
advanced_features = [c for c in advanced_candidates if c in train_df.columns]
try:
    all_features = list(dict.fromkeys(all_features + advanced_features))
except NameError:
    _advanced_features_pending = advanced_features
print(f"追加された高度特徴量: {len(advanced_features)}個 (🆕新特徴量含む)")


# %% colab={"base_uri": "https://localhost:8080/", "height": 310} id="yjC2SnrIiuhW" outputId="ac96bf5f-28cf-4e2f-a395-3c52f5b32736" trusted=true
# 位置ベース特徴量
# フィールド上での活動エリアを分析し、攻撃的な選手を識別

print("位置ベース特徴量を計算中...")

# フィールドを3つのエリアに分割（x座標ベース）
def categorize_position(x):
    """x座標からフィールドエリアを判定"""
    if x < 35:
        return 'defensive'  # 守備的エリア
    elif x < 75:
        return 'midfield'   # 中盤エリア
    else:
        return 'attacking'  # 攻撃的エリア

# 各アクションのエリアを判定
relevant_actions['start_zone'] = relevant_actions['start_x'].apply(categorize_position)

# ゾーン別アクション数を集計
zone_actions = (
    relevant_actions
    .pivot_table(
        index=['match_id', 'player_id'],
        columns='start_zone',
        values='period_id',
        aggfunc='count',
        fill_value=0
    )
    .add_prefix('zone_')
    .add_suffix('_actions')
    .reset_index()
)

# 各ゾーンでのアクション比率を計算
zone_actions['total_actions'] = (
    zone_actions.get('zone_defensive_actions', 0) +
    zone_actions.get('zone_midfield_actions', 0) +
    zone_actions.get('zone_attacking_actions', 0)
)

zone_actions['zone_attacking_actions_ratio'] = np.where(
    zone_actions['total_actions'] > 0,
    zone_actions.get('zone_attacking_actions', 0) / zone_actions['total_actions'],
    0
)

zone_actions['zone_midfield_actions_ratio'] = np.where(
    zone_actions['total_actions'] > 0,
    zone_actions.get('zone_midfield_actions', 0) / zone_actions['total_actions'],
    0
)

zone_actions['zone_defensive_actions_ratio'] = np.where(
    zone_actions['total_actions'] > 0,
    zone_actions.get('zone_defensive_actions', 0) / zone_actions['total_actions'],
    0
)
zone_actions = zone_actions.drop(columns=['total_actions'])

print(f"\nゾーン別アクション統計:")
for zone in ['defensive', 'midfield', 'attacking']:
    col_name = f'zone_{zone}_actions'
    if col_name in zone_actions.columns:
        mean_val = zone_actions[col_name].mean()
        print(f"  {zone:10s}エリア: 平均 {mean_val:.1f} アクション")

print(f"\n作成したデータ形状: {zone_actions.shape}")
display(zone_actions.head(3))

# %% colab={"base_uri": "https://localhost:8080/", "height": 219} id="W2m7WIjGiuhW" outputId="87b1e9c5-034e-4850-89e4-65e627031806" trusted=true
# 時間正規化特徴量
# 出場時間による影響を排除し、公平な比較を可能にする

print("時間正規化特徴量を計算中...")

per_minute_features = match_player_stats.copy()

# 全体アクション数の正規化
per_minute_features['action_count_per_minute'] = np.where(
    per_minute_features['minutes_played'] > 0,
    per_minute_features['action_count'] / per_minute_features['minutes_played'],
    0
)

# ゴール数をマージ・ゼロ埋め
per_minute_features = per_minute_features.merge(
    match_player_goals,
    on=['match_id', 'player_id'],
    how='left'
)
per_minute_features['goal_count'] = per_minute_features['goal_count'].fillna(0)

# ゴール数の正規化
per_minute_features['goal_count_per_minute'] = np.where(
    per_minute_features['minutes_played'] > 0,
    per_minute_features['goal_count'] / per_minute_features['minutes_played'],
    0
)

# アクションタイプ別アクション数をマージ・ゼロ埋め
per_minute_features = per_minute_features.merge(
    action_type_stats,
    on=['match_id', 'player_id'],
    how='left'
)
action_type_cols = [col for col in per_minute_features.columns if col.startswith('type_') and col.endswith('_count')] # アクションタイプ別アクション数の列
for col in action_type_cols:
    per_minute_features[col] = per_minute_features[col].fillna(0)

# アクションタイプ別アクション数の正規化
for col in action_type_cols:
    new_col_name = col.replace('_count', '_count_per_minute')
    per_minute_features[new_col_name] = np.where(
        per_minute_features['minutes_played'] > 0,
        per_minute_features[col] / per_minute_features['minutes_played'],
        0
    )

# 新規作成した列のみに絞り込み
per_minute_cols = [col for col in per_minute_features.columns if col.endswith('_per_minute')]
per_minute_features = per_minute_features[['match_id', 'player_id'] + per_minute_cols]

print(f"\n作成したデータ形状: {per_minute_features.shape}")
display(per_minute_features.head(3))

# %% colab={"base_uri": "https://localhost:8080/", "height": 199} id="5m47SqPMiuhW" outputId="5ceb8fa8-cc5a-478a-bb4b-7be3075b35f1" trusted=true
# 攻撃/守備バランス特徴量
# 選手のプレースタイルを定量化し、攻撃的な選手を識別

print("攻撃/守備バランス特徴量を計算中...")

# 攻撃/守備アクションの定義
offensive_actions = ['shot', 'pass', 'cross', 'take_on', 'dribble']
defensive_actions = ['tackle', 'interception', 'clearance']

# 各アクションの分類を付与
def categorize_ad(action):
    if action in offensive_actions:
        return 'offensive'
    elif action in defensive_actions:
        return 'defensive'
    else:
        return None

relevant_actions['action_type'] = relevant_actions['type_name'].apply(categorize_ad)

# 攻守別アクション数を集計
offense_defense_balance = (
    relevant_actions
    .pivot_table(
        index=['match_id', 'player_id'],
        columns='action_type',
        values='period_id',
        aggfunc='count',
        fill_value=0
    )
    .add_prefix('type_')
    .add_suffix('_actions')
    .reset_index()
)

# 攻守バランス指標を計算
offense_defense_balance['total_actions'] = (
    offense_defense_balance.get('type_offensive_actions', 0) +
    offense_defense_balance.get('type_defensive_actions', 0)
)

offense_defense_balance['type_offensive_action_ratio'] = np.where(
    offense_defense_balance['total_actions'] > 0,
    offense_defense_balance['type_offensive_actions'] / offense_defense_balance['total_actions'],
    0
)

offense_defense_balance = offense_defense_balance.drop(columns=['total_actions'])

print(f"\n作成したデータ形状: {offense_defense_balance.shape}")
display(offense_defense_balance.head(3))

# %% colab={"base_uri": "https://localhost:8080/", "height": 199} id="sLKXukn5pIUx" outputId="3c3f0531-8bd0-4e66-9c7e-dacfa3a0b86a" trusted=true
# 時系列要素を加味した特徴量
# xAGの定義を考えると、パスした味方のシュートが多いほどxAGは高くなる
# そこで、次アクションがシュートであるパスの数を選手-試合ごとに集計する

print("次アクションがシュートのパス数を計算中...")

# 直後のアクションタイプをシフトで付与
relevant_actions = relevant_actions.sort_values(['match_id', 'period_id', 'time_seconds'])  # 念の為アクションを時間でソート
relevant_actions["next_type"] = relevant_actions.groupby("match_id")["type_name"].shift(-1)

# pass → shot となっている行を抽出
pass_to_shot = relevant_actions[
    (relevant_actions["type_name"] == "pass") &
    (relevant_actions["next_type"] == "shot")
]

# match_id, player_idごとにカウント
pass_leads_to_shot = (
    pass_to_shot.groupby(["match_id", "player_id"])
    .size()
    .reset_index(name="pass_leads_to_shot")
)

print(f"\n作成したデータ形状: {pass_leads_to_shot.shape}")
display(pass_leads_to_shot.head(3))

# %% trusted=true
# プログレッシブ/ディープ系の特徴量
print("プログレッシブ/ディープ系特徴量を計算中...")

PASS_PROGRESSIVE_TYPES = {"pass", "cross", "freekick_crossed", "corner_crossed"}
CARRY_PROGRESSIVE_TYPES = {"carry", "dribble", "take_on"}

progressive_pass_actions = relevant_actions[
    relevant_actions["type_name"].isin(PASS_PROGRESSIVE_TYPES)
].copy()

if not progressive_pass_actions.empty:
    dx = (progressive_pass_actions["end_x"] - progressive_pass_actions["start_x"]).fillna(0.0)
    dy = (progressive_pass_actions["end_y"] - progressive_pass_actions["start_y"]).fillna(0.0)
else:
    dx = pd.Series(dtype=float)
    dy = pd.Series(dtype=float)

progressive_pass_actions["delta_x"] = dx
progressive_pass_actions["delta_total"] = np.hypot(dx, dy)
progressive_pass_actions["is_completed"] = progressive_pass_actions["result_name"] == "success"

FINAL_THIRD_X = 70.0
DEEP_COMPLETION_X = 85.0
PENALTY_AREA_X = 88.0
PROGRESS_ADVANCE_MIN = 10.0

progressive_pass_actions["is_progressive"] = (
    (progressive_pass_actions["delta_x"] >= PROGRESS_ADVANCE_MIN)
    | (
        (progressive_pass_actions["start_x"] < FINAL_THIRD_X)
        & (progressive_pass_actions["end_x"] >= FINAL_THIRD_X)
    )
    | (progressive_pass_actions["end_x"] >= DEEP_COMPLETION_X)
)

progressive_pass_actions["progressive_attempt"] = progressive_pass_actions["is_progressive"].astype(int)
progressive_pass_actions["progressive_success"] = (
    progressive_pass_actions["is_progressive"] & progressive_pass_actions["is_completed"]
).astype(int)
progressive_pass_actions["progressive_distance"] = np.where(
    progressive_pass_actions["is_progressive"],
    progressive_pass_actions["delta_total"],
    0.0,
)

progressive_pass_actions["is_final_third_entry"] = (
    progressive_pass_actions["is_completed"]
    & (progressive_pass_actions["start_x"] < FINAL_THIRD_X)
    & (progressive_pass_actions["end_x"] >= FINAL_THIRD_X)
)
progressive_pass_actions["is_deep_completion"] = (
    progressive_pass_actions["is_completed"]
    & (progressive_pass_actions["end_x"] >= DEEP_COMPLETION_X)
)
progressive_pass_actions["is_penalty_area_entry"] = (
    progressive_pass_actions["is_completed"]
    & (progressive_pass_actions["end_x"] >= PENALTY_AREA_X)
)

pass_progressive_features = (
    progressive_pass_actions.groupby(["match_id", "player_id"]).agg(
        progressive_pass_count=("progressive_attempt", "sum"),
        progressive_pass_success=("progressive_success", "sum"),
        progressive_pass_distance_total=("progressive_distance", "sum"),
        final_third_entry_count=("is_final_third_entry", "sum"),
        deep_completion_count=("is_deep_completion", "sum"),
        penalty_area_entry_count=("is_penalty_area_entry", "sum"),
    )
    .reset_index()
)

if "progressive_pass_count" in pass_progressive_features:
    pass_progressive_features["progressive_pass_success_rate"] = np.where(
        pass_progressive_features["progressive_pass_count"] > 0,
        pass_progressive_features["progressive_pass_success"]
        / pass_progressive_features["progressive_pass_count"],
        0.0,
    )
    pass_progressive_features["progressive_pass_distance_mean"] = np.where(
        pass_progressive_features["progressive_pass_count"] > 0,
        pass_progressive_features["progressive_pass_distance_total"]
        / pass_progressive_features["progressive_pass_count"],
        0.0,
    )
else:
    pass_progressive_features["progressive_pass_success_rate"] = []
    pass_progressive_features["progressive_pass_distance_mean"] = []

carry_actions = relevant_actions[
    relevant_actions["type_name"].isin(CARRY_PROGRESSIVE_TYPES)
].copy()

if not carry_actions.empty:
    carry_actions["end_x"] = carry_actions["end_x"].fillna(carry_actions["start_x"])
    carry_actions["end_y"] = carry_actions["end_y"].fillna(carry_actions["start_y"])
    carry_dx = (carry_actions["end_x"] - carry_actions["start_x"]).fillna(0.0)
    carry_dy = (carry_actions["end_y"] - carry_actions["start_y"]).fillna(0.0)
    carry_actions["delta_total"] = np.hypot(carry_dx, carry_dy)
    carry_actions["delta_x"] = carry_dx
    carry_actions["is_success"] = carry_actions["result_name"] == "success"
    carry_actions["is_progressive"] = (
        (carry_actions["delta_x"] >= 5.0)
        | (
            (carry_actions["start_x"] < FINAL_THIRD_X)
            & (carry_actions["end_x"] >= FINAL_THIRD_X)
        )
    )
    carry_actions["progressive_carry_attempt"] = carry_actions["is_progressive"].astype(int)
    carry_actions["progressive_carry_success"] = (
        carry_actions["is_progressive"] & carry_actions["is_success"]
    ).astype(int)
    carry_actions["progressive_carry_distance"] = np.where(
        carry_actions["is_progressive"], carry_actions["delta_total"], 0.0
    )

    carry_progressive_features = (
        carry_actions.groupby(["match_id", "player_id"]).agg(
            progressive_carry_count=("progressive_carry_attempt", "sum"),
            progressive_carry_success=("progressive_carry_success", "sum"),
            progressive_carry_distance_total=("progressive_carry_distance", "sum"),
        )
        .reset_index()
    )

    carry_progressive_features["progressive_carry_success_rate"] = np.where(
        carry_progressive_features["progressive_carry_count"] > 0,
        carry_progressive_features["progressive_carry_success"]
        / carry_progressive_features["progressive_carry_count"],
        0.0,
    )
    carry_progressive_features["progressive_carry_distance_mean"] = np.where(
        carry_progressive_features["progressive_carry_count"] > 0,
        carry_progressive_features["progressive_carry_distance_total"]
        / carry_progressive_features["progressive_carry_count"],
        0.0,
    )
else:
    carry_progressive_features = pd.DataFrame(
        columns=[
            "match_id",
            "player_id",
            "progressive_carry_count",
            "progressive_carry_success",
            "progressive_carry_distance_total",
            "progressive_carry_success_rate",
            "progressive_carry_distance_mean",
        ]
    )

progressive_features = pass_progressive_features.merge(
    carry_progressive_features,
    on=["match_id", "player_id"],
    how="outer",
).fillna(0.0)

print(f"作成したプログレッシブ系特徴量: {progressive_features.shape}")
display(progressive_features.head(3))


# %% trusted=true
# 学習型 xT (Expected Threat) 特徴量
print("学習型xT (value iteration) 特徴量を計算中...")

# グリッド定義 (16x12)
GRID_X_EDGES = np.linspace(0, 105, 17)
GRID_Y_EDGES = np.linspace(0, 68, 13)
NUM_X = len(GRID_X_EDGES) - 1
NUM_Y = len(GRID_Y_EDGES) - 1
NUM_ZONES = NUM_X * NUM_Y

def map_to_zone(x_array: np.ndarray, y_array: np.ndarray) -> np.ndarray:
    """Map coordinates to xT grid zone indices (0-191)."""
    x_idx = np.clip(np.digitize(x_array, GRID_X_EDGES) - 1, 0, NUM_X - 1)
    y_idx = np.clip(np.digitize(y_array, GRID_Y_EDGES) - 1, 0, NUM_Y - 1)
    return (y_idx * NUM_X + x_idx).astype(int)

# 利用するアクション種別
distribution_actions = {
    "pass", "cross", "throw_in", "corner_crossed", "freekick_crossed",
    "carry", "take_on", "dribble", "goal_kick", "clearance"
}
shot_actions = {"shot", "shot_penalty", "shot_freekick"}

# 学習用xTはtrainのアクションのみを使用（リーク防止）
train_match_ids_xt = set(train_df["match_id"])
train_actions = relevant_actions[relevant_actions["match_id"].isin(train_match_ids_xt)].copy()

# 座標欠損をゼロ埋めしてゾーン算出用に準備（trainのみ）
start_x_train = train_actions["start_x"].fillna(0).to_numpy()
start_y_train = train_actions["start_y"].fillna(0).to_numpy()
start_zones_train = map_to_zone(start_x_train, start_y_train)

transition_counts = np.zeros((NUM_ZONES, NUM_ZONES), dtype=np.float64)
shot_counts = np.zeros(NUM_ZONES, dtype=np.float64)
goal_counts = np.zeros(NUM_ZONES, dtype=np.float64)
ball_loss_counts = np.zeros(NUM_ZONES, dtype=np.float64)

# ショット関連統計（trainのみ）
shot_mask_train = train_actions["type_name"].isin(shot_actions)
if shot_mask_train.any():
    shot_zones_train = start_zones_train[shot_mask_train.to_numpy()]
    shot_counts += np.bincount(shot_zones_train, minlength=NUM_ZONES)
    goal_flags_train = train_actions.loc[shot_mask_train, "result_name"].eq("success").to_numpy(dtype=np.float64)
    goal_counts += np.bincount(shot_zones_train, weights=goal_flags_train, minlength=NUM_ZONES)

# パス・キャリー等のポゼッション遷移統計（trainのみ）
move_mask_train = train_actions["type_name"].isin(distribution_actions)
if move_mask_train.any():
    move_actions_train = train_actions.loc[move_mask_train].copy()
    move_start_zones_train = map_to_zone(
        move_actions_train["start_x"].fillna(0).to_numpy(),
        move_actions_train["start_y"].fillna(0).to_numpy(),
    )
    move_success_train = move_actions_train["result_name"].eq("success").to_numpy()

    if (~move_success_train).any():
        ball_loss_counts += np.bincount(move_start_zones_train[~move_success_train], minlength=NUM_ZONES)

    valid_success_idx_train = move_success_train & move_actions_train["end_x"].notna().to_numpy() & move_actions_train["end_y"].notna().to_numpy()
    if valid_success_idx_train.any():
        success_start_zones_train = move_start_zones_train[valid_success_idx_train]
        success_end_zones_train = map_to_zone(
            move_actions_train.loc[valid_success_idx_train, "end_x"].to_numpy(),
            move_actions_train.loc[valid_success_idx_train, "end_y"].to_numpy(),
        )
        np.add.at(transition_counts, (success_start_zones_train, success_end_zones_train), 1.0)

# xT価値反復（train統計で推定）
transition_totals = transition_counts.sum(axis=1)
total_counts = transition_totals + shot_counts + ball_loss_counts
safe_totals = np.where(total_counts == 0, 1.0, total_counts)

transition_probs = np.divide(
    transition_counts,
    safe_totals[:, None],
    out=np.zeros_like(transition_counts),
    where=safe_totals[:, None] > 0,
)
shot_prob = shot_counts / safe_totals
goal_given_shot = np.divide(
    goal_counts,
    shot_counts,
    out=np.zeros_like(goal_counts),
    where=shot_counts > 0,
)
immediate_reward = shot_prob * goal_given_shot

gamma = 0.95
xt_values = immediate_reward.copy()
max_iterations = 500
for iteration in range(max_iterations):
    updated = immediate_reward + gamma * transition_probs.dot(xt_values)
    max_delta = np.max(np.abs(updated - xt_values))
    xt_values = updated
    if max_delta < 1e-6:
        break
else:
    iteration += 1  # 収束しなかった場合のインジケータ

print(f"学習型xT value iteration: {iteration + 1} step(s), max_delta={max_delta:.2e}")
print(f"xT値の範囲: min={xt_values.min():.5f}, max={xt_values.max():.5f}")

# アクションベースのxT特徴量付与（trainで学習したxt_valuesを全行へ適用）
# 全行（train+test）の開始ゾーンを算出
start_x_all = relevant_actions["start_x"].fillna(0).to_numpy()
start_y_all = relevant_actions["start_y"].fillna(0).to_numpy()
start_zones_all = map_to_zone(start_x_all, start_y_all)

end_x = relevant_actions["end_x"].to_numpy()
end_y = relevant_actions["end_y"].to_numpy()
has_end_coords = np.isfinite(end_x) & np.isfinite(end_y)
end_zones_all = np.zeros(len(relevant_actions), dtype=int)
if has_end_coords.any():
    end_zones_all[has_end_coords] = map_to_zone(end_x[has_end_coords], end_y[has_end_coords])

start_values = xt_values[start_zones_all]
end_values = np.zeros(len(relevant_actions), dtype=np.float64)
end_values[has_end_coords] = xt_values[end_zones_all[has_end_coords]]

success_flag = relevant_actions["result_name"].eq("success").astype(int).to_numpy()
end_values_on_success = np.where(success_flag == 1, end_values, 0.0)

relevant_actions["xt_learned_start"] = start_values
relevant_actions["xt_learned_end"] = end_values
relevant_actions["xt_learned_delta"] = end_values_on_success - start_values
relevant_actions["xt_learned_positive_delta"] = np.clip(relevant_actions["xt_learned_delta"], 0.0, None)
relevant_actions["xt_learned_success"] = success_flag
relevant_actions["xt_learned_end_on_success"] = np.where(success_flag == 1, end_values, np.nan)
relevant_actions["xt_learned_delta_on_success"] = np.where(success_flag == 1, relevant_actions["xt_learned_delta"], np.nan)

xt_learned_features = (
    relevant_actions.groupby(["match_id", "player_id"])
    .agg(
        xt_learned_start_mean=("xt_learned_start", "mean"),
        xt_learned_start_max=("xt_learned_start", "max"),
        xt_learned_delta_sum=("xt_learned_delta", "sum"),
        xt_learned_delta_mean=("xt_learned_delta", "mean"),
        xt_learned_positive_delta_sum=("xt_learned_positive_delta", "sum"),
        xt_learned_positive_delta_mean=("xt_learned_positive_delta", "mean"),
        xt_learned_success_rate=("xt_learned_success", "mean"),
        xt_learned_action_count=("xt_learned_success", "count"),
        xt_learned_end_success_mean=("xt_learned_end_on_success", "mean"),
        xt_learned_delta_success_mean=("xt_learned_delta_on_success", "mean"),
    )
    .reset_index()
)

xt_learned_feature_cols = [col for col in xt_learned_features.columns if col not in {"match_id", "player_id"}]

train_df = train_df.merge(xt_learned_features, on=["match_id", "player_id"], how="left")
test_df = test_df.merge(xt_learned_features, on=["match_id", "player_id"], how="left")

train_df[xt_learned_feature_cols] = train_df[xt_learned_feature_cols].fillna(0.0)
test_df[xt_learned_feature_cols] = test_df[xt_learned_feature_cols].fillna(0.0)

train_df["xt_learned_action_count"] = train_df["xt_learned_action_count"].astype(int)
test_df["xt_learned_action_count"] = test_df["xt_learned_action_count"].astype(int)

print("学習型xT特徴量（サンプル）:")
display(xt_learned_features.head(3))



# %% [markdown] id="hHxhgz7ypDWn"
# ## Possession-Level Progression Features
#
# Learned xT highlights forward threat, so we aggregate possession speed and directness as complementary signals.
#

# %% trusted=true
print("Calculating possession progression features...")

pos_actions = (
    relevant_actions
    .reset_index(drop=False)
    .sort_values(["match_id", "period_id", "time_seconds", "index"], kind="mergesort")
    .rename(columns={"index": "action_index"})
    .copy()
)
pos_actions["time_seconds"] = pos_actions["time_seconds"].fillna(0.0)
pos_actions["team_id"] = pos_actions["team_id"].fillna("unknown_team")
pos_actions["new_match"] = pos_actions["match_id"].ne(pos_actions["match_id"].shift())
pos_actions["same_team_prev"] = pos_actions["team_id"].eq(pos_actions["team_id"].shift())
pos_actions["prev_success"] = pos_actions["result_name"].shift().eq("success")
pos_actions["time_diff"] = pos_actions.groupby("match_id")["time_seconds"].diff().fillna(0.0)
pos_actions["new_possession"] = (
    pos_actions["new_match"]
    | (~pos_actions["same_team_prev"].fillna(False))
    | (~pos_actions["prev_success"].fillna(True))
    | (pos_actions["time_diff"] > 15.0)
)
pos_actions.loc[pos_actions.index[0], "new_possession"] = True
pos_actions["possession_id"] = pos_actions["new_possession"].cumsum().astype(int)
pos_actions["possession_event_index"] = pos_actions.groupby("possession_id").cumcount() + 1

pos_group = pos_actions.groupby("possession_id").agg(
    match_id=("match_id", "first"),
    team_id=("team_id", "first"),
    start_x=("start_x", "first"),
    start_y=("start_y", "first"),
    end_x=("end_x", "last"),
    end_y=("end_y", "last"),
    start_time=("time_seconds", "first"),
    end_time=("time_seconds", "last"),
    action_count=("player_id", "count"),
    unique_players=("player_id", "nunique"),
    xt_positive_sum=("xt_learned_positive_delta", "sum"),
    xt_delta_sum=("xt_learned_delta", "sum"),
).reset_index()

pos_group["duration"] = (pos_group["end_time"] - pos_group["start_time"]).clip(lower=1.0)
pos_group["delta_x"] = pos_group["end_x"] - pos_group["start_x"]
pos_group["delta_y"] = pos_group["end_y"] - pos_group["start_y"]
pos_group["ground_distance"] = np.sqrt(np.square(pos_group["delta_x"]) + np.square(pos_group["delta_y"]))
pos_group["directness"] = np.divide(
    pos_group["delta_x"],
    pos_group["ground_distance"],
    out=np.zeros_like(pos_group["delta_x"]),
    where=pos_group["ground_distance"] > 0,
)
pos_group["speed_x"] = pos_group["delta_x"] / pos_group["duration"]
pos_group["speed_ground"] = pos_group["ground_distance"] / pos_group["duration"]
pos_group["xt_positive_per_second"] = np.divide(
    pos_group["xt_positive_sum"],
    pos_group["duration"],
    out=np.zeros_like(pos_group["xt_positive_sum"]),
    where=pos_group["duration"] > 0,
)

final_third_threshold = 70.0
final_third_steps = (
    pos_actions[pos_actions["end_x"].ge(final_third_threshold)]
    .groupby("possession_id")["possession_event_index"]
    .min()
)
final_third_times = (
    pos_actions[pos_actions["end_x"].ge(final_third_threshold)]
    .groupby("possession_id")["time_seconds"]
    .min()
)
pos_group["final_third_entry_step"] = pos_group["possession_id"].map(final_third_steps)
pos_group["final_third_entry_flag"] = pos_group["final_third_entry_step"].notna().astype(float)
pos_group["final_third_entry_time"] = pos_group["possession_id"].map(final_third_times)
pos_group["time_to_final_third"] = (
    pos_group["final_third_entry_time"] - pos_group["start_time"]
).where(pos_group["final_third_entry_flag"] > 0)

pos_player = (
    pos_actions[["match_id", "player_id", "possession_id"]]
    .drop_duplicates()
    .merge(
        pos_group[[
            "possession_id",
            "duration",
            "ground_distance",
            "directness",
            "speed_x",
            "speed_ground",
            "xt_positive_per_second",
            "xt_positive_sum",
            "xt_delta_sum",
            "action_count",
            "final_third_entry_flag",
            "final_third_entry_step",
            "time_to_final_third",
        ]],
        on="possession_id",
        how="left",
    )
)

player_pos_features = pos_player.groupby(["match_id", "player_id"]).agg(
    possession_count=("possession_id", "nunique"),
    possession_duration_mean=("duration", "mean"),
    possession_ground_distance_mean=("ground_distance", "mean"),
    possession_directness_mean=("directness", "mean"),
    possession_speed_x_mean=("speed_x", "mean"),
    possession_speed_ground_mean=("speed_ground", "mean"),
    possession_xt_positive_per_second_mean=("xt_positive_per_second", "mean"),
    possession_xt_positive_sum=("xt_positive_sum", "sum"),
    possession_xt_delta_sum=("xt_delta_sum", "sum"),
    possession_actions_per_pos_mean=("action_count", "mean"),
    possession_final_third_rate=("final_third_entry_flag", "mean"),
    possession_final_third_step_mean=("final_third_entry_step", "mean"),
    possession_time_to_final_third_mean=("time_to_final_third", "mean"),
).reset_index()

possession_feature_cols = [
    col for col in player_pos_features.columns if col not in {"match_id", "player_id"}
]
numeric_possession_cols = [col for col in possession_feature_cols if col != "possession_count"]

train_df = train_df.merge(player_pos_features, on=["match_id", "player_id"], how="left")
test_df = test_df.merge(player_pos_features, on=["match_id", "player_id"], how="left")

train_df["possession_count"] = train_df["possession_count"].fillna(0).astype(int)
test_df["possession_count"] = test_df["possession_count"].fillna(0).astype(int)

if numeric_possession_cols:
    train_df[numeric_possession_cols] = train_df[numeric_possession_cols].fillna(0.0)
    test_df[numeric_possession_cols] = test_df[numeric_possession_cols].fillna(0.0)

print("Possession progression features added:", len(possession_feature_cols))


# %% [markdown]
# ## Pass Network Features
#
# Network-centric statistics to capture player roles within possession flow.
#

# %% trusted=true
print("Calculating pass network features...")

sorted_actions = (
    relevant_actions
    .reset_index(drop=False)
    .sort_values(["match_id", "period_id", "time_seconds", "index"], kind="mergesort")
    .rename(columns={"index": "action_index"})
    .copy()
)

sorted_actions["next_player_id"] = sorted_actions.groupby("match_id")["player_id"].shift(-1)
sorted_actions["next_team_id"] = sorted_actions.groupby("match_id")["team_id"].shift(-1)

success_pass_mask = (
    (sorted_actions["type_name"] == "pass")
    & sorted_actions["result_name"].eq("success")
    & sorted_actions["next_team_id"].notna()
    & sorted_actions["next_team_id"].eq(sorted_actions["team_id"])
)

pass_edges = sorted_actions.loc[success_pass_mask, [
    "match_id",
    "team_id",
    "player_id",
    "next_player_id",
    "start_x",
    "start_y",
    "end_x",
    "end_y"
]].copy()

pass_edges = pass_edges.dropna(subset=["player_id", "next_player_id"])
pass_edges["player_id"] = pass_edges["player_id"].astype(str)
pass_edges["next_player_id"] = pass_edges["next_player_id"].astype(str)

pass_edges["pass_distance"] = np.sqrt(
    (pass_edges["end_x"] - pass_edges["start_x"]) ** 2 +
    (pass_edges["end_y"] - pass_edges["start_y"]) ** 2
)
pass_edges["lateral_shift"] = pass_edges["end_y"] - pass_edges["start_y"]
pass_edges["switch_flag"] = pass_edges["lateral_shift"].abs() >= 20.0

player_edge_stats = (
    pass_edges.groupby(["match_id", "player_id"])
    .agg(
        pass_net_attempts=("next_player_id", "count"),
        pass_net_avg_distance=("pass_distance", "mean"),
        pass_net_switch_rate=("switch_flag", "mean"),
    )
    .reset_index()
)

receiver_stats = (
    pass_edges.groupby(["match_id", "next_player_id"])
    .size()
    .reset_index(name="pass_net_receive_count")
    .rename(columns={"next_player_id": "player_id"})
)

centrality_records = []
for (match_id, team_id), group in pass_edges.groupby(["match_id", "team_id"]):
    players = set(group["player_id"]) | set(group["next_player_id"])
    if not players:
        continue

    G = nx.DiGraph()
    for src, tgt in group[["player_id", "next_player_id"]].itertuples(index=False):
        if G.has_edge(src, tgt):
            G[src][tgt]["weight"] += 1.0
        else:
            G.add_edge(src, tgt, weight=1.0)

    for node in players:
        if node not in G:
            G.add_node(node)

    out_degree = dict(G.out_degree(weight="weight"))
    in_degree = dict(G.in_degree(weight="weight"))
    try:
        betweenness = nx.betweenness_centrality(G, weight="weight", normalized=True)
    except Exception:
        betweenness = {node: 0.0 for node in players}

    undirected = G.to_undirected()
    if undirected.number_of_edges() > 0:
        clustering = nx.clustering(undirected, weight="weight")
    else:
        clustering = {node: 0.0 for node in players}

    for node in players:
        centrality_records.append({
            "match_id": match_id,
            "player_id": node,
            "pass_net_out_degree": out_degree.get(node, 0.0),
            "pass_net_in_degree": in_degree.get(node, 0.0),
            "pass_net_betweenness": betweenness.get(node, 0.0),
            "pass_net_clustering": clustering.get(node, 0.0),
        })

centrality_df = pd.DataFrame(centrality_records)
if centrality_df.empty:
    centrality_df = pd.DataFrame(columns=[
        "match_id",
        "player_id",
        "pass_net_out_degree",
        "pass_net_in_degree",
        "pass_net_betweenness",
        "pass_net_clustering",
    ])

if player_edge_stats.empty:
    player_edge_stats = pd.DataFrame(columns=[
        "match_id",
        "player_id",
        "pass_net_attempts",
        "pass_net_avg_distance",
        "pass_net_switch_rate",
    ])

if receiver_stats.empty:
    receiver_stats = pd.DataFrame(columns=[
        "match_id",
        "player_id",
        "pass_net_receive_count",
    ])

pass_network_features = (
    centrality_df
    .merge(player_edge_stats, on=["match_id", "player_id"], how="outer")
    .merge(receiver_stats, on=["match_id", "player_id"], how="outer")
)

if not pass_network_features.empty:
    numeric_cols = [
        col for col in pass_network_features.columns
        if col not in {"match_id", "player_id"}
    ]
    pass_network_features[numeric_cols] = pass_network_features[numeric_cols].fillna(0.0)

pass_network_feature_cols = [
    col for col in pass_network_features.columns if col not in {"match_id", "player_id"}
]

train_df = train_df.merge(pass_network_features, on=["match_id", "player_id"], how="left")
test_df = test_df.merge(pass_network_features, on=["match_id", "player_id"], how="left")

if pass_network_feature_cols:
    train_df[pass_network_feature_cols] = train_df[pass_network_feature_cols].fillna(0.0)
    test_df[pass_network_feature_cols] = test_df[pass_network_feature_cols].fillna(0.0)

print("Pass network features added:", len(pass_network_feature_cols))


# %% [markdown]
#
# ## 行為タイプ別 eΔxT 特徴量
#
# 学習済みxTに基づく空間価値と行為タイプ別の成功確率モデルを組み合わせ、リスク調整された期待xT増分 (eΔxT) を算出します。
#

# %% trusted=true

print("行為タイプ別 xPass モデルを構築しています...")

xpass_action_groups = {
    "pass": ["pass"],
    "cross": ["cross"],
    "carry": ["carry"],
    "dribble": ["dribble", "take_on"],
    "free_kick": ["freekick_crossed"],
    "corner": ["corner_crossed"],
}

train_match_ids = set(train_df["match_id"])

if "action_index" not in relevant_actions.columns:
    relevant_actions = relevant_actions.copy()
    relevant_actions["action_index"] = np.arange(len(relevant_actions))

if "xpass_prob" not in relevant_actions.columns:
    relevant_actions["xpass_prob"] = np.nan
if "xpass_action_group" not in relevant_actions.columns:
    relevant_actions["xpass_action_group"] = pd.NA

relevant_actions["is_train_action"] = relevant_actions["match_id"].isin(train_match_ids)

relevant_actions["end_x_filled"] = relevant_actions["end_x"].fillna(relevant_actions["start_x"])
relevant_actions["end_y_filled"] = relevant_actions["end_y"].fillna(relevant_actions["start_y"])
relevant_actions["delta_x"] = (relevant_actions["end_x_filled"] - relevant_actions["start_x"]).fillna(0.0)
relevant_actions["delta_y"] = (relevant_actions["end_y_filled"] - relevant_actions["start_y"]).fillna(0.0)
relevant_actions["distance"] = np.hypot(relevant_actions["delta_x"], relevant_actions["delta_y"])
relevant_actions["abs_delta_y"] = relevant_actions["delta_y"].abs()

for col in ["xt_learned_start", "xt_learned_delta", "xt_learned_positive_delta"]:
    if col in relevant_actions.columns:
        relevant_actions[col] = relevant_actions[col].fillna(0.0)

xpass_predictions = []
xpass_training_summary = []
xpass_calibration_records = []

xpass_numeric_candidates = [
    "start_x",
    "start_y",
    "end_x_filled",
    "end_y_filled",
    "delta_x",
    "delta_y",
    "distance",
    "abs_delta_y",
    "time_seconds",
    "minutes_played",
    "period_id",
    "is_home",
    "xt_learned_start",
    "xt_learned_delta",
    "xt_learned_positive_delta",
]

xpass_categorical_candidates = [
    "team_name_short",
    "bodypart_name",
    "competition",
    "match_venue",
]

for action_group, action_names in xpass_action_groups.items():
    subset_idx = relevant_actions["type_name"].isin(action_names)
    action_subset = relevant_actions.loc[subset_idx].copy()

    if action_subset.empty:
        continue

    action_subset["is_success"] = action_subset["result_name"].eq("success").astype(int)
    action_subset["is_home"] = action_subset["is_home"].fillna(False).astype(int)

    numeric_candidates_local = list(xpass_numeric_candidates)
    if "is_starter" in action_subset.columns:
        action_subset["is_starter"] = action_subset["is_starter"].fillna(False).astype(int)
        numeric_candidates_local.append("is_starter")

    if "minutes_played" in action_subset.columns:
        action_subset["minutes_played"] = action_subset["minutes_played"].fillna(0.0)

    categorical_features = [col for col in xpass_categorical_candidates if col in action_subset.columns]
    for col in categorical_features:
        action_subset[col] = action_subset[col].fillna("missing").astype("category")

    numeric_features = [col for col in numeric_candidates_local if col in action_subset.columns]
    used_features = numeric_features + categorical_features

    train_subset = action_subset[action_subset["is_train_action"]].copy()
    test_subset = action_subset[~action_subset["is_train_action"]].copy()

    if train_subset.empty or train_subset["match_id"].nunique() < 2:
        fallback = float(train_subset["is_success"].mean()) if len(train_subset) else 0.5
        fallback = float(np.clip(fallback, 1e-4, 1 - 1e-4))
        action_subset.loc[train_subset.index, "xpass_prob"] = fallback
        action_subset.loc[test_subset.index, "xpass_prob"] = fallback
        action_subset["xpass_action_group"] = action_group
        relevant_actions.loc[action_subset.index, "xpass_prob"] = action_subset["xpass_prob"]
        relevant_actions.loc[action_subset.index, "xpass_action_group"] = action_subset["xpass_action_group"]
        xpass_predictions.append(
            action_subset[
                [
                    "match_id",
                    "player_id",
                    "action_index",
                    "xpass_action_group",
                    "xpass_prob",
                    "xt_learned_delta",
                    "xt_learned_start",
                    "is_train_action",
                ]
            ]
        )
        xpass_training_summary.append(
            {
                "action_type": action_group,
                "train_actions": len(train_subset),
                "test_actions": len(test_subset),
                "success_rate": float(train_subset["is_success"].mean()) if len(train_subset) else np.nan,
            }
        )
        continue

    n_splits = min(5, max(2, train_subset["match_id"].nunique()))
    gkf = GroupKFold(n_splits=n_splits)

    params = {
        "objective": "binary",
        "metric": "binary_logloss",
        "learning_rate": 0.03,
        "num_leaves": 25,
        "feature_fraction": 0.7,
        "bagging_fraction": 0.7,
        "bagging_freq": 1,
        "min_data_in_leaf": 64,
        "min_gain_to_split": 0.01,
        "lambda_l1": 0.1,
        "lambda_l2": 0.1,
        "seed": SEED,
        "verbose": -1,
    }

    oof_preds = np.zeros(len(train_subset), dtype=float)
    test_preds = np.zeros(len(test_subset), dtype=float) if len(test_subset) else None
    models = []

    for fold, (tr_idx, val_idx) in enumerate(gkf.split(train_subset, groups=train_subset["match_id"])):
        X_tr = train_subset.iloc[tr_idx][used_features]
        y_tr = train_subset.iloc[tr_idx]["is_success"]
        X_val = train_subset.iloc[val_idx][used_features]
        y_val = train_subset.iloc[val_idx]["is_success"]

        train_ds = lgb.Dataset(
            X_tr,
            label=y_tr,
            categorical_feature=categorical_features or None,
            free_raw_data=False,
        )
        val_ds = lgb.Dataset(
            X_val,
            label=y_val,
            reference=train_ds,
            categorical_feature=categorical_features or None,
            free_raw_data=False,
        )

        model = lgb.train(
            params,
            train_ds,
            valid_sets=[val_ds],
            num_boost_round=800,
            callbacks=[lgb.early_stopping(80), lgb.log_evaluation(0)],
        )
        models.append(model)
        fold_pred = model.predict(X_val, num_iteration=model.best_iteration)
        oof_preds[val_idx] = fold_pred
        if len(test_subset):
            test_preds += model.predict(test_subset[used_features], num_iteration=model.best_iteration)

    if len(test_subset):
        test_preds = test_preds / len(models)

    # Plattスケーリングによるキャリブレーション（fold-aware to prevent leakage）
    if len(np.unique(train_subset["is_success"])) > 1:
        try:
            from sklearn.linear_model import LogisticRegression

            # Fold-aware calibration for OOF predictions to prevent data leakage
            calibrated_oof_preds = np.zeros(len(train_subset), dtype=float)
            
            for fold_idx, (tr_idx, val_idx) in enumerate(gkf.split(train_subset, groups=train_subset["match_id"])):
                # Fit calibration model on TRAINING fold only (exclude validation fold)
                calib_model = LogisticRegression(max_iter=1000, random_state=42)
                calib_model.fit(
                    oof_preds[tr_idx].reshape(-1, 1),
                    train_subset.iloc[tr_idx]["is_success"].to_numpy()
                )
                
                # Apply calibration to VALIDATION fold
                calibrated_oof_preds[val_idx] = calib_model.predict_proba(
                    oof_preds[val_idx].reshape(-1, 1)
                )[:, 1]
            
            oof_preds = calibrated_oof_preds
            
            # For test predictions: use ALL training data (this is correct)
            if test_preds is not None:
                final_calib = LogisticRegression(max_iter=1000, random_state=42)
                final_calib.fit(oof_preds.reshape(-1, 1), train_subset["is_success"].to_numpy())
                test_preds = final_calib.predict_proba(test_preds.reshape(-1, 1))[:, 1]
                
        except Exception as exc:
            print(f"Calibration failed for {action_group}: {exc}")

    oof_preds = np.clip(oof_preds, 1e-4, 1 - 1e-4)
    if len(test_subset):
        test_preds = np.clip(test_preds, 1e-4, 1 - 1e-4)

    train_mean = float(train_subset["is_success"].mean()) if len(train_subset) else 0.5
    fallback = float(np.clip(train_mean, 1e-4, 1 - 1e-4))

    action_subset.loc[train_subset.index, "xpass_prob"] = oof_preds
    if len(test_subset):
        action_subset.loc[test_subset.index, "xpass_prob"] = test_preds
    else:
        action_subset.loc[test_subset.index, "xpass_prob"] = fallback

    action_subset["xpass_action_group"] = action_group

    relevant_actions.loc[action_subset.index, "xpass_prob"] = action_subset["xpass_prob"]
    relevant_actions.loc[action_subset.index, "xpass_action_group"] = action_subset["xpass_action_group"]

    xpass_predictions.append(
        action_subset[
            [
                "match_id",
                "player_id",
                "action_index",
                "xpass_action_group",
                "xpass_prob",
                "xt_learned_delta",
                "xt_learned_start",
                "is_train_action",
            ]
        ]
    )

    xpass_training_summary.append(
        {
            "action_type": action_group,
            "train_actions": len(train_subset),
            "test_actions": len(test_subset),
            "success_rate": float(train_subset["is_success"].mean()) if len(train_subset) else np.nan,
        }
    )

    cal_df = train_subset[["is_success"]].copy()
    cal_df["pred"] = oof_preds
    cal_df["action_type"] = action_group
    try:
        unique_pred = np.unique(np.round(cal_df["pred"], 6))
        n_bins = min(10, max(4, len(unique_pred)))
        cal_df["bucket"] = pd.qcut(cal_df["pred"], q=n_bins, duplicates="drop")
        agg = cal_df.groupby(["action_type", "bucket"], observed=True).agg(
            pred_mean=("pred", "mean"),
            success_rate=("is_success", "mean"),
            count=("is_success", "size"),
        ).reset_index()
        xpass_calibration_records.append(agg)
    except ValueError:
        pass

if xpass_predictions:
    xpass_predictions_df = pd.concat(xpass_predictions, ignore_index=True)
else:
    xpass_predictions_df = pd.DataFrame(
        columns=[
            "match_id",
            "player_id",
            "action_index",
            "xpass_action_group",
            "xpass_prob",
            "xt_learned_delta",
            "xt_learned_start",
            "is_train_action",
        ]
    )

xpass_training_summary_df = pd.DataFrame(xpass_training_summary)
if not xpass_training_summary_df.empty:
    display(xpass_training_summary_df.sort_values("action_type"))

if xpass_calibration_records:
    calibration_df = pd.concat(xpass_calibration_records, ignore_index=True)
    display(calibration_df)



# %% colab={"base_uri": "https://localhost:8080/", "height": 977} id="nzaeWhMpbDlr" outputId="f99d35a4-fc96-4bcc-f176-50c7235075ef" trusted=true
# [exp0027] 5分割のStratifiedGroupKFoldを設定（match_idでグループ化 + 正例率で層化）
# scripts/cv.py の関数を使用
from scripts.cv import make_stratified_group_folds

train_df["fold"] = make_stratified_group_folds(
    train_df, 
    y_col='xAG', 
    threshold=0.1, 
    n_splits=5, 
    n_bins=5, 
    seed=42
)

# 関数は0-indexedのfoldを返すので、1-indexedに変換（既存コードとの互換性のため）
train_df["fold"] = train_df["fold"] + 1

# 結果を可視化 (通常のGroupKFoldと同じ形式)
gkf = GroupKFold(n_splits=5)

# xAG軸のスケールは共通化して見やすくする
x_min, x_max = train_df["xAG"].min(), train_df["xAG"].max()
xAG_vals = np.arange(x_min, x_max + 0.1, 0.1).round(1)

# 図: 各foldごとに 3カラム（Train分布, Val分布, match_idベン図）
fig, axes = plt.subplots(nrows=5, ncols=3, figsize=(18, 18), sharey=False, sharex=False)

for fold_num in range(1, 6):
    i = fold_num - 1
    val_mask = train_df["fold"] == fold_num
    trn_mask = train_df["fold"] != fold_num
    
    # fold列は既にセット済み (make_stratified_group_foldsで設定)

    # train/val の xAG 分布を取得（共通スケールにリインデックス）
    trn_counts = (
        train_df[trn_mask]["xAG"].value_counts().sort_index()
        .reindex(xAG_vals, fill_value=0)
    )
    val_counts = (
        train_df[val_mask]["xAG"].value_counts().sort_index()
        .reindex(xAG_vals, fill_value=0)
    )

    # 左列: 各foldのtrainデータ分布
    ax_train = axes[i, 0]
    ax_train.bar(trn_counts.index, trn_counts.values, width=0.08, color="steelblue")
    ax_train.set_title(f"Fold {i+1} - Train xAG 分布")
    ax_train.set_xlabel("xAG")
    ax_train.set_ylabel("頻度")

    # 中列: 各foldのvalidationデータ分布
    ax_val = axes[i, 1]
    ax_val.bar(val_counts.index, val_counts.values, width=0.08, color="orange")
    ax_val.set_title(f"Fold {i+1} - Val xAG 分布")
    ax_val.set_xlabel("xAG")
    ax_val.set_ylabel("頻度")

    # 右列: match_idのベン図（Train vs Val）
    ax_venn = axes[i, 2]
    trn_match_ids = set(train_df[trn_mask]["match_id"])
    val_match_ids = set(train_df[val_mask]["match_id"])
    v = venn2(
        [trn_match_ids, val_match_ids],
        set_labels=(f"Train match_id (n={len(trn_match_ids)})",
                    f"Val match_id (n={len(val_match_ids)})"),
        ax=ax_venn
    )
    ax_venn.set_title(f"Fold {i+1} - match_id の重なり")

# x軸を共通スケールに揃える（分布図の2カラムに適用）
for i in range(5):
    for j in [0, 1]:
        ax = axes[i, j]
        ax.set_xlim(x_min - 0.05, x_max + 0.05)  # 端を少し余裕持たせる
        ax.set_xticks(xAG_vals[::2])  # ラベルの数を間引き

plt.tight_layout()
plt.show()

# %% trusted=true
# [exp0027] 層化CVの品質検証: 各foldの正例率を確認
print("=" * 80)
print("StratifiedGroupKFold 品質検証")
print("=" * 80)

fold_stats = []
for fold_num in range(1, 6):
    val_mask = train_df["fold"] == fold_num
    trn_mask = train_df["fold"] != fold_num
    
    # バリデーション統計
    val_df = train_df[val_mask]
    val_pos_rate = (val_df["xAG"] >= 0.1).mean()
    val_size = len(val_df)
    val_matches = val_df["match_id"].nunique()
    val_mean_xag = val_df["xAG"].mean()
    
    # トレーニング統計
    trn_df = train_df[trn_mask]
    trn_pos_rate = (trn_df["xAG"] >= 0.1).mean()
    trn_size = len(trn_df)
    trn_matches = trn_df["match_id"].nunique()
    trn_mean_xag = trn_df["xAG"].mean()
    
    fold_stats.append({
        'Fold': fold_num,
        'Val_PositiveRate': val_pos_rate,
        'Train_PositiveRate': trn_pos_rate,
        'Val_Size': val_size,
        'Train_Size': trn_size,
        'Val_Matches': val_matches,
        'Train_Matches': trn_matches,
        'Val_Mean_xAG': val_mean_xag,
        'Train_Mean_xAG': trn_mean_xag
    })
    
    print(f"\n【Fold {fold_num}】")
    print(f"  Validation: {val_size:>6} samples, {val_matches:>4} matches, "
          f"正例率={val_pos_rate:.4f}, 平均xAG={val_mean_xag:.4f}")
    print(f"  Training:   {trn_size:>6} samples, {trn_matches:>4} matches, "
          f"正例率={trn_pos_rate:.4f}, 平均xAG={trn_mean_xag:.4f}")

# 統計サマリ
fold_stats_df = pd.DataFrame(fold_stats)
print("\n" + "=" * 80)
print("サマリー統計")
print("=" * 80)
print(f"正例率の標準偏差 (Validation): {fold_stats_df['Val_PositiveRate'].std():.6f}")
print(f"正例率の標準偏差 (Training):   {fold_stats_df['Train_PositiveRate'].std():.6f}")
print(f"平均xAGの標準偏差 (Validation): {fold_stats_df['Val_Mean_xAG'].std():.6f}")
print(f"平均xAGの標準偏差 (Training):   {fold_stats_df['Train_Mean_xAG'].std():.6f}")

# 表として表示
print("\n詳細統計テーブル:")
display(fold_stats_df)


# %% trusted=true
print("eΔxTのλ最適化と特徴量集約を実行中...")

import optuna
from optuna.samplers import TPESampler

edxt_feature_cols = []

if xpass_predictions_df.empty:
    print("xPassの対象アクションが存在しないため、eΔxT特徴量は追加されません。")
else:
    xpass_predictions_df = xpass_predictions_df.copy()
    xpass_predictions_df["xt_learned_delta"] = xpass_predictions_df["xt_learned_delta"].fillna(0.0)
    xpass_predictions_df["xt_learned_start"] = xpass_predictions_df["xt_learned_start"].fillna(0.0)
    xpass_predictions_df["success_component"] = xpass_predictions_df["xpass_prob"] * xpass_predictions_df["xt_learned_delta"]
    xpass_predictions_df["fail_component_raw"] = (1.0 - xpass_predictions_df["xpass_prob"]) * xpass_predictions_df["xt_learned_start"]
    xpass_predictions_df["fail_weight"] = (1.0 - xpass_predictions_df["xpass_prob"]).fillna(0.0)
    train_actions = xpass_predictions_df[xpass_predictions_df["is_train_action"]].copy()
    global_start_mean = float(train_actions["xt_learned_start"].mean()) if not train_actions.empty else 0.0
    global_start_std = float(train_actions["xt_learned_start"].std(ddof=0)) if not train_actions.empty else 0.0
    if global_start_std < 1e-6:
        global_start_std = 1.0
    global_start_median = float(train_actions["xt_learned_start"].median()) if not train_actions.empty else 0.0
    global_start_mad = float((train_actions["xt_learned_start"] - global_start_median).abs().median()) if not train_actions.empty else 0.0
    if global_start_mad < 1e-6:
        global_start_mad = 1.0
    # ============================================================
    # FIX ISSUE 1: Fold-aware normalization to prevent data leakage
    # Compute group statistics from OTHER folds only for each validation fold
    # ============================================================
    # Initialize columns for fold-aware statistics
    xpass_predictions_df["fail_group_mean"] = global_start_mean
    xpass_predictions_df["fail_group_std"] = global_start_std
    xpass_predictions_df["fail_group_median"] = global_start_median
    xpass_predictions_df["fail_group_mad"] = global_start_mad
    # For training data: use fold-aware statistics (exclude current fold)
    train_actions_with_fold = train_actions.merge(
        train_df[["match_id", "player_id", "fold"]],
        on=["match_id", "player_id"],
        how="left"
    )
    for fold in train_actions_with_fold["fold"].dropna().unique():
        # Get actions from OTHER folds (not current fold)
        other_folds_actions = train_actions_with_fold[train_actions_with_fold["fold"] != fold]
        if other_folds_actions.empty:
            continue
        # Compute statistics from other folds only
        fold_group_mean = other_folds_actions.groupby("xpass_action_group")["xt_learned_start"].mean()
        fold_group_std = other_folds_actions.groupby("xpass_action_group")["xt_learned_start"].std(ddof=0)
        fold_group_median = other_folds_actions.groupby("xpass_action_group")["xt_learned_start"].median()
        fold_group_mad = other_folds_actions.groupby("xpass_action_group")["xt_learned_start"].apply(
            lambda s: (s - s.median()).abs().median()
        )
        # Apply to current fold's validation data
        # Create the mask properly
        fold_match_player = train_df[train_df["fold"] == fold][["match_id", "player_id"]].copy()
        fold_match_player["_in_fold"] = True
        xpass_with_fold = xpass_predictions_df.merge(
            fold_match_player,
            on=["match_id", "player_id"],
            how="left"
        )
        current_fold_mask = (xpass_with_fold["is_train_action"]) & (xpass_with_fold["_in_fold"] == True)
        if current_fold_mask.sum() > 0:
            xpass_predictions_df.loc[current_fold_mask, "fail_group_mean"] = (
                xpass_predictions_df.loc[current_fold_mask, "xpass_action_group"]
                .map(fold_group_mean)
                .fillna(global_start_mean)
            )
            xpass_predictions_df.loc[current_fold_mask, "fail_group_std"] = (
                xpass_predictions_df.loc[current_fold_mask, "xpass_action_group"]
                .map(fold_group_std)
                .fillna(global_start_std)
            )
            xpass_predictions_df.loc[current_fold_mask, "fail_group_median"] = (
                xpass_predictions_df.loc[current_fold_mask, "xpass_action_group"]
                .map(fold_group_median)
                .fillna(global_start_median)
            )
            xpass_predictions_df.loc[current_fold_mask, "fail_group_mad"] = (
                xpass_predictions_df.loc[current_fold_mask, "xpass_action_group"]
                .map(fold_group_mad)
                .fillna(global_start_mad)
            )
    # For test data: use statistics from ALL training data
    test_mask = ~xpass_predictions_df["is_train_action"]
    if test_mask.sum() > 0:
        group_mean_all = train_actions.groupby("xpass_action_group")["xt_learned_start"].mean()
        group_std_all = train_actions.groupby("xpass_action_group")["xt_learned_start"].std(ddof=0)
        group_median_all = train_actions.groupby("xpass_action_group")["xt_learned_start"].median()
        group_mad_all = train_actions.groupby("xpass_action_group")["xt_learned_start"].apply(
            lambda s: (s - s.median()).abs().median()
        )
        xpass_predictions_df.loc[test_mask, "fail_group_mean"] = (
            xpass_predictions_df.loc[test_mask, "xpass_action_group"]
            .map(group_mean_all)
            .fillna(global_start_mean)
        )
        xpass_predictions_df.loc[test_mask, "fail_group_std"] = (
            xpass_predictions_df.loc[test_mask, "xpass_action_group"]
            .map(group_std_all)
            .fillna(global_start_std)
        )
        xpass_predictions_df.loc[test_mask, "fail_group_median"] = (
            xpass_predictions_df.loc[test_mask, "xpass_action_group"]
            .map(group_median_all)
            .fillna(global_start_median)
        )
        xpass_predictions_df.loc[test_mask, "fail_group_mad"] = (
            xpass_predictions_df.loc[test_mask, "xpass_action_group"]
            .map(group_mad_all)
            .fillna(global_start_mad)
        )
    # Ensure minimum std and mad to avoid division by zero
    xpass_predictions_df["fail_group_std"] = xpass_predictions_df["fail_group_std"].where(
        xpass_predictions_df["fail_group_std"] > 1e-6, global_start_std
    )
    xpass_predictions_df["fail_group_mad"] = xpass_predictions_df["fail_group_mad"].where(
        xpass_predictions_df["fail_group_mad"] > 1e-6, global_start_mad
    )
    # Validation: Ensure no NaN values remain
    assert xpass_predictions_df["fail_group_mean"].isna().sum() == 0, "fail_group_mean has NaN values"
    assert xpass_predictions_df["fail_group_std"].isna().sum() == 0, "fail_group_std has NaN values"
    assert xpass_predictions_df["fail_group_median"].isna().sum() == 0, "fail_group_median has NaN values"
    assert xpass_predictions_df["fail_group_mad"].isna().sum() == 0, "fail_group_mad has NaN values"
    print(f"✓ Fold-aware normalization applied: {len(train_actions_with_fold['fold'].dropna().unique())} folds processed")
    pass_train = train_actions[train_actions["xpass_action_group"] == "pass"]
    if not pass_train.empty:
        pass_reference = np.sort(pass_train["xt_learned_start"].to_numpy())
    else:
        pass_reference = None
    def _quantile_center(values: np.ndarray, reference: np.ndarray) -> np.ndarray:
        if reference is None or len(reference) == 0:
            return np.zeros_like(values, dtype=float)
        ranks = np.searchsorted(reference, values, side="left")
        quantiles = (ranks + 0.5) / len(reference)
        quantiles = np.clip(quantiles, 1e-6, 1 - 1e-6)
        return quantiles - 0.5
    xpass_predictions_df["fail_component_scaled"] = 0.0
    pass_mask = xpass_predictions_df["xpass_action_group"] == "pass"
    if pass_mask.any():
        if pass_reference is not None and len(pass_reference) > 10:
            centered = _quantile_center(xpass_predictions_df.loc[pass_mask, "xt_learned_start"].to_numpy(), pass_reference)
        else:
            centered = (
                (xpass_predictions_df.loc[pass_mask, "xt_learned_start"] - xpass_predictions_df.loc[pass_mask, "fail_group_median"]) / xpass_predictions_df.loc[pass_mask, "fail_group_mad"]
            ).to_numpy()
        xpass_predictions_df.loc[pass_mask, "fail_component_scaled"] = xpass_predictions_df.loc[pass_mask, "fail_weight"].to_numpy() * centered
    non_pass_mask = ~pass_mask
    if non_pass_mask.any():
        centered = (
            xpass_predictions_df.loc[non_pass_mask, "xt_learned_start"].to_numpy() - xpass_predictions_df.loc[non_pass_mask, "fail_group_mean"].to_numpy()
        ) / xpass_predictions_df.loc[non_pass_mask, "fail_group_std"].to_numpy()
        xpass_predictions_df.loc[non_pass_mask, "fail_component_scaled"] = xpass_predictions_df.loc[non_pass_mask, "fail_weight"].to_numpy() * centered
    train_actions = xpass_predictions_df[xpass_predictions_df["is_train_action"]].copy()
    train_meta = train_df[["match_id", "player_id", "fold", "xAG"]].copy()
    fold_labels = sorted(train_df["fold"].unique())
    def _weighted_rmse_local(y_true, y_pred):
        weights = make_sample_weight(y_true)
        return float(np.sqrt(np.mean(weights * (y_true - y_pred) ** 2) + 1e-9))
    aggregated_components = (
        train_actions.groupby(["match_id", "player_id", "xpass_action_group"])
        .agg(
            success_sum=("success_component", "sum"),
            fail_sum_scaled=("fail_component_scaled", "sum"),
            fail_sum_raw=("fail_component_raw", "sum"),
            fail_weight_sum=("fail_weight", "sum"),
            action_count=("success_component", "count"),
        )
        .reset_index()
    )
    lambda_per_type = {}
    lambda_meta = []
    lambda_bounds = {
        "pass": (-0.3, 0.6),
        "cross": (-0.6, 2.0),
        "dribble": (-0.6, 2.0),
        "carry": (-0.6, 2.0),
        "corner": (-1.5, 3.0),
        "free_kick": (-1.5, 3.0),
    }
    for action_group in sorted(xpass_predictions_df["xpass_action_group"].dropna().unique()):
        type_df = aggregated_components[aggregated_components["xpass_action_group"] == action_group].copy()
        type_df = type_df.merge(train_meta, on=["match_id", "player_id"], how="left")
        type_df = type_df.dropna(subset=["xAG", "fold"])
        if type_df.empty:
            lambda_per_type[action_group] = 0.0
            continue
        lam_low, lam_high = lambda_bounds.get(action_group, (-1.0, 2.0))
        def objective(trial):
            lam = trial.suggest_float("lambda", lam_low, lam_high)
            preds = type_df["success_sum"] - lam * type_df["fail_sum_scaled"]
            scores = []
            for fold in fold_labels:
                fold_mask = type_df["fold"] == fold
                if not fold_mask.any():
                    continue
                scores.append(
                    _weighted_rmse_local(
                        type_df.loc[fold_mask, "xAG"].to_numpy(),
                        preds.loc[fold_mask].to_numpy(),
                    )
                )
            return float(np.mean(scores)) if scores else 1.0
        study = optuna.create_study(direction="minimize", sampler=TPESampler(seed=SEED))
        study.optimize(objective, n_trials=35, show_progress_bar=False)
        best_lambda = float(study.best_params["lambda"])
        lambda_per_type[action_group] = best_lambda
        lambda_meta.append(
            {
                "action_type": action_group,
                "lambda": best_lambda,
                "lambda_min": lam_low,
                "lambda_max": lam_high,
                "train_rows": int(type_df.shape[0]),
                "actions_per_player_mean": float(type_df["action_count"].mean()),
                "fail_scaled_mean": float(type_df["fail_sum_scaled"].mean()),
            }
        )
    lambda_meta_df = pd.DataFrame(lambda_meta)
    if not lambda_meta_df.empty:
        display(lambda_meta_df.sort_values("lambda"))
        print("λ分布統計:")
        display(lambda_meta_df["lambda"].describe())
    for action_group, lam_value in lambda_per_type.items():
        type_actions = xpass_predictions_df[xpass_predictions_df["xpass_action_group"] == action_group].copy()
        if type_actions.empty:
            continue
        type_actions["edxt_value_scaled"] = type_actions["success_component"] - lam_value * type_actions["fail_component_scaled"]
        type_actions["edxt_positive_scaled"] = np.clip(type_actions["edxt_value_scaled"], 0.0, None)
        type_actions["edxt_value"] = type_actions["success_component"] - lam_value * type_actions["fail_component_raw"]
        type_actions["edxt_positive"] = np.clip(type_actions["edxt_value"], 0.0, None)
        agg_train = (
            type_actions[type_actions["is_train_action"]]
            .groupby(["match_id", "player_id"])
            .agg(
                edxt_sum=("edxt_value", "sum"),
                edxt_mean=("edxt_value", "mean"),
                edxt_max=("edxt_value", "max"),
                edxt_positive_sum=("edxt_positive", "sum"),
                edxt_positive_mean=("edxt_positive", "mean"),
                edxt_scaled_sum=("edxt_value_scaled", "sum"),
                edxt_scaled_mean=("edxt_value_scaled", "mean"),
                edxt_scaled_max=("edxt_value_scaled", "max"),
                edxt_scaled_positive_sum=("edxt_positive_scaled", "sum"),
                edxt_scaled_positive_mean=("edxt_positive_scaled", "mean"),
                edxt_count=("edxt_value", "count"),
                success_sum=("success_component", "sum"),
                fail_sum_raw=("fail_component_raw", "sum"),
                fail_sum_scaled=("fail_component_scaled", "sum"),
                fail_weight_sum=("fail_weight", "sum"),
            )
            .reset_index()
        )
        agg_test = (
            type_actions[~type_actions["is_train_action"]]
            .groupby(["match_id", "player_id"])
            .agg(
                edxt_sum=("edxt_value", "sum"),
                edxt_mean=("edxt_value", "mean"),
                edxt_max=("edxt_value", "max"),
                edxt_positive_sum=("edxt_positive", "sum"),
                edxt_positive_mean=("edxt_positive", "mean"),
                edxt_scaled_sum=("edxt_value_scaled", "sum"),
                edxt_scaled_mean=("edxt_value_scaled", "mean"),
                edxt_scaled_max=("edxt_value_scaled", "max"),
                edxt_scaled_positive_sum=("edxt_positive_scaled", "sum"),
                edxt_scaled_positive_mean=("edxt_positive_scaled", "mean"),
                edxt_count=("edxt_value", "count"),
                success_sum=("success_component", "sum"),
                fail_sum_raw=("fail_component_raw", "sum"),
                fail_sum_scaled=("fail_component_scaled", "sum"),
                fail_weight_sum=("fail_weight", "sum"),
            )
            .reset_index()
        )
        col_map = {
            "edxt_sum": f"{action_group}_edxt_sum",
            "edxt_mean": f"{action_group}_edxt_mean",
            "edxt_max": f"{action_group}_edxt_max",
            "edxt_positive_sum": f"{action_group}_edxt_positive_sum",
            "edxt_positive_mean": f"{action_group}_edxt_positive_mean",
            "edxt_scaled_sum": f"{action_group}_edxt_scaled_sum",
            "edxt_scaled_mean": f"{action_group}_edxt_scaled_mean",
            "edxt_scaled_max": f"{action_group}_edxt_scaled_max",
            "edxt_scaled_positive_sum": f"{action_group}_edxt_scaled_positive_sum",
            "edxt_scaled_positive_mean": f"{action_group}_edxt_scaled_positive_mean",
            "edxt_count": f"{action_group}_edxt_count",
            "success_sum": f"{action_group}_edxt_success_sum",
            "fail_sum_raw": f"{action_group}_edxt_fail_sum",
            "fail_sum_scaled": f"{action_group}_edxt_fail_scaled_sum",
            "fail_weight_sum": f"{action_group}_fail_weight_sum",
        }
        train_df = train_df.merge(agg_train.rename(columns=col_map), on=["match_id", "player_id"], how="left")
        test_df = test_df.merge(agg_test.rename(columns=col_map), on=["match_id", "player_id"], how="left")
        for new_col in col_map.values():
            if new_col not in train_df.columns:
                train_df[new_col] = 0.0
            if new_col not in test_df.columns:
                test_df[new_col] = 0.0
            train_df[new_col] = train_df[new_col].fillna(0.0)
            test_df[new_col] = test_df[new_col].fillna(0.0)
        edxt_feature_cols.extend(col_map.values())
    openplay_groups = {"pass", "cross", "dribble", "carry"}
    setpiece_groups = {"corner", "free_kick"}
    def _sum_columns(df, cols, new_col):
        found = [col for col in cols if col in df.columns]
        if found:
            df[new_col] = df[found].sum(axis=1)
        else:
            df[new_col] = 0.0
    setpiece_scaled_cols = [f"{g}_edxt_scaled_positive_sum" for g in setpiece_groups]
    openplay_scaled_cols = [f"{g}_edxt_scaled_positive_sum" for g in openplay_groups]
    setpiece_raw_cols = [f"{g}_edxt_positive_sum" for g in setpiece_groups]
    openplay_raw_cols = [f"{g}_edxt_positive_sum" for g in openplay_groups]
    for df in (train_df, test_df):
        _sum_columns(df, setpiece_scaled_cols, "setpiece_edxt_scaled_positive_sum")
        _sum_columns(df, openplay_scaled_cols, "openplay_edxt_scaled_positive_sum")
        _sum_columns(df, setpiece_raw_cols, "setpiece_edxt_positive_sum")
        _sum_columns(df, openplay_raw_cols, "openplay_edxt_positive_sum")
        scaled_denom = df["setpiece_edxt_scaled_positive_sum"] + df["openplay_edxt_scaled_positive_sum"]
        raw_denom = df["setpiece_edxt_positive_sum"] + df["openplay_edxt_positive_sum"]
        df["setpiece_edxt_scaled_ratio"] = np.where(scaled_denom > 0, df["setpiece_edxt_scaled_positive_sum"] / scaled_denom, 0.0)
        df["setpiece_edxt_raw_ratio"] = np.where(raw_denom > 0, df["setpiece_edxt_positive_sum"] / raw_denom, 0.0)
    edxt_feature_cols.extend([
        "setpiece_edxt_scaled_positive_sum",
        "openplay_edxt_scaled_positive_sum",
        "setpiece_edxt_positive_sum",
        "openplay_edxt_positive_sum",
        "setpiece_edxt_scaled_ratio",
        "setpiece_edxt_raw_ratio",
    ])
    edxt_feature_cols = sorted(dict.fromkeys(edxt_feature_cols))


# %% trusted=true

print("チーム文脈特徴量を追加しています...")

team_context_feature_cols = []
team_base_candidates = [
    col
    for col in train_df.columns
    if col.startswith("xt_learned_") or col.startswith("possession_xt_") or col.endswith("_edxt_sum")
]
team_base_columns = [col for col in team_base_candidates if np.issubdtype(train_df[col].dtype, np.number)]

if team_base_columns:
    for df in (train_df, test_df):
        for col in team_base_columns:
            team_sum = df.groupby(["match_id", "Squad"])[col].transform("sum")
            sum_col = f"{col}_team_sum"
            share_col = f"{col}_team_share"
            lopo_col = f"{col}_team_lopo"
            df[sum_col] = team_sum
            df[share_col] = np.where(team_sum != 0, df[col] / team_sum, 0.0)
            df[lopo_col] = team_sum - df[col]
            df[sum_col] = df[sum_col].fillna(0.0)
            df[share_col] = df[share_col].fillna(0.0)
            df[lopo_col] = df[lopo_col].fillna(0.0)
            if df is train_df:
                team_context_feature_cols.extend([sum_col, share_col, lopo_col])

    team_context_feature_cols = sorted(dict.fromkeys(team_context_feature_cols))
else:
    team_context_feature_cols = []


# %% trusted=true

print("リーグ別ゲーティング特徴を作成しています...")

comp_interaction_feature_cols = []
comp_cross_base_features = [
    col
    for col in [
        "xt_learned_delta_sum",
        "xt_learned_positive_delta_sum",
        "possession_xt_positive_sum",
        "possession_xt_delta_sum",
        "possession_speed_ground_mean",
    ]
    if col in train_df.columns
]

if comp_cross_base_features:
    comp_dummy_train = pd.get_dummies(train_df["Comp"].astype(str), prefix="comp_gate", dtype=float)
    comp_dummy_test = pd.get_dummies(test_df["Comp"].astype(str), prefix="comp_gate", dtype=float)
    comp_dummy_train, comp_dummy_test = comp_dummy_train.align(comp_dummy_test, join="outer", axis=1, fill_value=0.0)
    comp_dummy_test = comp_dummy_test[comp_dummy_train.columns]

    train_base = train_df[comp_cross_base_features].fillna(0.0)
    test_base = test_df[comp_cross_base_features].fillna(0.0)

    for base_col in comp_cross_base_features:
        train_values = train_base[base_col].to_numpy()
        test_values = test_base[base_col].to_numpy()
        for comp_col in comp_dummy_train.columns:
            feat_name = f"{base_col}__{comp_col}"
            train_df[feat_name] = train_values * comp_dummy_train[comp_col].to_numpy()
            test_df[feat_name] = test_values * comp_dummy_test[comp_col].to_numpy()
            comp_interaction_feature_cols.append(feat_name)

    comp_interaction_feature_cols = sorted(dict.fromkeys(comp_interaction_feature_cols))
else:
    comp_interaction_feature_cols = []


# %% [markdown]
# ## 🆕 新規GCA特徴量の追加 (exp0035拡張)
#
# GCA（Goal Creating Actions）分析に基づく空間的・連鎖的特徴量を追加します。
#
# **追加特徴量グループ (28列)**:
# 1. GCA空間特徴量（ゾーン14、ハーフスペース、カットバック）
# 2. ラインブレイク/パッキング特徴量
# 3. パス連鎖品質特徴量（1-2、三人目、速攻）
# 4. PA進入受け手文脈特徴量
# 5. セットプレー/ボディパート特徴量

# %% trusted=true
# 新規GCA特徴量関数のインポート
from scripts.advanced_features import (
    build_gca_spatial_features,
    build_linebreak_packing_features,
    build_pass_chain_quality_features,
    build_box_entry_receiving_features,
    build_setplay_bodypart_features,
)

print("="*60)
print("🚀 新規GCA特徴量の生成開始")
print("="*60)

# アクションデータの分割（train/test）
action_data_train = relevant_actions[relevant_actions['match_id'].isin(train_df['match_id'])].copy()
action_data_test = relevant_actions[relevant_actions['match_id'].isin(test_df['match_id'])].copy()

# 1. GCA空間特徴量
print("\n[1/5] GCA空間特徴量（ゾーン14、ハーフスペース、カットバック）")
gca_spatial_train = build_gca_spatial_features(
    actions=action_data_train,
    match_col="match_id",
    player_col="player_id",
    team_col="team_id",
    type_col="type_name",
    result_col="result_name",
)
gca_spatial_test = build_gca_spatial_features(
    actions=action_data_test,
    match_col="match_id",
    player_col="player_id",
    team_col="team_id",
    type_col="type_name",
    result_col="result_name",
)
print(f"   ✅ 生成完了: {gca_spatial_train.shape[1]-2}列 (train: {len(gca_spatial_train)} rows)")

# 2. ラインブレイク/パッキング特徴量
print("\n[2/5] ラインブレイク/パッキング特徴量")
linebreak_train = build_linebreak_packing_features(
    actions=action_data_train,
    match_col="match_id",
    player_col="player_id",
    type_col="type_name",
    result_col="result_name",
)
linebreak_test = build_linebreak_packing_features(
    actions=action_data_test,
    match_col="match_id",
    player_col="player_id",
    type_col="type_name",
    result_col="result_name",
)
print(f"   ✅ 生成完了: {linebreak_train.shape[1]-2}列 (train: {len(linebreak_train)} rows)")

# 3. パス連鎖品質特徴量
print("\n[3/5] パス連鎖品質特徴量（1-2、三人目、速攻）")
pass_chain_train = build_pass_chain_quality_features(
    actions=action_data_train,
    match_col="match_id",
    player_col="player_id",
    team_col="team_id",
    type_col="type_name",
    time_col="time_seconds",
)
pass_chain_test = build_pass_chain_quality_features(
    actions=action_data_test,
    match_col="match_id",
    player_col="player_id",
    team_col="team_id",
    type_col="type_name",
    time_col="time_seconds",
)
print(f"   ✅ 生成完了: {pass_chain_train.shape[1]-2}列 (train: {len(pass_chain_train)} rows)")

# 4. PA進入受け手文脈特徴量
print("\n[4/5] PA進入受け手文脈特徴量")
box_entry_train = build_box_entry_receiving_features(
    actions=action_data_train,
    match_col="match_id",
    player_col="player_id",
    type_col="type_name",
    result_col="result_name",
)
box_entry_test = build_box_entry_receiving_features(
    actions=action_data_test,
    match_col="match_id",
    player_col="player_id",
    type_col="type_name",
    result_col="result_name",
)
print(f"   ✅ 生成完了: {box_entry_train.shape[1]-2}列 (train: {len(box_entry_train)} rows)")

# 5. セットプレー/ボディパート特徴量
print("\n[5/5] セットプレー/ボディパート特徴量")
setplay_bp_train = build_setplay_bodypart_features(
    actions=action_data_train,
    match_col="match_id",
    player_col="player_id",
    type_col="type_name",
    bodypart_col="bodypart_name",
)
setplay_bp_test = build_setplay_bodypart_features(
    actions=action_data_test,
    match_col="match_id",
    player_col="player_id",
    type_col="type_name",
    bodypart_col="bodypart_name",
)
print(f"   ✅ 生成完了: {setplay_bp_train.shape[1]-2}列 (train: {len(setplay_bp_train)} rows)")

print("\n" + "="*60)
print("✅ 全ての新規GCA特徴量生成完了")
print("="*60)

# %% trusted=true
# 新規特徴量のマージ
print("\n📊 新規特徴量をtrain_df/test_dfにマージ中...")

# train_dfに統合
train_df = train_df.merge(gca_spatial_train, on=["match_id", "player_id"], how="left")
train_df = train_df.merge(linebreak_train, on=["match_id", "player_id"], how="left")
train_df = train_df.merge(pass_chain_train, on=["match_id", "player_id"], how="left")
train_df = train_df.merge(box_entry_train, on=["match_id", "player_id"], how="left")
train_df = train_df.merge(setplay_bp_train, on=["match_id", "player_id"], how="left")

# test_dfに統合
test_df = test_df.merge(gca_spatial_test, on=["match_id", "player_id"], how="left")
test_df = test_df.merge(linebreak_test, on=["match_id", "player_id"], how="left")
test_df = test_df.merge(pass_chain_test, on=["match_id", "player_id"], how="left")
test_df = test_df.merge(box_entry_test, on=["match_id", "player_id"], how="left")
test_df = test_df.merge(setplay_bp_test, on=["match_id", "player_id"], how="left")

# 新規特徴量リストの定義
new_gca_features = [
    # GCA空間特徴量 (10列)
    "zone14_origin_pass_count", "zone14_origin_pass_success_rate", "zone14_preGCA_count",
    "halfspace_L_to_box_count", "halfspace_L_to_box_success_rate",
    "halfspace_R_to_box_count", "halfspace_R_to_box_success_rate",
    "cutback_count", "cutback_success_rate", "cutback_next_shot_rate",
    # ラインブレイク/パッキング (5列)
    "linebreak_third_transition_count", "linebreak_third_transition_rate",
    "through_channel_pass_count", "through_channel_pass_rate", "packing_approx_score_mean",
    # パス連鎖品質 (3列)
    "one_two_chain_count", "third_man_release_count", "burst_window_SCA_rate",
    # PA進入受け手文脈 (5列)
    "box_entry_from_zone14_count", "box_entry_from_halfspace_L_count",
    "box_entry_from_halfspace_R_count", "facing_forward_share_in_box", "first_touch_shot_rate_in_box",
    # セットプレー/ボディパート (5列)
    "setplay_GCA_share", "openplay_GCA_share",
    "bodypart_on_key_pass_rate_right", "bodypart_on_key_pass_rate_left", "bodypart_on_key_pass_rate_head",
]

# 実際に存在する列のみフィルタ
new_gca_features = [f for f in new_gca_features if f in train_df.columns]

# NaN埋め（念のため）
for col in new_gca_features:
    if col in train_df.columns:
        train_df[col] = train_df[col].fillna(0.0)
    if col in test_df.columns:
        test_df[col] = test_df[col].fillna(0.0)

print(f"\n✅ マージ完了:")
print(f"   - 新規特徴量: {len(new_gca_features)}列")
print(f"   - train_df shape: {train_df.shape}")
print(f"   - test_df shape: {test_df.shape}")

print(f"\n📋 新規GCA特徴量リスト:")
for i, feat in enumerate(new_gca_features, 1):
    print(f"   {i:2d}. {feat}")

# %% trusted=true
# ============================================================
# 攻撃テンポ + 視野・認知特徴量の追加 (exp0035拡張 Phase 2)
# ============================================================

from scripts.advanced_features import (
    build_attack_tempo_features,
    build_vision_cognition_features,
)

print("=" * 60)
print("🚀 攻撃テンポ + 視野・認知特徴量の生成開始")
print("=" * 60)

# 1. 攻撃テンポ特徴量
print("\n[1/2] 攻撃テンポ・リズム特徴量（5列）")
tempo_train = build_attack_tempo_features(
    actions=action_data_train,
    match_col="match_id",
    player_col="player_id",
    team_col="team_id",
    type_col="type_name",
    time_col="time_seconds",
)
tempo_test = build_attack_tempo_features(
    actions=action_data_test,
    match_col="match_id",
    player_col="player_id",
    team_col="team_id",
    type_col="type_name",
    time_col="time_seconds",
)
print(f"   ✅ 生成完了: {tempo_train.shape[1]-2}列 (train: {len(tempo_train)} rows)")

# 2. 視野・認知特徴量
print("\n[2/2] 視野・認知系特徴量（4列）")
vision_train = build_vision_cognition_features(
    actions=action_data_train,
    match_col="match_id",
    player_col="player_id",
    team_col="team_id",
    type_col="type_name",
)
vision_test = build_vision_cognition_features(
    actions=action_data_test,
    match_col="match_id",
    player_col="player_id",
    team_col="team_id",
    type_col="type_name",
)
print(f"   ✅ 生成完了: {vision_train.shape[1]-2}列 (train: {len(vision_train)} rows)")

print("\n" + "=" * 60)
print("✅ 全ての新規特徴量生成完了")
print("=" * 60)

# %% trusted=true
# ============================================================
# 新規特徴量のマージ
# ============================================================

print("\n📊 新規特徴量をtrain_df/test_dfにマージ中...")

# train_dfに統合
train_df = train_df.merge(tempo_train, on=["match_id", "player_id"], how="left")
train_df = train_df.merge(vision_train, on=["match_id", "player_id"], how="left")

# test_dfに統合
test_df = test_df.merge(tempo_test, on=["match_id", "player_id"], how="left")
test_df = test_df.merge(vision_test, on=["match_id", "player_id"], how="left")

# 新規特徴量リストの定義
tempo_vision_features = [
    # 攻撃テンポ特徴量 (5列)
    "possession_duration_before_shot",
    "pass_tempo_variance",
    "acceleration_phase_count",
    "quick_transition_rate",
    "slow_buildup_gca_rate",
    
    # 視野・認知特徴量 (4列)
    "switch_play_gca",
    "blind_side_pass_count",
    "cross_field_progression",
    "vision_angle_wide_pass",
]

# 実際に存在する列のみフィルタ
tempo_vision_features = [f for f in tempo_vision_features if f in train_df.columns]

# NaN埋め（念のため）
for col in tempo_vision_features:
    if col in train_df.columns:
        train_df[col] = train_df[col].fillna(0.0)
    if col in test_df.columns:
        test_df[col] = test_df[col].fillna(0.0)

print(f"\n✅ マージ完了:")
print(f"   - 新規特徴量: {len(tempo_vision_features)}列")
print(f"   - train_df shape: {train_df.shape}")
print(f"   - test_df shape: {test_df.shape}")

# 特徴量リスト表示
print(f"\n📋 新規特徴量リスト:")
for i, feat in enumerate(tempo_vision_features, 1):
    print(f"   {i:2d}. {feat}")

# %% trusted=true
# ============================================================
# tempo_vision_features を一時保存（後で all_features に追加）
# ============================================================

# この段階では all_features がまだ定義されていないため、
# tempo_vision_features を保存しておき、all_features 定義セル実行後に自動追加される
print(f"✅ tempo_vision_features ({len(tempo_vision_features)}列) を保存")
print(f"ℹ️  all_features 定義セル実行後に自動追加されます")

# カテゴリカル変数は変更なし（新規特徴量は全て数値型）
if 'categorical_features' in globals():
    print(f"ℹ️  カテゴリカル特徴量: {len(categorical_features)}列 (変更なし)")

# %% trusted=true
# ============================================================
# 単調性制約の更新（任意）
# ============================================================

# 新規特徴量で単調増加制約を適用するもの
new_monotone_increase = [
    "acceleration_phase_count",    # 加速フェーズは多いほど良い
    "switch_play_gca",             # サイドチェンジGCAは多いほど良い
    "blind_side_pass_count",       # ブラインドサイドパスは多いほど良い
    "cross_field_progression",     # 対角線的前進は多いほど良い
    "vision_angle_wide_pass",      # 広角視野パスは多いほど良い
]

# 既存の単調増加特徴量リストに追加（LightGBM）
if 'lgbm_monotone_increase_features' in globals():
    lgbm_monotone_increase_features = lgbm_monotone_increase_features + new_monotone_increase
    
    # LightGBM制約ベクトル再生成
    if 'lgbm_features' in globals():
        monotone_constraints = [
            1 if feat in lgbm_monotone_increase_features else 0
            for feat in lgbm_features
        ]
        print(f"✅ LightGBM単調増加制約: {sum(monotone_constraints)}個の特徴量")

# CatBoost用（任意）
if 'catboost_monotone_increase_features' in globals():
    catboost_monotone_increase_features = catboost_monotone_increase_features + new_monotone_increase
    print(f"✅ CatBoost単調増加制約: {len(catboost_monotone_increase_features)}個の特徴量")

# %% [markdown] id="A7xHPcUziuhW"
# ## 特徴量の統合
#
# 作成した全ての特徴量を統合し、train/testデータにマージします。

# %% colab={"base_uri": "https://localhost:8080/"} id="oiPaa-Dz6Gvj" outputId="cc523b94-b8ff-47fd-b87c-8ced83eb52a0" trusted=true
# 応用特徴量をtrain/testへマージ
train_df = (
    train_df
    .merge(success_rates, on=['match_id', 'player_id'], how='left')
    .merge(zone_actions, on=['match_id', 'player_id'], how='left')
    .merge(per_minute_features, on=['match_id', 'player_id'], how='left')
    .merge(offense_defense_balance, on=['match_id', 'player_id'], how='left')
    .merge(pass_leads_to_shot, on=['match_id', 'player_id'], how='left')
    .merge(progressive_features, on=['match_id', 'player_id'], how='left')
)

train_df['pass_leads_to_shot'] = train_df['pass_leads_to_shot'].fillna(0)

progressive_cols = [col for col in progressive_features.columns if col not in ['match_id', 'player_id']]
for col in progressive_cols:
    train_df[col] = train_df[col].fillna(0.0)


test_df = (
    test_df
    .merge(success_rates, on=['match_id', 'player_id'], how='left')
    .merge(zone_actions, on=['match_id', 'player_id'], how='left')
    .merge(per_minute_features, on=['match_id', 'player_id'], how='left')
    .merge(offense_defense_balance, on=['match_id', 'player_id'], how='left')
    .merge(pass_leads_to_shot, on=['match_id', 'player_id'], how='left')
    .merge(progressive_features, on=['match_id', 'player_id'], how='left')
)

test_df['pass_leads_to_shot'] = test_df['pass_leads_to_shot'].fillna(0)
for col in progressive_cols:
    test_df[col] = test_df[col].fillna(0.0)

print(f"マージ後のtrainデータshape: {train_df.shape}")
print(f"マージ後のtestデータshape: {test_df.shape}")


# %% [markdown] id="FHzc1BjgZYRR"
# ## クロスバリデーション分割
#
# 初回のbaselineでは、全データをランダムに分割するKFoldを用いましたが、今回はデータ特性に合わせた別の分割方法を行います。
#
# EDAで確認したように、今回はtrainデータとtestデータについては、match_idの重なりはありません。
# すなわち、testデータを予測するときには、これまで見たことのない試合のデータに対して予測をする必要があります。
# この状況をtrainデータ内部でのCross Validationでもなるべく再現することによって、実際のタスクに近い状況で正しい評価が可能になります。
#
# ここでは、GroupKFoldを用いて、trainデータをmatch_idが被らないように5分割します。こうすることで、各foldでのtrainデータとvalidデータのmatch_idが重ならなくなります。
#

# %% [markdown] id="EqDZiepKaIf3"
# ## モデル学習用データ準備

# %% trusted=true

# ターゲットエンコーディング特徴量の作成
print("ターゲットエンコーディング特徴量を作成中...")

# Squad×Opponentの交互作用特徴を作成
train_df["Squad_x_Opponent"] = train_df["Squad"].astype(str) + "_vs_" + train_df["Opponent"].astype(str)
test_df["Squad_x_Opponent"] = test_df["Squad"].astype(str) + "_vs_" + test_df["Opponent"].astype(str)

target_encoding_cols = ["player_id", "Squad", "Opponent", "Squad_x_Opponent"]
global_mean = train_df["xAG"].mean()
smoothing = 10.0
fold_labels = sorted(train_df["fold"].unique())

for col in target_encoding_cols:
    enc_col = f"{col}_target_enc"
    train_df[enc_col] = np.nan

    for fold in fold_labels:
        trn = train_df[train_df["fold"] != fold]
        val_mask = train_df["fold"] == fold

        stats = trn.groupby(col)["xAG"].agg(["sum", "count"])
        stats["encoding"] = (stats["sum"] + global_mean * smoothing) / (stats["count"] + smoothing)

        train_df.loc[val_mask, enc_col] = train_df.loc[val_mask, col].map(stats["encoding"]).fillna(global_mean)

    overall_stats = train_df.groupby(col)["xAG"].agg(["sum", "count"])
    overall_stats["encoding"] = (overall_stats["sum"] + global_mean * smoothing) / (overall_stats["count"] + smoothing)

    test_df[enc_col] = test_df[col].map(overall_stats["encoding"]).fillna(global_mean)

    missing_train = train_df[enc_col].isna().sum()
    missing_test = test_df[enc_col].isna().sum()

    if missing_train > 0:
        train_df.loc[train_df[enc_col].isna(), enc_col] = global_mean
    if missing_test > 0:
        test_df.loc[test_df[enc_col].isna(), enc_col] = global_mean

    print(f"  {col}: train missing {int(missing_train)}, test missing {int(missing_test)}")

print("リーグ/チームバイアス調整特徴を計算中...")

train_df["Comp_target_enc"] = np.nan
comp_smoothing = 5.0

for fold in fold_labels:
    trn = train_df[train_df["fold"] != fold]
    val_mask = train_df["fold"] == fold

    comp_stats = trn.groupby("Comp")["xAG"].agg(["sum", "count"])
    comp_stats["encoding"] = (comp_stats["sum"] + global_mean * comp_smoothing) / (comp_stats["count"] + comp_smoothing)

    train_df.loc[val_mask, "Comp_target_enc"] = train_df.loc[val_mask, "Comp"].map(comp_stats["encoding"]).fillna(global_mean)

comp_overall_stats = train_df.groupby("Comp")["xAG"].agg(["sum", "count"])
comp_overall_stats["encoding"] = (comp_overall_stats["sum"] + global_mean * comp_smoothing) / (comp_overall_stats["count"] + comp_smoothing)

test_df["Comp_target_enc"] = test_df["Comp"].map(comp_overall_stats["encoding"]).fillna(global_mean)

train_df["Comp_target_enc"] = train_df["Comp_target_enc"].fillna(global_mean)

test_df["Comp_target_enc"] = test_df["Comp_target_enc"].fillna(global_mean)

train_df["Squad_comp_residual"] = train_df["Squad_target_enc"] - train_df["Comp_target_enc"]
test_df["Squad_comp_residual"] = test_df["Squad_target_enc"] - test_df["Comp_target_enc"]

train_df["Squad_global_residual"] = train_df["Squad_target_enc"] - global_mean
test_df["Squad_global_residual"] = test_df["Squad_target_enc"] - global_mean

# diff between Squad and Opponent TE for matchup effect
train_df["Squad_vs_opponent_gap"] = train_df["Squad_target_enc"] - train_df["Opponent_target_enc"]
test_df["Squad_vs_opponent_gap"] = test_df["Squad_target_enc"] - test_df["Opponent_target_enc"]

print("  Squad_comp_residual などの新特徴を追加しました。")



# %% colab={"base_uri": "https://localhost:8080/"} id="507qeMbXaIf3" outputId="bc0b39f4-fe13-4491-8f88-71c960669942" trusted=true

# 各特徴量グループの定義
base_features = ["age", "action_count", "avg_x", "avg_y", "minutes_played", "goal_count"]
categorical_features = ["Comp", "Squad", "Venue"]
action_type_features = [col for col in train_df.columns if (col.startswith('type_')) and (col.endswith('_count'))]
success_rate_features = [
    col for col in train_df.columns
    if col.endswith('_success_rate') and not col.startswith('progressive_')
]
zone_features = [col for col in train_df.columns if col.startswith('zone_')]
per_minute_features = [col for col in train_df.columns if col.endswith('_per_minute')]
ad_balance_features = ['type_offensive_actions', 'type_defensive_actions', 'type_offensive_action_ratio']
sequencial_features = ['pass_leads_to_shot']
progressive_feature_cols = [
    col for col in train_df.columns
    if col.startswith('progressive_')
    or col in ['deep_completion_count', 'final_third_entry_count', 'penalty_area_entry_count']
]
xt_cols = [col for col in train_df.columns if col.startswith('xt_')]
target_encoding_features = [f"{col}_target_enc" for col in ["player_id", "Squad", "Opponent", "Squad_x_Opponent"]]
extra_bias_features = ["Comp_target_enc", "Squad_comp_residual", "Squad_global_residual", "Squad_vs_opponent_gap"]
possession_features = [col for col in train_df.columns if col.startswith('possession_')]
pass_network_features = [col for col in train_df.columns if col.startswith('pass_net_')]
edxt_features = sorted(globals().get('edxt_feature_cols', []))
team_context_features = sorted(globals().get('team_context_feature_cols', []))
comp_interaction_features = sorted(globals().get('comp_interaction_feature_cols', []))

all_features = (
    base_features
    + categorical_features
    + action_type_features
    + success_rate_features
    + zone_features
    + per_minute_features
    + ad_balance_features
    + sequencial_features
    + progressive_feature_cols
    + xt_cols
    + target_encoding_features
    + extra_bias_features
    + possession_features
    + pass_network_features
    + edxt_features
    + team_context_features
    + comp_interaction_features
)

all_features = list(dict.fromkeys(all_features))

# 追加: 高度特徴量を all_features に含める（学習用データ作成の前に実施）
advanced_candidates = [
    "second_assist_count",
    "SCA_1",
    "SCA_2",
    "GCA_1",
    "GCA_2",
    "nstep_to_shot",
    "nstep_xt_delta",
    "pass_dist_mean",
    "pass_dist_max",
    "to_goal_angle_abs_mean",
    "to_goal_dist_mean",
    "pass_to_shot_latency_mean",
    "pass_to_shot_latency_min",
    "risk_creativity_sum",
    "xpass_mean",
    "xpass_min",
    "pass_success_minus_xpass",
    "xpass_deep_mean",
    "xpass_box_mean",
    # 時系列トレンド
    "xAG_expanding_mean",
    "xAG_rolling3_mean",
    "xAG_diff_prev",
    # 🆕 新特徴量（EXP0025）
    "first_half_actions", "second_half_actions", "final_15min_actions",
    "early_10min_actions", "time_weighted_intensity",
    "defensive_zone_actions", "middle_zone_actions", "attacking_zone_actions",
    "halfspace_left_actions", "halfspace_right_actions", "central_corridor_actions",
    "final_third_penetrations", "box_entries",
    "betweenness_centrality", "closeness_centrality", "degree_centrality",
    "pass_receiver_diversity", "unique_pass_partners",
    "longchain_to_shot", "longchain_xt_delta",
    "position_variance_x", "position_variance_y", "position_range_x",
    "position_range_y", "avg_action_distance",
]
advanced_features = [c for c in advanced_candidates if c in train_df.columns]
if advanced_features:
    all_features = list(dict.fromkeys(all_features + advanced_features))
    print(f"追加された高度特徴量（学習前反映）: {len(advanced_features)}個 (🆕新特徴量25個含む)")

# カテゴリカル変数については、列の型を「category」に変更しておく
for col in categorical_features:
    train_df[col] = train_df[col].astype("category")
    test_df[col] = test_df[col].astype("category")

print(f"  - 使用する特徴量数: {len(all_features)}個")
print(f"  - 基本特徴量: {len(base_features)}個")
print(f"  - カテゴリカル特徴量: {len(categorical_features)}個")
print(f"  - アクション特徴量(type_*_count): {len(action_type_features)}個")
print(f"  - 成功率系: {len(success_rate_features)}個")
print(f"  - ゾーン系: {len(zone_features)}個")
print(f"  - per_minute系: {len(per_minute_features)}個")
print(f"  - 攻守バランス系: {len(ad_balance_features)}個")
print(f"  - 時系列系: {len(sequencial_features)}個")
print(f"  - プログレッシブ系: {len(progressive_feature_cols)}個")
print(f"  - xT系: {len(xt_cols)}個")
print(f"  - ターゲットエンコーディング系: {len(target_encoding_features)}個")
print(f"  - ポゼッション進攻系: {len(possession_features)}個")
print(f"  - パスネットワーク系: {len(pass_network_features)}個")
print(f"  - eΔxT系: {len(edxt_features)}個")
print(f"  - チーム文脈系: {len(team_context_features)}個")
print(f"  - リーグ相互作用系: {len(comp_interaction_features)}個")

# 🆕 攻撃テンポ+視野・認知特徴量を追加 (exp0035 Phase 2)
if 'tempo_vision_features' in globals():
    all_features = all_features + tempo_vision_features
    all_features = list(dict.fromkeys(all_features))  # 重複削除
    print(f"✅ 攻撃テンポ+視野・認知特徴量: {len(tempo_vision_features)}列追加")



# %% trusted=true
# 🆕 新規GCA特徴量をall_featuresに追加 (exp0035)
if 'new_gca_features' in globals() and new_gca_features:
    # 実在する特徴量のみフィルタ（念のため再確認）
    new_gca_features_valid = [f for f in new_gca_features if f in train_df.columns]
    
    # all_featuresに追加（重複排除）
    all_features = list(dict.fromkeys(all_features + new_gca_features_valid))
    
    print(f"\n🆕 新規GCA特徴量を追加:")
    print(f"   - 追加数: {len(new_gca_features_valid)}個")
    print(f"   - 総特徴量数: {len(all_features)}個")
    
    # 特徴量グループ別サマリー
    gca_spatial = [f for f in new_gca_features_valid if 'zone14' in f or 'halfspace' in f or 'cutback' in f]
    linebreak = [f for f in new_gca_features_valid if 'linebreak' in f or 'through_channel' in f or 'packing' in f]
    chain = [f for f in new_gca_features_valid if 'one_two' in f or 'third_man' in f or 'burst' in f]
    box_entry = [f for f in new_gca_features_valid if 'box_entry' in f or 'facing_forward' in f or 'first_touch' in f]
    setplay = [f for f in new_gca_features_valid if 'setplay' in f or 'openplay' in f or 'bodypart' in f]
    
    print(f"\n   📊 内訳:")
    print(f"      - GCA空間特徴量: {len(gca_spatial)}個")
    print(f"      - ラインブレイク/パッキング: {len(linebreak)}個")
    print(f"      - パス連鎖品質: {len(chain)}個")
    print(f"      - PA進入受け手文脈: {len(box_entry)}個")
    print(f"      - セットプレー/ボディパート: {len(setplay)}個")
else:
    print("⚠️  new_gca_features が定義されていません（特徴量生成セルを先に実行してください）")

# %% colab={"base_uri": "https://localhost:8080/"} id="sUaVQAMtR2wk" outputId="a5af6982-d1c3-4d7d-f5aa-a51963962df1" trusted=true
# モデル学習用データの作成
X_train = train_df[all_features + ["fold"]]
y_train = train_df["xAG"]
X_test = test_df[all_features]

print(f"\nモデル学習用データ形状: X_train {X_train.shape}, y_train {y_train.shape}, X_test {X_test.shape}")

# %% [markdown] id="cCk52KvKaIf3"
# ## モデル学習（LightGBM）
#
#
#

# %% [markdown] id="fBFkOqtLhraM"
# 特徴量の数も増えており、ハイパーパラメータは探索してみないと分かりません。
#
# ここでは、より効率的に良さそうなパラメータを見つけられるOptunaを用いて最適化を行います。
# Optunaは探索空間から試行回数ごとに候補を提案し、良かった試行の情報を活かしながら次の探索に反映させるベイズ的最適化アルゴリズム（TPEサンプラー）を利用できるため、総当たりのGridSearchよりも少ない試行で良い結果に辿り着きやすいのがメリットです。
# （参考: https://zenn.dev/robes/articles/d53ff6d665650f ）


# %% colab={"base_uri": "https://localhost:8080/"} id="ngpAJwwnaIf3" outputId="321e83a6-aee6-4b0a-88c6-199be978c2c2" trusted=true
import optuna
from optuna.samplers import TPESampler

# Optunaで最適化しないベースパラメータ
base_params = {
    "objective": "regression_l2",
    "boosting_type": "gbdt",
    "random_state": SEED,
    "verbosity": -1,
    "force_col_wise": True,
    "bagging_fraction": 0.8,
    "bagging_freq": 1,
    "feature_fraction": 0.8,
}

# Optunaで探索するハイパーパラメータの概要
optuna_search_space = {
    "num_leaves": (10, 64),
    "learning_rate": (0.01, 0.1),
    "min_child_samples": (10, 50),
}

print("Optuna用ハイパーパラメータ設定完了")
print(f"探索対象パラメータ: {list(optuna_search_space.keys())}")

# %% colab={"base_uri": "https://localhost:8080/"} id="-hkjQJvTvhMO" outputId="61949fbb-9b97-43cd-9bfc-e704053383ba" trusted=true
try:
    base_dir = Path(__file__).resolve().parent
except NameError:  # __file__ はノートブック実行時には定義されない
    base_dir = Path.cwd()

log_dir = base_dir / "logs"
log_dir.mkdir(parents=True, exist_ok=True)


def save_training_run(
    cv_scores,
    oof_score,
    optuna_summary,
    best_params,
    log_directory: Path,
    log_prefix: str = "host_baseline_002",
):
    """Persist CV metrics to reusable JSON/text logs."""

    timestamp = datetime.now().astimezone().isoformat(timespec="seconds")

    metrics_payload = {
        "run_timestamp": timestamp,
        "cv": {
            "scores": [float(score) for score in cv_scores],
            "mean": float(np.mean(cv_scores)),
            "std": float(np.std(cv_scores)),
        },
        "per_fold": {f"fold_{idx + 1}": float(score) for idx, score in enumerate(cv_scores)},
        "oof_rmse": float(oof_score),
        "optuna": optuna_summary,
        "best_params": {key: (float(val) if isinstance(val, (np.floating, np.integer)) else val)
                         for key, val in best_params.items()},
    }

    metrics_path = log_directory / f"{log_prefix}_metrics.json"
    metrics_path.write_text(json.dumps(metrics_payload, ensure_ascii=False, indent=2), encoding="utf-8")

    log_lines = [
        f"[{timestamp}] {log_prefix}",
        f"  CV mean: {metrics_payload['cv']['mean']:.4f}",
        f"  CV std: {metrics_payload['cv']['std']:.4f}",
        f"  OOF RMSE: {metrics_payload['oof_rmse']:.4f}",
    ]
    for idx, score in enumerate(cv_scores, start=1):
        log_lines.append(f"  Fold {idx}: {score:.4f}")

    log_lines.append(
        "  Optuna best trial: "
        f"{optuna_summary['best_trial_number']} (CV mean {optuna_summary['best_cv_value']:.6f}, "
        f"fold1 RMSE {optuna_summary['fold1_val_rmse']:.6f})"
    )

    log_path = log_directory / f"{log_prefix}_training.log"
    with log_path.open("a", encoding="utf-8") as fp:
        fp.write("\n".join(log_lines) + "\n")


# Optunaによるハイパーパラメータチューニング
print("Optunaによるチューニングを開始します...")

# Fold 1を検証用に確保（後でスコア確認に利用）
trn_mask = train_df["fold"] != 1
val_mask = train_df["fold"] == 1

X_tr = train_df.loc[trn_mask, all_features].copy()
X_val = train_df.loc[val_mask, all_features].copy()
y_tr = y_train.loc[trn_mask].copy()
y_val = y_train.loc[val_mask].copy()

def objective(trial):
    params = base_params.copy()
    params.update({
        "num_leaves": trial.suggest_int("num_leaves", *optuna_search_space["num_leaves"]),
        "learning_rate": trial.suggest_float("learning_rate", *optuna_search_space["learning_rate"], log=True),
        "min_child_samples": trial.suggest_int("min_child_samples", *optuna_search_space["min_child_samples"]),
    })

    cv_scores = []
    for fold in range(1, 4):  # 計算量を抑えるためFold1~3でCV
        trn_mask_cv = train_df["fold"] != fold
        val_mask_cv = train_df["fold"] == fold

        X_tr_cv = train_df.loc[trn_mask_cv, all_features].copy()
        X_val_cv = train_df.loc[val_mask_cv, all_features].copy()
        y_tr_cv = y_train.loc[trn_mask_cv].copy()
        y_val_cv = y_train.loc[val_mask_cv].copy()

        train_weights_cv = make_sample_weight(y_tr_cv)
        val_weights_cv = make_sample_weight(y_val_cv)
        train_data = lgb.Dataset(X_tr_cv, label=y_tr_cv, weight=train_weights_cv)
        val_data = lgb.Dataset(X_val_cv, label=y_val_cv, weight=val_weights_cv)

        model = lgb.train(
            params,
            train_data,
            valid_sets=[val_data],
            feval=weighted_rmse_feval,
            num_boost_round=1000,
            callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)],
        )

        preds = model.predict(X_val_cv, num_iteration=model.best_iteration)
        cv_scores.append(weighted_rmse(y_val_cv, preds))

    return float(np.mean(cv_scores))

optuna.logging.set_verbosity(optuna.logging.WARNING)
study = optuna.create_study(direction="minimize", sampler=TPESampler(seed=SEED))
study.optimize(objective, n_trials=30, show_progress_bar=True)

best_params = study.best_trial.params.copy()
best_lgbm_params = base_params.copy()
best_lgbm_params.update(best_params)

# Fold1でのスコアを再確認
train_weights = make_sample_weight(y_tr)
val_weights = make_sample_weight(y_val)
train_data = lgb.Dataset(X_tr, label=y_tr, weight=train_weights)
val_data = lgb.Dataset(X_val, label=y_val, weight=val_weights)
best_model = lgb.train(
    best_lgbm_params,
    train_data,
    valid_sets=[train_data, val_data],
    valid_names=["train", "val"],
    feval=weighted_rmse_feval,
    num_boost_round=1000,
    callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)],
)

val_pred = best_model.predict(X_val, num_iteration=best_model.best_iteration)
val_score = weighted_rmse(y_val, val_pred)

print("=== Optunaチューニング結果 ===")
print(f"最良Trial番号: {study.best_trial.number}")
print(f"平均CV RMSE: {study.best_value:.6f}")
print(f"Fold1 Validation RMSE: {val_score:.6f}")
print("最適化されたパラメータ:")
for param, value in best_params.items():
    print(f"  {param}: {value}")

# %% colab={"base_uri": "https://localhost:8080/"} id="f1o2kC3wsi0n" outputId="2e156a42-4637-433a-f6db-76205a1e3b21" trusted=true
# 最適化されたパラメータでの5-Fold Cross Validation
print("最適化されたパラメータでの5-Fold Cross Validationを開始...")

# 単調性制約の設定
# xAGと単調増加関係にある特徴量を選定
monotone_increase_features = [
    # プログレッシブ系（攻撃的な前進プレー → xAG増加）
    'progressive_pass_count',
    'progressive_pass_success',
    'progressive_pass_distance_total',
    'progressive_pass_distance_mean',
    'progressive_carry_count',
    'progressive_carry_success',
    'progressive_carry_distance_total',
    'progressive_carry_distance_mean',
    'deep_completion_count',  # ディープゾーン到達数
    'final_third_entry_count',  # ファイナルサード進入数
    'penalty_area_entry_count',  # ペナルティエリア進入数

    # シュート・ゴール系（創造性の指標）
    'goal_count',  # ゴール数
    'pass_leads_to_shot',  # パス→ショットの連鎖

    # 攻撃的ゾーン活動（前線でのプレー → xAG増加）
    'zone_attacking_actions',  # 攻撃ゾーンでのアクション数
    'zone_attacking_actions_ratio',  # 攻撃ゾーン比率

    # 攻撃的バランス
    'type_offensive_actions',  # 攻撃アクション数
    'type_offensive_action_ratio',  # 攻撃アクション比率
]

missing_monotone_features = [feat for feat in monotone_increase_features if feat not in all_features]
if missing_monotone_features:
    print("単調性制約対象として指定したものの、特徴量一覧に存在しない列があります:")
    for feat in missing_monotone_features:
        print(f"  - {feat}")

applied_monotone_features = [feat for feat in monotone_increase_features if feat in all_features]

# all_features内での各特徴量のインデックスを取得し、単調性ベクトルを構築
monotone_constraints = [0] * len(all_features)  # デフォルトは制約なし(0)
for feat in applied_monotone_features:
    idx = all_features.index(feat)
    monotone_constraints[idx] = 1  # 単調増加制約

print(f"\n単調性制約を適用: {len(applied_monotone_features)}個の特徴量")
print("単調増加制約を適用した特徴量:")
for feat in applied_monotone_features:
    print(f"  - {feat}")

# LightGBM学習のパラメータを設定（Optunaで最適化されたパラメータを使用）
lgbm_params = {
    "objective": "regression_l2",
    "boosting_type": "gbdt",
    "random_state": SEED,
    "verbosity": -1,
    "force_col_wise": True,
    "bagging_fraction": 0.8,
    "bagging_freq": 1,
    "feature_fraction": 0.8,
    "monotone_constraints": monotone_constraints,  # 単調性制約を追加
    "monotone_constraints_method": "advanced",  # advanced methodを使用
}

# Optunaの最適パラメータをマージ
lgbm_params.update(best_params)

print(f"\n使用するパラメータ: {lgbm_params}")

# 5-Foldでのモデル学習（最適化されたパラメータ使用）
oof_preds = np.zeros(len(X_train))
cv_scores = []
models = []
feature_importance = pd.DataFrame()

# Training the models on the entire training data
for fold in range(5):
    print(f"=== Fold {fold + 1} ===")

    # データ分割
    trn_mask = train_df["fold"] != fold+1
    val_mask = train_df["fold"] == fold+1

    X_tr = train_df.loc[trn_mask, all_features].copy()
    X_val = train_df.loc[val_mask, all_features].copy()
    y_tr = y_train.loc[trn_mask].copy()
    y_val = y_train.loc[val_mask].copy()

    # LightGBMデータセット作成
    train_weights = make_sample_weight(y_tr)
    val_weights = make_sample_weight(y_val)
    train_data = lgb.Dataset(X_tr, label=y_tr, weight=train_weights)
    val_data = lgb.Dataset(X_val, label=y_val, weight=val_weights, reference=train_data)

    # モデル学習（最適パラメータ使用）
    model = lgb.train(
        lgbm_params,
        train_data,
        valid_sets=[train_data, val_data],
        valid_names=["train", "val"],
        feval=weighted_rmse_feval,
        num_boost_round=1000,
        callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)]
    )

    # validationデータに対する予測、スコア算出
    y_pred_val = model.predict(X_val, num_iteration=model.best_iteration)
    oof_preds[val_mask] = y_pred_val
    score = weighted_rmse(y_val, y_pred_val)
    cv_scores.append(score)
    models.append(model)  # このfoldのモデルをmodelsに格納

    print(f"Fold {fold + 1} RMSE: {score:.4f}")

    # 特徴量重要度算出
    fold_importance = pd.DataFrame({
        "feature": all_features,
        "importance": model.feature_importance(importance_type="gain"),
        "fold": fold + 1
    })
    feature_importance = pd.concat([feature_importance, fold_importance], axis=0)

cv_mean = float(np.mean(cv_scores))
cv_std = float(np.std(cv_scores))

print("=== Cross Validation Results (Optimized Parameters) ===")
print(f"CV RMSE: {cv_mean:.4f} (+/- {cv_std * 2:.4f})")
for i, score in enumerate(cv_scores):
    print(f"Fold {i + 1}: {score:.4f}")

# OOF予測のスコア算出
oof_score = weighted_rmse(y_train, oof_preds)
print(f"OOF RMSE: {oof_score:.4f}")

optuna_summary = {
    "best_trial_number": int(study.best_trial.number),
    "best_cv_value": float(study.best_value),
    "fold1_val_rmse": float(val_score),
}

save_training_run(
    cv_scores=cv_scores,
    oof_score=oof_score,
    optuna_summary=optuna_summary,
    best_params=best_params,
    log_directory=log_dir,
)

print(f"メトリクスを保存しました: {log_dir / 'host_baseline_002_metrics.json'}")
print(f"ログを追記しました: {log_dir / 'host_baseline_002_training.log'}")


# %% [markdown]
# ## CatBoost モデル学習
#
# CatBoostを使用して、LGBMと相補的なモデルを構築します。
#
# **CatBoostの特徴:**
# - **Ordered Target Statistics**: リークを回避しながらカテゴリ変数を効果的に処理
# - **高次交互作用**: カテゴリ変数間の複雑な交互作用を自動的に学習
# - **対称木構造**: より安定した予測を実現
#
# **実装方針:**
# - カテゴリカル特徴量を明示的に指定
# - LGBMと異なるseed/列サブセットで多様性を確保
# - 単調性制約は必要最小限に抑制
# - 重み付きRMSEに対応したサンプル重みを使用

# %% trusted=true
# CatBoost用のカテゴリカル特徴量を明示的に定義
catboost_categorical_features = ['Comp', 'Squad', 'Venue']

# CatBoostで使用する特徴量（LGBMと同じ特徴量を使用）
catboost_features = all_features.copy()

print(f"CatBoostで使用する特徴量数: {len(catboost_features)}個")
print(f"カテゴリカル特徴量: {catboost_categorical_features}")

# カテゴリカル特徴量のインデックスを取得
cat_features_idx = [catboost_features.index(col) for col in catboost_categorical_features if col in catboost_features]
print(f"カテゴリカル特徴量インデックス: {cat_features_idx}")

# %% trusted=true
# CatBoost用のOptunaハイパーパラメータ最適化
print("CatBoost ハイパーパラメータ最適化を開始...")

def objective_catboost(trial):
    """
    CatBoost用のOptuna目的関数
    LGBMと相補性を持たせるため、異なる探索空間を設定
    """
    params = {
        'iterations': 1000,
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),
        'depth': trial.suggest_int('depth', 4, 10),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),
        'random_strength': trial.suggest_float('random_strength', 0.1, 10.0, log=True),
        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),
        'border_count': trial.suggest_int('border_count', 32, 255),
        'loss_function': 'RMSE',
        'eval_metric': 'RMSE',
        'random_seed': SEED + 100,  # LGBMと異なるseedで多様性確保
        'verbose': False,
        'early_stopping_rounds': 100,
        'cat_features': cat_features_idx,
    }
    
    # Fold 1のみで評価（高速化のため）
    fold = 0
    trn_mask = train_df['fold'] != fold + 1
    val_mask = train_df['fold'] == fold + 1
    
    X_tr = train_df.loc[trn_mask, catboost_features].copy()
    X_val = train_df.loc[val_mask, catboost_features].copy()
    y_tr = y_train.loc[trn_mask].copy()
    y_val = y_train.loc[val_mask].copy()
    
    # サンプル重みを計算
    train_weights = make_sample_weight(y_tr)
    
    # CatBoost Pool作成
    train_pool = cb.Pool(
        data=X_tr,
        label=y_tr,
        weight=train_weights,
        cat_features=cat_features_idx
    )
    val_pool = cb.Pool(
        data=X_val,
        label=y_val,
        cat_features=cat_features_idx
    )
    
    # モデル学習
    model = cb.CatBoostRegressor(**params)
    model.fit(
        train_pool,
        eval_set=val_pool,
        verbose=False,
        plot=False
    )
    
    # 予測とスコア計算
    y_pred_val = model.predict(X_val)
    val_score = weighted_rmse(y_val, y_pred_val)
    
    return val_score

# Optuna最適化実行
catboost_study = optuna.create_study(
    direction='minimize',
    sampler=TPESampler(seed=SEED)
)

catboost_study.optimize(
    objective_catboost,
    n_trials=30,  # LGBMと同様に30試行
    show_progress_bar=True
)

catboost_best_params = catboost_study.best_params
print(f"\nCatBoost最適化完了")
print(f"Best trial: {catboost_study.best_trial.number}")
print(f"Best validation RMSE: {catboost_study.best_value:.4f}")
print(f"Best parameters: {catboost_best_params}")

# %% trusted=true
# CatBoost用の単調性制約設定
# LGBMよりも制約を緩めて、モデルの多様性を確保
print("CatBoost単調性制約を設定中...")

# 最も重要な特徴量のみに単調性制約を適用（LGBMより少なめ）
catboost_monotone_increase_features = [
    'goal_count',  # ゴール数
    'pass_leads_to_shot',  # パス→ショット
    'progressive_pass_count',  # プログレッシブパス数
    'deep_completion_count',  # ディープゾーン到達
    'penalty_area_entry_count',  # ペナルティエリア進入
    'zone_attacking_actions',  # 攻撃ゾーンアクション
]

# 特徴量が存在するもののみをフィルタリング
applied_catboost_monotone_features = [
    feat for feat in catboost_monotone_increase_features 
    if feat in catboost_features
]

# CatBoostの単調性制約形式: 文字列のカンマ区切り
# 形式: "feature_idx1,feature_idx2,..." で方向を指定
# または各特徴量に対して (idx, direction) のタプルのリスト
# ここでは、特徴量名ベースで辞書形式を使用

# CatBoostの単調性制約: 全特徴量に対する制約リスト（0=制約なし, 1=増加, -1=減少）
catboost_monotone_constraints = [0] * len(catboost_features)

for feat in applied_catboost_monotone_features:
    if feat in catboost_features:
        idx = catboost_features.index(feat)
        catboost_monotone_constraints[idx] = 1  # 単調増加制約

print(f"単調性制約を適用: {len(applied_catboost_monotone_features)}個の特徴量")
print(f"  (LGBMの{len(applied_monotone_features)}個から削減し、モデルの多様性を確保)")
for feat in applied_catboost_monotone_features:
    print(f"  - {feat}")

# %% trusted=true
# CatBoostでの5-Fold Cross Validation
print("\nCatBoostでの5-Fold Cross Validationを開始...")

# 最適化されたパラメータをベースに設定
catboost_params = {
    'iterations': 1000,
    'loss_function': 'RMSE',
    'eval_metric': 'RMSE',
    'random_seed': SEED + 100,  # LGBMと異なるseed
    'verbose': False,
    'early_stopping_rounds': 100,
    'cat_features': cat_features_idx,
    'monotone_constraints': catboost_monotone_constraints,  # 単調性制約
}

# Optunaの最適パラメータをマージ
catboost_params.update(catboost_best_params)

print(f"使用するパラメータ: {catboost_params}")

# CV結果を格納
catboost_oof_preds = np.zeros(len(X_train))
catboost_cv_scores = []
catboost_models = []
catboost_feature_importance = pd.DataFrame()

for fold in range(5):
    print(f"\n=== CatBoost Fold {fold + 1} ===")
    
    # データ分割
    trn_mask = train_df['fold'] != fold + 1
    val_mask = train_df['fold'] == fold + 1
    
    X_tr = train_df.loc[trn_mask, catboost_features].copy()
    X_val = train_df.loc[val_mask, catboost_features].copy()
    y_tr = y_train.loc[trn_mask].copy()
    y_val = y_train.loc[val_mask].copy()
    
    # サンプル重みを計算
    train_weights = make_sample_weight(y_tr)
    
    # CatBoost Pool作成
    train_pool = cb.Pool(
        data=X_tr,
        label=y_tr,
        weight=train_weights,  # 重み付きRMSE対応
        cat_features=cat_features_idx
    )
    val_pool = cb.Pool(
        data=X_val,
        label=y_val,
        cat_features=cat_features_idx
    )
    
    # モデル学習
    model = cb.CatBoostRegressor(**catboost_params)
    model.fit(
        train_pool,
        eval_set=val_pool,
        verbose=100,
        plot=False
    )
    
    # 予測とスコア計算
    y_pred_val = model.predict(X_val)
    catboost_oof_preds[val_mask] = y_pred_val
    score = weighted_rmse(y_val, y_pred_val)
    catboost_cv_scores.append(score)
    catboost_models.append(model)
    
    print(f"Fold {fold + 1} Weighted RMSE: {score:.4f}")
    
    # 特徴量重要度を取得
    fold_importance = pd.DataFrame({
        'feature': catboost_features,
        'importance': model.get_feature_importance(),
        'fold': fold + 1
    })
    catboost_feature_importance = pd.concat(
        [catboost_feature_importance, fold_importance], 
        axis=0
    )

# CV結果のサマリー
catboost_cv_mean = float(np.mean(catboost_cv_scores))
catboost_cv_std = float(np.std(catboost_cv_scores))

print("\n=== CatBoost Cross Validation Results ===")
print(f"CV Weighted RMSE: {catboost_cv_mean:.4f} (+/- {catboost_cv_std * 2:.4f})")
for i, score in enumerate(catboost_cv_scores):
    print(f"Fold {i + 1}: {score:.4f}")

# OOF予測のスコア
catboost_oof_score = weighted_rmse(y_train, catboost_oof_preds)
print(f"\nCatBoost OOF Weighted RMSE: {catboost_oof_score:.4f}")

# LGBMとの比較
print(f"\n--- モデル比較 ---")
print(f"LightGBM OOF RMSE: {oof_score:.4f}")
print(f"CatBoost OOF RMSE: {catboost_oof_score:.4f}")
print(f"差分: {catboost_oof_score - oof_score:+.4f}")

# %% [markdown]
# ## モデルブレンディング（LightGBM + CatBoost）
#
# LightGBMとCatBoostの予測を加重平均でブレンドします。
#
# **ブレンディング戦略:**
# - OOF予測でブレンド比率を最適化
# - グリッドサーチで最適な重みを探索
# - LGBMとCatBoostの相補性を活用

# %% trusted=true
# [exp0029] 非負制約スタッキング (NNLS) でブレンディング
print("=" * 80)
print("非負制約スタッキング (NNLS) を適用中...")
print("=" * 80)

from scripts.stacking import fit_nnls_stack, predict_nnls_stack

# 1. スタッキング用の特徴量を作成
print("\n1. スタッキング用特徴量を作成中...")

# 基本予測 + 派生特徴（正領域の強調）
oof_features = np.column_stack([
    oof_preds,                           # LGBM予測
    catboost_oof_preds,                  # CatBoost予測
    np.maximum(oof_preds - 0.1, 0),     # LGBM正領域強調
    np.maximum(catboost_oof_preds - 0.1, 0)  # CatBoost正領域強調
])

feature_names = ['LGBM', 'CatBoost', 'LGBM_pos', 'CatBoost_pos']
print(f"  特徴量数: {oof_features.shape[1]}")
print(f"  特徴量名: {feature_names}")

# 2. サンプル重みを作成（wRMSEと同じ重み）
sample_weight = make_sample_weight(y_train)

# 3. NNLSでスタッキング係数を学習
print("\n2. NNLS係数を学習中...")
nnls_coefs = fit_nnls_stack(
    oof_features,
    y_train.values,
    sample_weight
)

print(f"\n【学習された係数】")
for name, coef in zip(feature_names, nnls_coefs):
    print(f"  {name:15s}: {coef:.6f}")
print(f"  合計: {nnls_coefs.sum():.6f}")

# 4. OOF予測でスコア評価
print("\n3. OOF予測でスコアを評価中...")
nnls_oof_preds = predict_nnls_stack(oof_features, nnls_coefs)

nnls_oof_score = weighted_rmse(y_train, nnls_oof_preds)

# 比較用: グリッドサーチも実行
print("\n4. 比較用: グリッドサーチも実行中...")
best_grid_weight = 0.5
best_grid_score = float('inf')

for lgb_weight in np.arange(0.0, 1.01, 0.05):
    cb_weight = 1.0 - lgb_weight
    grid_oof = lgb_weight * oof_preds + cb_weight * catboost_oof_preds
    grid_score = weighted_rmse(y_train, grid_oof)
    
    if grid_score < best_grid_score:
        best_grid_score = grid_score
        best_grid_weight = lgb_weight

best_grid_cb_weight = 1.0 - best_grid_weight

print(f"\n{'=' * 80}")
print("【スコア比較】")
print(f"{'=' * 80}")
print(f"LightGBM単体 OOF:     {oof_score:.6f}")
print(f"CatBoost単体 OOF:     {catboost_oof_score:.6f}")
print(f"グリッドサーチOOF:    {best_grid_score:.6f}  (LGBM: {best_grid_weight:.2f}, CB: {best_grid_cb_weight:.2f})")
print(f"NNLS スタッキングOOF: {nnls_oof_score:.6f}")
print(f"\n【改善量】")
print(f"NNLS vs LGBM:         {nnls_oof_score - oof_score:+.6f}")
print(f"NNLS vs グリッド:     {nnls_oof_score - best_grid_score:+.6f}")

# 5. NNLSを採用
print("\n✅ NNLSスタッキングを採用")
blended_oof_preds = nnls_oof_preds
best_blend_score = nnls_oof_score

# グリッドサーチの重みも保存（後で使うため）
best_lgbm_weight = best_grid_weight
best_catboost_weight = best_grid_cb_weight

# %% [markdown]
# ## テストデータに対する推論（ブレンドモデル）

# %% trusted=true
# CatBoostでテストデータに対する推論
print("CatBoostでテストデータに対する推論を実行中...")

catboost_test_preds = np.zeros(len(X_test))

for fold, model in enumerate(catboost_models):
    fold_preds = model.predict(test_df[catboost_features])
    catboost_test_preds += fold_preds / 5
    print(f"Fold {fold + 1}: 予測完了")

print(f"\nCatBoost予測統計:")
print(f"  Mean: {catboost_test_preds.mean():.4f}")
print(f"  Std: {catboost_test_preds.std():.4f}")
print(f"  Min: {catboost_test_preds.min():.4f}")
print(f"  Max: {catboost_test_preds.max():.4f}")

# %% trusted=true
# LightGBMでテストデータに対する推論（既存のmodelsを使用）
print("LightGBMでテストデータに対する推論を実行中...")

lgbm_test_preds = np.zeros(len(X_test))

for fold, model in enumerate(models):
    fold_preds = model.predict(X_test, num_iteration=model.best_iteration)
    lgbm_test_preds += fold_preds / 5
    print(f"Fold {fold + 1}: 予測完了")

print(f"\nLightGBM予測統計:")
print(f"  Mean: {lgbm_test_preds.mean():.4f}")
print(f"  Std: {lgbm_test_preds.std():.4f}")
print(f"  Min: {lgbm_test_preds.min():.4f}")
print(f"  Max: {lgbm_test_preds.max():.4f}")

# %% trusted=true
# [exp0029] NNLSでテスト予測を作成
print("\nNNLSスタッキングでテスト予測を作成中...")

# テストデータ用の特徴量を作成
test_features = np.column_stack([
    lgbm_test_preds,                           # LGBM予測
    catboost_test_preds,                       # CatBoost予測
    np.maximum(lgbm_test_preds - 0.1, 0),     # LGBM正領域強調
    np.maximum(catboost_test_preds - 0.1, 0)  # CatBoost正領域強調
])

# NNLS係数で予測
blended_test_preds = predict_nnls_stack(test_features, nnls_coefs)

print(f"\nブレンド予測統計:")
print(f"  Mean: {blended_test_preds.mean():.4f}")
print(f"  Std: {blended_test_preds.std():.4f}")
print(f"  Min: {blended_test_preds.min():.4f}")
print(f"  Max: {blended_test_preds.max():.4f}")

# 予測値の分布を可視化
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

axes[0].hist(lgbm_test_preds, bins=50, alpha=0.7, color='blue', edgecolor='black')
axes[0].set_title('LightGBM予測分布', fontsize=14)
axes[0].set_xlabel('xAG予測値')
axes[0].set_ylabel('頻度')
axes[0].grid(alpha=0.3)

axes[1].hist(catboost_test_preds, bins=50, alpha=0.7, color='green', edgecolor='black')
axes[1].set_title('CatBoost予測分布', fontsize=14)
axes[1].set_xlabel('xAG予測値')
axes[1].set_ylabel('頻度')
axes[1].grid(alpha=0.3)

axes[2].hist(blended_test_preds, bins=50, alpha=0.7, color='purple', edgecolor='black')
axes[2].set_title('ブレンド予測分布', fontsize=14)
axes[2].set_xlabel('xAG予測値')
axes[2].set_ylabel('頻度')
axes[2].grid(alpha=0.3)

plt.tight_layout()
plt.show()

# モデル間の相関を確認
correlation = np.corrcoef(lgbm_test_preds, catboost_test_preds)[0, 1]
print(f"\nLGBMとCatBoostの予測相関: {correlation:.4f}")
print(f"(相関が低いほど、ブレンディングによる改善効果が大きい)")

# %% [markdown]
# ## [exp0031] 木モデル版MoE: Low/High専門家 + 温度ゲート
#
# **目的**: y<0.1を得意とするLow専門家と、y≥0.1を得意とするHigh専門家を学習し、校正済みゲート確率で温度制御されたソフト合成を行う。
#
# **手順**:
# 1. ゲート分類器（LGBM binary）でy≥0.1の確率をOOF推定→Isotonic校正
# 2. Low専門家（y<0.1のみ学習）とHigh専門家（y≥0.1のみ学習）をOOF学習
# 3. Soft合成（温度τ）とHard合成（閾値t）をOOFで最適化
# 4. ベスト設定でテスト推論・評価
#

# %% trusted=true
# [exp0031] Step 1: ゲート分類器（LGBM binary）をOOFで学習
print("=" * 80)
print("Step 1: ゲート分類器の学習（y≥0.1の判別）")
print("=" * 80)

from sklearn.metrics import roc_auc_score, average_precision_score
from sklearn.isotonic import IsotonicRegression

# バイナリラベルを作成（y >= 0.1 → 1, else → 0）
is_positive = (y_train.values >= 0.1).astype(int)
print(f"\n正例数: {is_positive.sum()} / {len(is_positive)} ({is_positive.mean()*100:.2f}%)")

# OOF用の配列を初期化
gate_oof_raw = np.zeros(len(X_train))
gate_test_raw = np.zeros(len(X_test))
gate_fold_scores = []

# Fold別にゲート分類器を学習
for fold in range(1, 6):
    print(f"\n=== ゲートFold {fold} ===")
    
    trn_mask = train_df["fold"] != fold
    val_mask = train_df["fold"] == fold
    
    X_tr = train_df.loc[trn_mask, all_features].copy()
    y_tr_gate = is_positive[trn_mask]
    X_val = train_df.loc[val_mask, all_features].copy()
    y_val_gate = is_positive[val_mask]
    
    # LGBMの分類器パラメータ
    gate_params = {
        'objective': 'binary',
        'metric': 'binary_logloss',  # カスタム評価を使用
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1,
        'random_state': SEED + fold,
        'scale_pos_weight': len(y_tr_gate) / y_tr_gate.sum() - 1  # 不均衡対応
    }
    
    # LightGBM Dataset作成
    train_gate = lgb.Dataset(X_tr, label=y_tr_gate)
    val_gate = lgb.Dataset(X_val, label=y_val_gate, reference=train_gate)
    
    # モデル学習
    gate_model = lgb.train(
        gate_params,
        train_gate,
        valid_sets=[val_gate],
        num_boost_round=500,
        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]
    )
    
    # OOF予測（確率）
    gate_oof_raw[val_mask] = gate_model.predict(X_val, num_iteration=gate_model.best_iteration)
    
    # テスト予測（確率）
    gate_test_raw += gate_model.predict(X_test[all_features], num_iteration=gate_model.best_iteration) / 5
    
    # Foldのスコア
    fold_auc = roc_auc_score(y_val_gate, gate_oof_raw[val_mask])
    fold_ap = average_precision_score(y_val_gate, gate_oof_raw[val_mask])
    gate_fold_scores.append((fold_auc, fold_ap))
    print(f"Fold {fold} - AUC: {fold_auc:.4f}, AP: {fold_ap:.4f}")

# 全体スコア
overall_auc = roc_auc_score(is_positive, gate_oof_raw)
overall_ap = average_precision_score(is_positive, gate_oof_raw)

print(f"\n{'='*80}")
print(f"ゲート分類器 OOF スコア:")
print(f"  AUC: {overall_auc:.4f}")
print(f"  AP: {overall_ap:.4f}")
print(f"{'='*80}")

# 予測の統計
print(f"\nOOF確率統計:")
print(f"  Mean: {gate_oof_raw.mean():.4f}")
print(f"  Std: {gate_oof_raw.std():.4f}")
print(f"  Min: {gate_oof_raw.min():.4f}")
print(f"  Max: {gate_oof_raw.max():.4f}")


# %% trusted=true
# [exp0031] Step 2: ゲート確率をIsotonicで校正
print("\n" + "=" * 80)
print("Step 2: ゲート確率の校正（Isotonic Regression）")
print("=" * 80)

# IsotonicRegressionで校正（OOFで学習）
iso_gate = IsotonicRegression(increasing=True, out_of_bounds="clip")
iso_gate.fit(gate_oof_raw, is_positive)

# 校正後の確率
gate_oof_cal = iso_gate.predict(gate_oof_raw)
gate_test_cal = iso_gate.predict(gate_test_raw)

# 校正前後の比較
print(f"\n校正前OOF確率:")
print(f"  Mean: {gate_oof_raw.mean():.4f}")
print(f"  正例の平均確率: {gate_oof_raw[is_positive == 1].mean():.4f}")
print(f"  負例の平均確率: {gate_oof_raw[is_positive == 0].mean():.4f}")

print(f"\n校正後OOF確率:")
print(f"  Mean: {gate_oof_cal.mean():.4f}")
print(f"  正例の平均確率: {gate_oof_cal[is_positive == 1].mean():.4f}")
print(f"  負例の平均確率: {gate_oof_cal[is_positive == 0].mean():.4f}")

# 校正曲線（10分位で確認）
print(f"\n校正曲線（10分位）:")
print(f"{'分位':<10} {'予測確率':<15} {'実際の正例率':<15} {'サンプル数':<10}")
print("-" * 50)

for i in range(10):
    lower = np.percentile(gate_oof_cal, i * 10)
    upper = np.percentile(gate_oof_cal, (i + 1) * 10)
    mask = (gate_oof_cal >= lower) & (gate_oof_cal <= upper)
    
    if mask.sum() > 0:
        pred_prob = gate_oof_cal[mask].mean()
        actual_rate = is_positive[mask].mean()
        count = mask.sum()
        print(f"{i+1:<10} {pred_prob:<15.4f} {actual_rate:<15.4f} {count:<10}")

print(f"\n校正の効果:")
print(f"  校正前 AUC: {overall_auc:.4f}")
print(f"  校正後 AUC: {roc_auc_score(is_positive, gate_oof_cal):.4f}")
print(f"  校正前 AP: {overall_ap:.4f}")
print(f"  校正後 AP: {average_precision_score(is_positive, gate_oof_cal):.4f}")


# %% trusted=true
# [exp0031] Step 3: Low専門家（y < 0.1のみ学習）
print("\n" + "=" * 80)
print("Step 3: Low専門家の学習（y < 0.1のみ）")
print("=" * 80)

# Low領域のマスク
low_mask = y_train.values < 0.1
print(f"\nLow領域サンプル数: {low_mask.sum()} / {len(y_train)} ({low_mask.mean()*100:.2f}%)")

# OOF用の配列を初期化
low_oof_preds = np.zeros(len(X_train))
low_test_preds = np.zeros(len(X_test))
low_fold_scores = []

# Fold別にLow専門家を学習
for fold in range(1, 6):
    print(f"\n=== Low専門家 Fold {fold} ===")
    
    trn_mask = (train_df["fold"] != fold) & low_mask
    val_mask = (train_df["fold"] == fold) & low_mask
    
    # Low領域のみ抽出
    X_tr_low = train_df.loc[trn_mask, all_features].copy()
    y_tr_low = y_train[trn_mask].copy()
    X_val_low = train_df.loc[val_mask, all_features].copy()
    y_val_low = y_train[val_mask].copy()
    
    print(f"Train Low: {len(X_tr_low)}, Val Low: {len(X_val_low)}")
    
    # サンプル重み（wRMSEと整合、ただしLow領域のみなので殆ど1.0）
    train_weights_low = make_sample_weight(y_tr_low)
    val_weights_low = make_sample_weight(y_val_low)
    
    # LGBMの回帰パラメータ（既存と同様）
    low_params = {
        'objective': 'regression',
        'metric': 'None',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1,
        'random_state': SEED + fold
    }
    
    # LightGBM Dataset作成
    train_low = lgb.Dataset(X_tr_low, label=y_tr_low, weight=train_weights_low)
    val_low = lgb.Dataset(X_val_low, label=y_val_low, weight=val_weights_low, reference=train_low)
    
    # モデル学習
    low_model = lgb.train(
        low_params,
        train_low,
        valid_sets=[val_low],
        feval=weighted_rmse_feval,
        num_boost_round=500,
        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]
    )
    
    # OOF予測（全trainに対して予測、Low領域のみ格納）
    low_oof_preds[val_mask] = low_model.predict(X_val_low, num_iteration=low_model.best_iteration)
    
    # テスト予測
    low_test_preds += low_model.predict(X_test[all_features], num_iteration=low_model.best_iteration) / 5
    
    # Foldのスコア（Low領域のみ）
    fold_score = weighted_rmse(y_val_low, low_oof_preds[val_mask])
    low_fold_scores.append(fold_score)
    print(f"Fold {fold} Low wRMSE: {fold_score:.6f}")

# 非負クリップ
low_oof_preds = np.clip(low_oof_preds, 0, None)
low_test_preds = np.clip(low_test_preds, 0, None)

# Low専門家の全体スコア（Low領域のみ）
low_overall_score = weighted_rmse(y_train[low_mask], low_oof_preds[low_mask])

print(f"\n{'='*80}")
print(f"Low専門家 OOF スコア（Low領域のみ）:")
print(f"  wRMSE: {low_overall_score:.6f}")
print(f"  各Fold: {[f'{s:.6f}' for s in low_fold_scores]}")
print(f"  Mean: {np.mean(low_fold_scores):.6f} ± {np.std(low_fold_scores):.6f}")
print(f"{'='*80}")

print(f"\nLow専門家OOF予測統計（Low領域のみ）:")
print(f"  Mean: {low_oof_preds[low_mask].mean():.4f}")
print(f"  Std: {low_oof_preds[low_mask].std():.4f}")
print(f"  Min: {low_oof_preds[low_mask].min():.4f}")
print(f"  Max: {low_oof_preds[low_mask].max():.4f}")


# %% trusted=true
# [exp0034] Step 4.5: High専門家のOptuna最適化
print("\n" + "=" * 80)
print("Step 4.5: High専門家のOptuna最適化")
print("=" * 80)

import optuna

# High領域のデータを準備（Fold1のみで最適化）
fold1_train_mask = train_df["fold"] != 1
fold1_val_mask = train_df["fold"] == 1


X_fold1_train = train_df.loc[fold1_train_mask, all_features]
y_fold1_train = y_train.loc[fold1_train_mask]
X_fold1_val   = train_df.loc[fold1_val_mask, all_features]
y_fold1_val   = y_train.loc[fold1_val_mask]

# High領域のマスク（閾値0.1）
threshold = 0.1
high_train_mask = y_fold1_train >= threshold
high_val_mask = y_fold1_val >= threshold

X_high_train = X_fold1_train[high_train_mask]
y_high_train = y_fold1_train[high_train_mask]
X_high_val = X_fold1_val[high_val_mask]
y_high_val = y_fold1_val[high_val_mask]

print(f"\nHigh領域のデータサイズ:")
print(f"  Train: {len(y_high_train)} samples ({high_train_mask.mean():.2%})")
print(f"  Val: {len(y_high_val)} samples ({high_val_mask.mean():.2%})")

# High領域の重み
w_high_train = np.where(y_high_train >= threshold, 5.0, 1.0)
w_high_val = np.where(y_high_val >= threshold, 5.0, 1.0)

# Optunaの目的関数
def objective_high(trial):
    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'verbosity': -1,
        'seed': 42,
        'deterministic': True,
        'force_row_wise': True,
        
        # 最適化対象パラメータ
        'num_leaves': trial.suggest_int('num_leaves', 10, 60),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),
        'min_child_samples': trial.suggest_int('min_child_samples', 20, 100),
        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 2.0),
        'feature_fraction': trial.suggest_float('feature_fraction', 0.7, 1.0),
        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.7, 1.0),
        'bagging_freq': 5,
    }
    
    # LightGBM Dataset作成
    train_data = lgb.Dataset(X_high_train, label=y_high_train, weight=w_high_train)
    val_data = lgb.Dataset(X_high_val, label=y_high_val, weight=w_high_val, reference=train_data)
    
    # 学習
    model = lgb.train(
        params,
        train_data,
        num_boost_round=1000,
        valid_sets=[val_data],
        callbacks=[
            lgb.early_stopping(stopping_rounds=50, verbose=False),
            lgb.log_evaluation(0)
        ]
    )
    
    # 予測とスコア計算
    preds = model.predict(X_high_val)
    score = weighted_rmse(y_high_val, preds)
    
    return score

# Optuna最適化実行
print(f"\nOptuna最適化を開始（50 trials）...")
study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))
study.optimize(objective_high, n_trials=50, show_progress_bar=True)

# 最適パラメータ
best_high_params = study.best_params.copy()
print(f"\n✅ High専門家の最適パラメータ:")
for key, value in best_high_params.items():
    print(f"  {key}: {value}")

print(f"\nBest trial: {study.best_trial.number}")
print(f"Best validation wRMSE: {study.best_value:.6f}")

# 最適パラメータをベースパラメータに統合
high_expert_params = {
    'objective': 'regression',
    'metric': 'rmse',
    'boosting_type': 'gbdt',
    'verbosity': -1,
    'seed': 42,
    'deterministic': True,
    'force_row_wise': True,
    'bagging_freq': 5,
}
high_expert_params.update(best_high_params)

print(f"\n✅ High専門家の最終パラメータ準備完了")


# %% trusted=true
# [exp0031] Step 4: High専門家（y >= 0.1のみ学習）
print("\n" + "=" * 80)
print("Step 4: High専門家の学習（y >= 0.1のみ）")
print("=" * 80)

# High領域のマスク
high_mask = y_train.values >= 0.1
print(f"\nHigh領域サンプル数: {high_mask.sum()} / {len(y_train)} ({high_mask.mean()*100:.2f}%)")

# OOF用の配列を初期化
high_oof_preds = np.zeros(len(X_train))
high_test_preds = np.zeros(len(X_test))
high_fold_scores = []

# Fold別にHigh専門家を学習
for fold in range(1, 6):
    print(f"\n=== High専門家 Fold {fold} ===")
    
    trn_mask = (train_df["fold"] != fold) & high_mask
    val_mask = (train_df["fold"] == fold) & high_mask
    
    # High領域のみ抽出
    X_tr_high = train_df.loc[trn_mask, all_features].copy()
    y_tr_high = y_train[trn_mask].copy()
    X_val_high = train_df.loc[val_mask, all_features].copy()
    y_val_high = y_train[val_mask].copy()
    
    print(f"Train High: {len(X_tr_high)}, Val High: {len(X_val_high)}")
    
    # サンプル重み（wRMSEと整合、High領域は殆ど5.0）
    train_weights_high = make_sample_weight(y_tr_high)
    val_weights_high = make_sample_weight(y_val_high)
    
    # LGBMの回帰パラメータ（既存と同様）
    high_params = {
        'objective': 'regression',
        'metric': 'None',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1,
        'random_state': SEED + fold + 100  # Low専門家と異なるseed
    }
    
    # LightGBM Dataset作成
    train_high = lgb.Dataset(X_tr_high, label=y_tr_high, weight=train_weights_high)
    val_high = lgb.Dataset(X_val_high, label=y_val_high, weight=val_weights_high, reference=train_high)
    
    # モデル学習
    high_model = lgb.train(
        high_params,
        train_high,
        valid_sets=[val_high],
        feval=weighted_rmse_feval,
        num_boost_round=500,
        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]
    )
    
    # OOF予測（全trainに対して予測、High領域のみ格納）
    high_oof_preds[val_mask] = high_model.predict(X_val_high, num_iteration=high_model.best_iteration)
    if fold == 1:
        high_feature_importance = pd.DataFrame({
            'feature': all_features,
            'importance': high_model.feature_importance(importance_type='gain')
        }).sort_values('importance', ascending=False)
        print("\n" + "="*80)
        print("High専門家の特徴重要度 Top 30 (Fold 1)")
        print("="*80)
        print(high_feature_importance.head(30).to_string(index=False))
        print("="*80 + "\n")
        
    # テスト予測
    high_test_preds += high_model.predict(X_test[all_features], num_iteration=high_model.best_iteration) / 5
    
    # Foldのスコア（High領域のみ）
    fold_score = weighted_rmse(y_val_high, high_oof_preds[val_mask])
    high_fold_scores.append(fold_score)
    print(f"Fold {fold} High wRMSE: {fold_score:.6f}")

# 非負クリップ
high_oof_preds = np.clip(high_oof_preds, 0, None)
high_test_preds = np.clip(high_test_preds, 0, None)

# High専門家の全体スコア（High領域のみ）
high_overall_score = weighted_rmse(y_train[high_mask], high_oof_preds[high_mask])

print(f"\n{'='*80}")
print(f"High専門家 OOF スコア（High領域のみ）:")
print(f"  wRMSE: {high_overall_score:.6f}")
print(f"  各Fold: {[f'{s:.6f}' for s in high_fold_scores]}")
print(f"  Mean: {np.mean(high_fold_scores):.6f} ± {np.std(high_fold_scores):.6f}")
print(f"{'='*80}")

print(f"\nHigh専門家OOF予測統計（High領域のみ）:")
print(f"  Mean: {high_oof_preds[high_mask].mean():.4f}")
print(f"  Std: {high_oof_preds[high_mask].std():.4f}")
print(f"  Min: {high_oof_preds[high_mask].min():.4f}")
print(f"  Max: {high_oof_preds[high_mask].max():.4f}")


# %% trusted=true
# [exp0031] Step 5: MoE合成の最適化（Soft vs Hard）
print("\n" + "=" * 80)
print("Step 5: MoE合成の最適化")
print("=" * 80)

# Soft合成関数: y_moe = g^τ * y_high + (1 - g^τ) * y_low
def soft_moe_blend(gate_prob, y_low, y_high, tau=1.0):
    """温度τでゲート確率を調整したソフト合成"""
    gate_adj = np.power(gate_prob, tau)
    return gate_adj * y_high + (1 - gate_adj) * y_low

# Hard合成関数: y_moe = (g >= t) * y_high + (g < t) * y_low
def hard_moe_blend(gate_prob, y_low, y_high, threshold=0.5):
    """閾値でゲートをハード判定して合成"""
    gate_hard = (gate_prob >= threshold).astype(float)
    return gate_hard * y_high + (1 - gate_hard) * y_low

# Soft合成の温度τを探索
print("\n【Soft合成の温度τ最適化】")
tau_candidates = [0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 2.0]
soft_results = []

for tau in tau_candidates:
    moe_oof = soft_moe_blend(gate_oof_cal, low_oof_preds, high_oof_preds, tau=tau)
    moe_oof = np.clip(moe_oof, 0, None)  # 非負クリップ
    score = weighted_rmse(y_train.values, moe_oof)
    soft_results.append((tau, score))
    print(f"  τ={tau:.2f}: wRMSE={score:.6f}")

# ベストτを取得
best_tau, best_soft_score = min(soft_results, key=lambda x: x[1])
print(f"\n  ✅ ベスト τ={best_tau:.2f}, wRMSE={best_soft_score:.6f}")

# Hard合成の閾値tを探索
print("\n【Hard合成の閾値t最適化】")
threshold_candidates = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]
hard_results = []

for t in threshold_candidates:
    moe_oof = hard_moe_blend(gate_oof_cal, low_oof_preds, high_oof_preds, threshold=t)
    moe_oof = np.clip(moe_oof, 0, None)
    score = weighted_rmse(y_train.values, moe_oof)
    hard_results.append((t, score))
    print(f"  t={t:.2f}: wRMSE={score:.6f}")

# ベストtを取得
best_threshold, best_hard_score = min(hard_results, key=lambda x: x[1])
print(f"\n  ✅ ベスト t={best_threshold:.2f}, wRMSE={best_hard_score:.6f}")

# Soft vs Hard 比較
print(f"\n{'='*80}")
print(f"【Soft vs Hard 比較】")
print(f"  Soft合成（τ={best_tau:.2f}）: wRMSE={best_soft_score:.6f}")
print(f"  Hard合成（t={best_threshold:.2f}）: wRMSE={best_hard_score:.6f}")

if best_soft_score < best_hard_score:
    print(f"\n  ✅ Soft合成がベスト（差分: {best_hard_score - best_soft_score:.6f}）")
    use_soft = True
else:
    print(f"\n  ✅ Hard合成がベスト（差分: {best_soft_score - best_hard_score:.6f}）")
    use_soft = False
print(f"{'='*80}")

# ベスト設定でOOF予測を作成
if use_soft:
    moe_oof_preds = soft_moe_blend(gate_oof_cal, low_oof_preds, high_oof_preds, tau=best_tau)
    moe_test_preds = soft_moe_blend(gate_test_cal, low_test_preds, high_test_preds, tau=best_tau)
    blend_method = f"Soft(τ={best_tau:.2f})"
else:
    moe_oof_preds = hard_moe_blend(gate_oof_cal, low_oof_preds, high_oof_preds, threshold=best_threshold)
    moe_test_preds = hard_moe_blend(gate_test_cal, low_test_preds, high_test_preds, threshold=best_threshold)
    blend_method = f"Hard(t={best_threshold:.2f})"

# 非負クリップ
moe_oof_preds = np.clip(moe_oof_preds, 0, None)
moe_test_preds = np.clip(moe_test_preds, 0, None)

# 最終OOFスコア
final_moe_score = weighted_rmse(y_train.values, moe_oof_preds)
print(f"\n【最終MoE OOFスコア】 {blend_method}")
print(f"  wRMSE: {final_moe_score:.6f}")

# Fold別スコアを計算
moe_fold_scores = []
for fold in range(1, 6):
    fold_mask = train_df["fold"] == fold
    fold_score = weighted_rmse(y_train[fold_mask], moe_oof_preds[fold_mask])
    moe_fold_scores.append(fold_score)
    print(f"  Fold {fold}: {fold_score:.6f}")

print(f"\n  Mean: {np.mean(moe_fold_scores):.6f} ± {np.std(moe_fold_scores):.6f}")


# %% trusted=true
# [exp0028] Isotonic校正: 正領域（y >= 0.1）のみを校正
print("=" * 80)
print("Isotonic校正を適用中...")
print("=" * 80)

from scripts.calibration import fit_isotonic_positive, apply_isotonic_positive

# OOF予測で校正モデルを学習
print("\n1. OOF予測でIsotonicモデルを学習中...")
iso_model = fit_isotonic_positive(
    y_true=y_train.values,
    y_pred=blended_oof_preds,  # ブレンド後のOOF予測
    threshold=0.1,
    pos_weight=5.0
)

# OOF予測を校正して評価
print("\n2. OOF予測を校正中...")
calibrated_oof_preds = apply_isotonic_positive(
    y_pred=blended_oof_preds,
    iso_model=iso_model,
    threshold=0.1
)

# 校正前後のスコア比較
before_rmse = weighted_rmse(y_train.values, blended_oof_preds)
after_rmse = weighted_rmse(y_train.values, calibrated_oof_preds)

print(f"\n【校正効果】")
print(f"  校正前 wRMSE: {before_rmse:.6f}")
print(f"  校正後 wRMSE: {after_rmse:.6f}")
print(f"  改善量: {before_rmse - after_rmse:.6f} ({(before_rmse - after_rmse) / before_rmse * 100:.2f}%)")

# 正領域・負領域別の分析
from sklearn.metrics import mean_squared_error

pos_mask = y_train.values >= 0.1
neg_mask = y_train.values < 0.1

print(f"\n【正領域（y >= 0.1）】")
print(f"  サンプル数: {pos_mask.sum()}")
print(f"  校正前 RMSE: {np.sqrt(mean_squared_error(y_train[pos_mask], blended_oof_preds[pos_mask])):.6f}")
print(f"  校正後 RMSE: {np.sqrt(mean_squared_error(y_train[pos_mask], calibrated_oof_preds[pos_mask])):.6f}")

print(f"\n【負領域（y < 0.1）】")
print(f"  サンプル数: {neg_mask.sum()}")
print(f"  校正前 RMSE: {np.sqrt(mean_squared_error(y_train[neg_mask], blended_oof_preds[neg_mask])):.6f}")
print(f"  校正後 RMSE: {np.sqrt(mean_squared_error(y_train[neg_mask], calibrated_oof_preds[neg_mask])):.6f}")

# 3. テストデータに適用
print("\n3. テストデータに校正を適用中...")
calibrated_test_preds = apply_isotonic_positive(
    y_pred=blended_test_preds,
    iso_model=iso_model,
    threshold=0.1
)

print(f"\n【テスト予測の統計】")
print(f"  校正前:")
print(f"    Mean: {blended_test_preds.mean():.6f}")
print(f"    Std:  {blended_test_preds.std():.6f}")
print(f"    Min:  {blended_test_preds.min():.6f}")
print(f"    Max:  {blended_test_preds.max():.6f}")
print(f"  校正後:")
print(f"    Mean: {calibrated_test_preds.mean():.6f}")
print(f"    Std:  {calibrated_test_preds.std():.6f}")
print(f"    Min:  {calibrated_test_preds.min():.6f}")
print(f"    Max:  {calibrated_test_preds.max():.6f}")

# 4. ブレンド予測を校正版に置き換え
blended_test_preds = calibrated_test_preds
blended_oof_preds = calibrated_oof_preds

print("\n✅ Isotonic校正完了")



# %% trusted=true
# [exp0034] Step 5.5: ゼロ閾値最適化（分布補正）

print("\n" + "=" * 80)
print("Step 5.5: ゼロ閾値最適化（予測分布の補正）")
print("=" * 80)

# 訓練データの真のゼロ率を確認
true_zero_rate = (y_train.values == 0).mean()
print(f"\n真のゼロ率: {true_zero_rate:.4f} ({(y_train.values == 0).sum()} / {len(y_train)})")

# MoE予測の小さい値の分布を確認
print(f"\nMoE予測の低値域分布:")
for thresh in [0.0, 0.02, 0.04, 0.06, 0.08, 0.10]:
    count = (moe_oof_preds < thresh).sum()
    rate = count / len(moe_oof_preds)
    print(f"  < {thresh:.2f}: {count:5d} ({rate:.4f})")

# ゼロ閾値の探索（wRMSEを最小化する閾値を探す）
print(f"\nゼロ閾値の最適化:")
zero_threshold_candidates = [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.10]
best_zero_threshold = 0.0
best_zero_score = final_moe_score

for zero_thresh in zero_threshold_candidates:
    # 閾値以下を0に補正
    moe_oof_corrected = moe_oof_preds.copy()
    moe_oof_corrected[moe_oof_corrected < zero_thresh] = 0.0
    
    # スコア計算
    score = weighted_rmse(y_train.values, moe_oof_corrected)
    
    # ゼロ率
    zero_rate = (moe_oof_corrected == 0).mean()
    
    print(f"  閾値={zero_thresh:.2f}: wRMSE={score:.6f}, ゼロ率={zero_rate:.4f} (改善: {final_moe_score - score:+.6f})")
    
    if score < best_zero_score:
        best_zero_score = score
        best_zero_threshold = zero_thresh

print(f"\n✅ 最適ゼロ閾値: {best_zero_threshold:.2f}")
print(f"   補正前 wRMSE: {final_moe_score:.6f}")
print(f"   補正後 wRMSE: {best_zero_score:.6f}")
print(f"   改善量: {final_moe_score - best_zero_score:.6f}")

# 最適閾値で予測を補正
moe_oof_preds_original = moe_oof_preds.copy()  # 元の予測を保存
moe_test_preds_original = moe_test_preds.copy()

moe_oof_preds[moe_oof_preds < best_zero_threshold] = 0.0
moe_test_preds[moe_test_preds < best_zero_threshold] = 0.0

# 補正後の最終スコア
final_moe_score = best_zero_score

# 補正後のFold別スコア
print(f"\n補正後のFold別スコア:")
moe_fold_scores = []
for fold in range(1, 6):
    fold_mask = train_df["fold"] == fold
    fold_score = weighted_rmse(y_train[fold_mask], moe_oof_preds[fold_mask])
    moe_fold_scores.append(fold_score)
    print(f"  Fold {fold}: {fold_score:.6f}")

print(f"\n  Mean: {np.mean(moe_fold_scores):.6f} ± {np.std(moe_fold_scores):.6f}")

# 補正後の予測分布
print(f"\n補正後の予測統計:")
print(f"  ゼロ率: {(moe_oof_preds == 0).mean():.4f}")
print(f"  Mean: {moe_oof_preds.mean():.4f}")
print(f"  Std: {moe_oof_preds.std():.4f}")
print(f"  Min: {moe_oof_preds.min():.4f}")
print(f"  Max: {moe_oof_preds.max():.4f}")

# %% trusted=true
# [exp0034] Step 6: ベースラインとの比較・メトリクス保存

print("\n" + "=" * 80)
print("Step 6: ベースラインとの比較・メトリクス保存")
print("=" * 80)

# 既存ベースラインのスコア（NNLS/Isotonic校正）
lgbm_oof_score = weighted_rmse(y_train.values, oof_preds)  # LGBM単体
nnls_oof_score = weighted_rmse(y_train.values, blended_oof_preds)  # NNLS統合

# calibrated_oof_predsが存在する場合のみ使用、なければblended_oof_predsを使用
try:
    iso_oof_score = weighted_rmse(y_train.values, calibrated_oof_preds)  # Isotonic校正後
except NameError:
    iso_oof_score = nnls_oof_score  # まだ校正されていない場合はNNLSスコアを使用
    print("警告: calibrated_oof_predsが未定義のため、Isotonic校正スコアをNNLSスコアで代用")

print(f"\n【ベースライン比較】")
print(f"  LGBM単体:         {lgbm_oof_score:.6f}")
print(f"  NNLS統合:         {nnls_oof_score:.6f}")
print(f"  Isotonic校正:     {iso_oof_score:.6f}")
print(f"  MoE ({blend_method}): {final_moe_score:.6f}")

print(f"\n【改善量】")
print(f"  vs LGBM単体:      {lgbm_oof_score - final_moe_score:+.6f}")
print(f"  vs NNLS統合:      {nnls_oof_score - final_moe_score:+.6f}")
print(f"  vs Isotonic校正:  {iso_oof_score - final_moe_score:+.6f}")

# メトリクスをJSON形式で保存
moe_metrics = {
    "exp_id": "exp0034_moe_tree",
    "method": blend_method,
    "gate_auc": float(overall_auc),
    "gate_ap": float(overall_ap),
    "best_tau": float(best_tau) if use_soft else None,
    "best_threshold": float(best_threshold) if not use_soft else None,
    "cv_scores": [float(s) for s in moe_fold_scores],
    "cv_mean": float(np.mean(moe_fold_scores)),
    "cv_std": float(np.std(moe_fold_scores)),
    "oof_score": float(final_moe_score),
    "low_expert_score": float(low_overall_score),
    "high_expert_score": float(high_overall_score),
    "baseline_lgbm": float(lgbm_oof_score),
    "baseline_nnls": float(nnls_oof_score),
    "baseline_isotonic": float(iso_oof_score),
    "improvement_vs_lgbm": float(lgbm_oof_score - final_moe_score),
    "improvement_vs_nnls": float(nnls_oof_score - final_moe_score),
    "improvement_vs_isotonic": float(iso_oof_score - final_moe_score)
}

# JSONファイルに保存
from pathlib import Path
import json

log_dir = Path("logs")
log_dir.mkdir(exist_ok=True)
metrics_file = log_dir / "host_moe_high_optuna_001_metrics.json"

with open(metrics_file, "w") as f:
    json.dump(moe_metrics, f, indent=2, ensure_ascii=False)

print(f"\n✅ メトリクスを保存: {metrics_file}")

# OOF詳細を保存
artifacts_dir = Path("artifacts")
artifacts_dir.mkdir(exist_ok=True)

oof_detail = pd.DataFrame({
    "match_id": train_df["match_id"],
    "player_id": train_df["player_id"],
    "y_true": y_train.values,
    "fold": train_df["fold"],
    "gate_raw": gate_oof_raw,
    "gate_cal": gate_oof_cal,
    "low_pred": low_oof_preds,
    "high_pred": high_oof_preds,
    "moe_pred": moe_oof_preds
})

oof_file = artifacts_dir / "oof_predictions_moe.csv"
oof_detail.to_csv(oof_file, index=False)
print(f"✅ OOF詳細を保存: {oof_file}")

print(f"\n{'='*80}")
print(f"MoE実装完了！")
print(f"{'='*80}")


# %% trusted=true
# [exp0034] Step 7: 提出ファイル作成

print("\n" + "=" * 80)
print("Step 7: 提出ファイル作成")
print("=" * 80)

# テスト予測の統計
print(f"\nMoEテスト予測統計:")
print(f"  Mean: {moe_test_preds.mean():.4f}")
print(f"  Std: {moe_test_preds.std():.4f}")
print(f"  Min: {moe_test_preds.min():.4f}")
print(f"  Max: {moe_test_preds.max():.4f}")

# 提出ファイルの作成
submission_dir = Path("submissions")
submission_dir.mkdir(exist_ok=True)

moe_submission = pd.DataFrame({
    "match_id": test_df["match_id"],
    "player_id": test_df["player_id"],
    "xAG": moe_test_preds
})

submission_file = submission_dir / "host_moe_high_optuna_001_submissions.csv"
moe_submission.to_csv(submission_file, index=False)
print(f"\n✅ 提出ファイルを保存: {submission_file}")

# 予測分布の可視化
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# OOF予測
axes[0].hist(moe_oof_preds, bins=50, alpha=0.7, color='purple', edgecolor='black')
axes[0].set_title(f'MoE OOF予測分布\n(wRMSE: {final_moe_score:.6f})', fontsize=14)
axes[0].set_xlabel('xAG予測値')
axes[0].set_ylabel('頻度')
axes[0].grid(alpha=0.3)

# テスト予測
axes[1].hist(moe_test_preds, bins=50, alpha=0.7, color='orange', edgecolor='black')
axes[1].set_title('MoEテスト予測分布', fontsize=14)
axes[1].set_xlabel('xAG予測値')
axes[1].set_ylabel('頻度')
axes[1].grid(alpha=0.3)

# ゲート確率の分布
axes[2].hist(gate_oof_cal, bins=50, alpha=0.7, color='blue', edgecolor='black')
axes[2].set_title(f'ゲート確率分布（校正済み）\n(AUC: {overall_auc:.4f})', fontsize=14)
axes[2].set_xlabel('P(y >= 0.1)')
axes[2].set_ylabel('頻度')
axes[2].grid(alpha=0.3)

plt.tight_layout()
plt.show()

print(f"\n{'='*80}")
print(f"exp0034 完了！")
print(f"  Method: {blend_method}")
print(f"  OOF wRMSE: {final_moe_score:.6f}")
print(f"  Metrics: {metrics_file}")
print(f"  Submission: {submission_file}")
print(f"{'='*80}")


# %% [markdown] id="Sap_9i9DaIf3"
# ## テストデータに対する推論

# %% colab={"base_uri": "https://localhost:8080/"} id="3Xs1lGqfaIf4" outputId="47eceb84-34b8-480a-dfd5-85eebb203bc7" trusted=true
# アンサンブル予測（5モデルの平均） on Test Data
test_preds = np.zeros(len(X_test))

for model in models:
    pred = model.predict(X_test, num_iteration=model.best_iteration)
    test_preds += pred

test_preds /= len(models)


print(f"\n=== Test Set Predictions ===")
print(f"予測xAG範囲: {test_preds.min():.3f} 〜 {test_preds.max():.3f}")

# test_dfに予測結果を追加
test_df['predicted_xAG'] = test_preds

# %% [markdown] id="yL_4w1qWaIf4"
# ## 予測結果の分析

# %% colab={"base_uri": "https://localhost:8080/", "height": 998} id="KLKnE3KajdZp" outputId="d9b9cad3-dedc-458d-aad3-e23986d7e5a3" trusted=true
# 特徴量重要度の平均計算
feature_importance_mean = feature_importance.groupby('feature')['importance'].agg(['mean', 'std']).reset_index()
feature_importance_mean = feature_importance_mean.sort_values('mean', ascending=False)

# 可視化
plt.figure(figsize=(10, 8))
sns.barplot(data=feature_importance_mean.head(15), x='mean', y='feature')
plt.title('Top 15 Feature Importance (Weighted RMSE Baseline xAG Model)')
plt.xlabel('Importance')
plt.tight_layout()
plt.show()

print("特徴量重要度 Top 10:")
print(feature_importance_mean.head(10))

# %% colab={"base_uri": "https://localhost:8080/", "height": 607} id="kGIeBEIPaIf4" outputId="92e7fb77-a19e-45f3-c778-c010f56a2adf" trusted=true
# train, test予測値の分布を可視化
plt.figure(figsize=(8, 6))
sns.histplot(oof_preds, stat='density', kde=True, alpha=0.2, label='OOF予測', linewidth=0)
sns.histplot(test_preds, stat='density', kde=True, alpha=0.2, label='Test予測', linewidth=0)

# train正解値の分布を可視化
vc = y_train.value_counts().sort_index()
heights = vc / vc.sum() / 0.1 # 棒グラフの高さをdensity に合わせる
plt.bar(vc.index, heights, width=0.03, alpha=0.6, label='OOF正解', align='center')

plt.xlabel('xAG')
plt.ylabel('密度')
plt.xlim(0, 1)
plt.title('xAG予測値の分布（OOF予測 vs Test予測 vs OOF正解）')
plt.legend()
plt.tight_layout()
plt.show()

# %% [markdown] id="61-ufcOruCGb"
# 正解が0.0のデータの重み付けが小さいため、全体的に正の値を予想する傾向が見られる。

# %% [markdown] id="u4I2O1ntaIf4"
# ## 提出ファイル作成

# %% id="cbhq5ARAaIf4" trusted=true
# ブレンドモデルの予測で提出ファイルを作成
submission_df['xAG'] = blended_test_preds

# 提出ファイル保存
submission_path = log_dir / 'submission_blend_lgbm_catboost.csv'
submission_df.to_csv(submission_path, index=False)

print(f"提出ファイルを保存しました: {submission_path}")
print(f"\n提出ファイルの統計:")
print(submission_df['xAG'].describe())

# 個別モデルの提出ファイルも保存
submission_lgbm = submission_df.copy()
submission_lgbm['xAG'] = lgbm_test_preds
submission_lgbm.to_csv(log_dir / 'submission_lgbm_only.csv', index=False)
print(f"\nLightGBM単独の提出ファイルも保存: {log_dir / 'submission_lgbm_only.csv'}")

submission_catboost = submission_df.copy()
submission_catboost['xAG'] = catboost_test_preds
submission_catboost.to_csv(log_dir / 'submission_catboost_only.csv', index=False)
print(f"CatBoost単独の提出ファイルも保存: {log_dir / 'submission_catboost_only.csv'}")

# サマリー表示
print("\n=== 最終結果サマリー ===")
print(f"LightGBM OOF RMSE: {oof_score:.4f}")
print(f"CatBoost OOF RMSE: {catboost_oof_score:.4f}")
print(f"ブレンドOOF RMSE: {best_blend_score:.4f}")
print(f"\nNNLSスタッキング係数:")
for name, coef in zip(feature_names, nnls_coefs):
    print(f"  {name:15s}: {coef:.6f}")
print(f"\n比較: グリッドサーチの最適比率:")
print(f"  LightGBM: {best_lgbm_weight:.2f}")
print(f"  CatBoost: {best_catboost_weight:.2f}")
print(f"\n提出ファイル:")
print(f"  1. submission_blend_lgbm_catboost.csv (NNLS推奨)")
print(f"  2. submission_lgbm_only.csv")
print(f"  3. submission_catboost_only.csv")


# %% trusted=true

# %% trusted=true

# %% trusted=true

# %% trusted=true

# %% trusted=true
