# exp0034: Highå°‚é–€å®¶Optunaæœ€é©åŒ–ç‰ˆMoE

## ğŸ¯ å®Ÿé¨“æ¦‚è¦

**ææ¡ˆ**: Highå°‚é–€å®¶ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’Optunaæœ€é©åŒ–ã—ã€MoEå…¨ä½“ã®ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹

**ãƒ™ãƒ¼ã‚¹**: exp0033ï¼ˆã‚¼ãƒ­é–¾å€¤æœ€é©åŒ–ç‰ˆMoEï¼‰ã‹ã‚‰æ´¾ç”Ÿ

**èª²é¡Œ**: exp0031ã§Highå°‚é–€å®¶ã®RMSE=0.392ã¨æ€§èƒ½ãŒä½ãã€Lowå°‚é–€å®¶ã¨åŒã˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã„ãŸ

---

## ğŸ”¬ æ”¹å–„æ‰‹æ³•

### Highå°‚é–€å®¶å°‚ç”¨ã®Optunaæœ€é©åŒ–

#### å•é¡Œ
- **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æœªæœ€é©åŒ–**: Low/Highå°‚é–€å®¶ãŒåŒã˜ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
- **Highé ˜åŸŸã®ç‰¹æ€§**: ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªãï¼ˆç´„30%ï¼‰ã€éå­¦ç¿’ã—ã‚„ã™ã„
- **æ€§èƒ½ã®ä½ã•**: Highå°‚é–€å®¶ã®RMSE=0.392ã§æ”¹å–„ä½™åœ°ãŒå¤§ãã„

#### è§£æ±ºç­–
```python
# 1. Highé ˜åŸŸã®ãƒ‡ãƒ¼ã‚¿ã§æœ€é©åŒ–ï¼ˆFold1ã®ã¿ï¼‰
X_high_train = X_fold1_train[y_fold1_train >= 0.1]
y_high_train = y_fold1_train[y_fold1_train >= 0.1]

# 2. Optunaã§æœ€é©åŒ–
def objective_high(trial):
    params = {
        'num_leaves': trial.suggest_int('num_leaves', 10, 60),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),
        'min_child_samples': trial.suggest_int('min_child_samples', 20, 100),
        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 2.0),
        'feature_fraction': trial.suggest_float('feature_fraction', 0.7, 1.0),
        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.7, 1.0),
    }
    # ... å­¦ç¿’ãƒ»è©•ä¾¡
    return weighted_rmse(y_high_val, preds)

study.optimize(objective_high, n_trials=50)

# 3. æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§Highå°‚é–€å®¶ã‚’å­¦ç¿’
high_expert_params.update(study.best_params)
```

#### æœ€é©åŒ–å¯¾è±¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- `num_leaves`: æœ¨ã®è¤‡é›‘ã•ï¼ˆ10ã€œ60ï¼‰
- `learning_rate`: å­¦ç¿’ç‡ï¼ˆ0.01ã€œ0.1ã€å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
- `min_child_samples`: ãƒãƒ¼ãƒ‰ã®æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆ20ã€œ100ã€éå­¦ç¿’æŠ‘åˆ¶ï¼‰
- `reg_alpha`: L1æ­£å‰‡åŒ–ï¼ˆ0.0ã€œ1.0ï¼‰
- `reg_lambda`: L2æ­£å‰‡åŒ–ï¼ˆ0.0ã€œ2.0ï¼‰
- `feature_fraction`: ç‰¹å¾´é‡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆ0.7ã€œ1.0ï¼‰
- `bagging_fraction`: ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆ0.7ã€œ1.0ï¼‰

#### æœŸå¾…åŠ¹æœ
1. **Highå°‚é–€å®¶ã®æ€§èƒ½å‘ä¸Š**: RMSE 0.392 â†’ 0.25ï¼ˆç›®æ¨™ï¼‰
2. **MoEå…¨ä½“ã®æ”¹å–„**: OOF 0.2271 â†’ 0.22ä»¥ä¸‹
3. **éå­¦ç¿’ã®æŠ‘åˆ¶**: Highé ˜åŸŸã«é©ã—ãŸæ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

---

## ğŸ“Š æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ

### ã‚¹ã‚³ã‚¢æ”¹å–„ç›®æ¨™
- **exp0031 Highå°‚é–€å®¶**: wRMSE 0.392
- **exp0034 Highå°‚é–€å®¶ç›®æ¨™**: 0.25ï¼ˆ-0.142æ”¹å–„ï¼‰
- **exp0031 MoE OOF**: 0.2271
- **exp0034 MoE OOFç›®æ¨™**: 0.22ï¼ˆ-0.007æ”¹å–„ï¼‰

### æ”¹å–„ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 
1. **Highé ˜åŸŸå°‚ç”¨æœ€é©åŒ–**: Highé ˜åŸŸã®ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã«åˆã‚ã›ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
2. **æ­£å‰‡åŒ–ã®é©æ­£åŒ–**: ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªã„Highé ˜åŸŸã«æœ€é©ãªæ­£å‰‡åŒ–
3. **ã‚¼ãƒ­é–¾å€¤ã¨ã®ç›¸ä¹—åŠ¹æœ**: exp0033ã®ã‚¼ãƒ­é–¾å€¤æœ€é©åŒ– + Highå°‚é–€å®¶æ”¹å–„

---

## ğŸ“ å®Ÿè£…ã®è©³ç´°

### å®Ÿè£…ãƒ•ãƒ­ãƒ¼

#### Step 4.5ï¼ˆæ–°è¦ï¼‰: Highå°‚é–€å®¶ã®Optunaæœ€é©åŒ–
```python
# Fold1ã§Highé ˜åŸŸã®æœ€é©åŒ–
X_high_train = X_fold1_train[y_fold1_train >= 0.1]
y_high_train = y_fold1_train[y_fold1_train >= 0.1]

# Optunaæœ€é©åŒ–ï¼ˆ50 trialsï¼‰
study = optuna.create_study(direction='minimize')
study.optimize(objective_high, n_trials=50)

# æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—
best_high_params = study.best_params
high_expert_params.update(best_high_params)
```

#### Step 4ï¼ˆæ›´æ–°ï¼‰: Highå°‚é–€å®¶ã®å­¦ç¿’
```python
# high_expert_paramsã‚’ä½¿ç”¨ï¼ˆOptunaæœ€é©åŒ–æ¸ˆã¿ï¼‰
high_model = lgb.train(high_expert_params, train_high, ...)
```

#### Step 5.5: ã‚¼ãƒ­é–¾å€¤æœ€é©åŒ–ï¼ˆexp0033ã‹ã‚‰ç¶™æ‰¿ï¼‰
```python
# MoEäºˆæ¸¬ã«ã‚¼ãƒ­é–¾å€¤ã‚’é©ç”¨
moe_oof_preds[moe_oof_preds < best_zero_threshold] = 0.0
moe_test_preds[moe_test_preds < best_zero_threshold] = 0.0
```

---

## ğŸ” æ¤œè¨¼ãƒã‚¤ãƒ³ãƒˆ

### Highå°‚é–€å®¶ã®æ”¹å–„ç¢ºèª
- [ ] Optunaæœ€é©åŒ–ãŒåæŸã—ã¦ã„ã‚‹ã‹ï¼ˆ50 trialsã§ååˆ†ã‹ï¼‰
- [ ] æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¦¥å½“ãªç¯„å›²ã‹
- [ ] Highå°‚é–€å®¶ã®OOF wRMSEãŒexp0031æ¯”ã§æ”¹å–„ã—ã¦ã„ã‚‹ã‹

### MoEå…¨ä½“ã®æ”¹å–„ç¢ºèª
- [ ] MoE OOF wRMSEãŒexp0031/exp0033æ¯”ã§æ”¹å–„ã—ã¦ã„ã‚‹ã‹
- [ ] ã‚²ãƒ¼ãƒˆåˆ†é›¢ç²¾åº¦ï¼ˆAUC/APï¼‰ãŒç¶­æŒã•ã‚Œã¦ã„ã‚‹ã‹
- [ ] ã‚¼ãƒ­é–¾å€¤æœ€é©åŒ–ã¨ã®ç›¸ä¹—åŠ¹æœãŒã‚ã‚‹ã‹

### äºˆæ¸¬ã®å¥å…¨æ€§
- [ ] OOFäºˆæ¸¬ã®åˆ†å¸ƒãŒè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨é¡ä¼¼ã—ã¦ã„ã‚‹ã‹
- [ ] ãƒ†ã‚¹ãƒˆäºˆæ¸¬ã®çµ±è¨ˆé‡ãŒå¦¥å½“ã‹
- [ ] Foldåˆ¥ã‚¹ã‚³ã‚¢ã®ã°ã‚‰ã¤ããŒå‰Šæ¸›ã•ã‚Œã¦ã„ã‚‹ã‹

---

## ğŸ“‚ æˆæœç‰©

- `training_with_high_optuna.ipynb`: Highå°‚é–€å®¶Optunaæœ€é©åŒ–ç‰ˆMoEå®Ÿè£…
- `logs/host_moe_high_optuna_001_metrics.json`: ãƒ¡ãƒˆãƒªã‚¯ã‚¹
- `artifacts/oof_predictions_moe_high_optuna.csv`: OOFè©³ç´°
- `submissions/host_moe_high_optuna_001_submission.csv`: æå‡ºãƒ•ã‚¡ã‚¤ãƒ«

---

## ğŸš€ å®Ÿè¡Œæ–¹æ³•

```bash
# Jupyter Notebook/Labã§å®Ÿè¡Œ
jupyter lab experiments/exp0034/training_with_high_optuna.ipynb

# å®Ÿè¡Œã‚»ãƒ«é †åº:
# 1. ã‚»ãƒ«1ã€œ68: exp0033ã¨åŒã˜ãã€ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã€œLowå°‚é–€å®¶ã¾ã§å®Ÿè¡Œ
# 2. ã‚»ãƒ«69: Highå°‚é–€å®¶ã®Optunaæœ€é©åŒ–ï¼ˆæ–°è¦è¿½åŠ ã€50 trialsï¼‰
# 3. ã‚»ãƒ«70: Highå°‚é–€å®¶ã®å­¦ç¿’ï¼ˆæœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä½¿ç”¨ï¼‰
# 4. ã‚»ãƒ«71: ã‚²ãƒ¼ãƒˆåˆ†é¡å™¨ã®å­¦ç¿’
# 5. ã‚»ãƒ«72: MoEäºˆæ¸¬ã®åˆæˆ
# 6. ã‚»ãƒ«73: ã‚¼ãƒ­é–¾å€¤æœ€é©åŒ–ï¼ˆexp0033ã‹ã‚‰ç¶™æ‰¿ï¼‰
# 7. ã‚»ãƒ«74: ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜
# 8. ã‚»ãƒ«75: æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
```

### å®Ÿè¡Œæ™‚é–“ã®ç›®å®‰
- Highå°‚é–€å®¶Optunaæœ€é©åŒ–: ç´„15ã€œ20åˆ†ï¼ˆ50 trialsã€GPUä½¿ç”¨æ™‚ï¼‰
- å…¨ä½“å®Ÿè¡Œæ™‚é–“: ç´„30ã€œ40åˆ†

---

## ğŸ’¡ æ¬¡ã‚¹ãƒ†ãƒƒãƒ—ã®å€™è£œ

### ã•ã‚‰ãªã‚‹æ”¹å–„æ¡ˆ
1. **Lowå°‚é–€å®¶ã®Optunaæœ€é©åŒ–**: Highå°‚é–€å®¶ã¨åŒæ§˜ã«Lowå°‚é–€å®¶ã‚‚æœ€é©åŒ–ï¼ˆexp0035å€™è£œï¼‰
2. **3å°‚é–€å®¶MoE**: Low/Mid/Highã®éšå±¤çš„åˆ†é›¢
3. **æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿Ï„ã®å†æœ€é©åŒ–**: å°‚é–€å®¶æ€§èƒ½å‘ä¸Šå¾Œã®Ï„æ¢ç´¢
4. **CatBoost Highå°‚é–€å®¶**: LightGBMã¨ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
5. **Highé ˜åŸŸç‰¹åŒ–ç‰¹å¾´é‡**: ã‚­ãƒ¼ãƒ‘ã‚¹ã€ã‚¹ãƒ«ãƒ¼ãƒ‘ã‚¹ã€æ±ºå®šæ©Ÿé–¢é€£ã®ç‰¹å¾´é‡è¿½åŠ 

### å®Ÿé¨“ã®é€²ã‚æ–¹
- Highå°‚é–€å®¶ã®æ”¹å–„ãŒç¢ºèªã§ããŸã‚‰ã€Lowå°‚é–€å®¶ã‚‚æœ€é©åŒ–
- ä¸¡å°‚é–€å®¶ã®æ€§èƒ½ãŒå‘ä¸Šã—ãŸã‚‰ã€3å°‚é–€å®¶MoEãªã©é«˜åº¦åŒ–ã‚’æ¤œè¨

---

## ğŸ“– å‚è€ƒ

- **Optunaæœ€é©åŒ–**: exp0027ã§StratifiedGKFoldã§CVå®‰å®šåŒ–ã—ãŸå®Ÿç¸¾ã‚ã‚Š
- **Highé ˜åŸŸã®ç‰¹æ€§**: ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªãï¼ˆç´„30%ï¼‰ã€éå­¦ç¿’ã—ã‚„ã™ã„
- **æ­£å‰‡åŒ–ã®é‡è¦æ€§**: min_child_samplesã€reg_alphaã€reg_lambdaãŒéå­¦ç¿’æŠ‘åˆ¶ã«åŠ¹æœçš„
- **exp0032ã®æ•™è¨“**: å¯¾æ•°å¤‰æ›ã¯æ‚ªåŒ–ã—ãŸãŸã‚ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã«æ³¨åŠ›

---

## ğŸ·ï¸ ã‚¿ã‚°

`MoE` `High-Expert` `Optuna` `Hyperparameter-Optimization` `Regularization` `Zero-Threshold`

---

## ğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆ

```
exp0034/
â”œâ”€â”€ .gitignore                                   # Gitç„¡è¦–è¨­å®š
â”œâ”€â”€ README.md                                    # å®Ÿé¨“æ¦‚è¦ï¼ˆã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
â”œâ”€â”€ EXPERIMENT_METADATA.yaml                     # å®Ÿé¨“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
â”œâ”€â”€ training_with_high_optuna.ipynb             # ãƒ¡ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯
â”œâ”€â”€ logs/                                       # å®Ÿè¡Œãƒ­ã‚°ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹
â”‚   â”œâ”€â”€ .gitkeep
â”‚   â””â”€â”€ host_moe_high_optuna_001_metrics.json    # å®Ÿè¡Œå¾Œã«ç”Ÿæˆ
â”œâ”€â”€ artifacts/                                  # OOFäºˆæ¸¬ãªã©ã®ä¸­é–“æˆæœç‰©
â”‚   â”œâ”€â”€ .gitkeep
â”‚   â””â”€â”€ oof_predictions_moe_high_optuna.csv      # å®Ÿè¡Œå¾Œã«ç”Ÿæˆ
â””â”€â”€ submissions/                                # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«
    â”œâ”€â”€ .gitkeep
    â””â”€â”€ host_moe_high_optuna_001_submission.csv  # å®Ÿè¡Œå¾Œã«ç”Ÿæˆ
```

### ãƒ•ã‚¡ã‚¤ãƒ«èª¬æ˜

- **README.md**: å®Ÿé¨“ã®æ¦‚è¦ã€æ‰‹æ³•ã€æœŸå¾…åŠ¹æœã€å®Ÿè¡Œæ–¹æ³•
- **EXPERIMENT_METADATA.yaml**: å®Ÿé¨“ã®æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆæ—¥ä»˜ã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã€ç›®æ¨™ãªã©ï¼‰
- **training_with_high_optuna.ipynb**: å®Ÿé¨“ã®å®Ÿè¡Œãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯
- **logs/**: å®Ÿè¡Œçµæœã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆJSONå½¢å¼ï¼‰
- **artifacts/**: OOFäºˆæ¸¬ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿
- **submissions/**: Kaggleæå‡ºç”¨CSVãƒ•ã‚¡ã‚¤ãƒ«
